//=------ VecCloneVP.cpp - Vector function to loop transform -*- C++ -*------=//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
///
/// \file
/// This pass inserts the body of a vector function inside a vector length
/// trip count scalar loop for functions that are declared SIMD. The pass
/// currently follows the gcc vector ABI requirements for name mangling
/// encodings, but will be extended in the future to also support the Intel
/// vector ABI. References to both ABIs can be found here:
///
/// https://sourceware.org/glibc/wiki/libmvec?action=AttachFile&do=view&target=VectorABI.txt
/// https://software.intel.com/sites/default/files/managed/b4/c8/Intel-Vector-Function-ABI.pdf
///
/// Conceptually, this pass performs the following transformation:
///
/// Before Translation:
///
/// main.cpp
///
/// #pragma omp declare simd uniform(a) linear(k)
/// extern float dowork(float *a, float b, int k);
///
/// float a[4096];
/// float b[4096];
/// int main() {
///   int k;
///   for (k = 0; k < 4096; k++) {
///     b[k] = k;
///   }
/// #pragma clang loop vectorize(enable)
///   for (k = 0; k < 4096; k++) {
///     a[k] = k * 0.5;
///     a[k] = dowork(a, b[k], k);
///   }
/// }
///
/// dowork.cpp
///
/// #pragma omp declare simd uniform(a) linear(k) #0
/// float dowork(float *a, float b, int k) {
///   return sinf(a[k]) + b;
/// }
///
/// attributes #0 = { nounwind uwtable "vector-variants"="_ZGVbM4uvl_",
/// "ZGVbN4uvl_", ... }
///
/// After Translation:
///
/// dowork.cpp
///
/// // Non-masked variant
///
/// <VL x float> "_ZGVbN4uvl_dowork(float *a, <VL x float> b, int k) {
///   alloc <VL x float> vec_ret;
///   alloc <VL x float> vec_b;
///   // casts from vector to scalar pointer allows loop to be in a scalar form
///   // that can be vectorized easily.
///   ret_cast = bitcast <VL x float>* vec_ret to float*;
///   vec_b_cast = bitcast <VL x float>* vec_b to float*;
///   store <VL x float> b, <VL x float>* vec_b;
///   for (int i = 0; i < VL; i++, k++) {
///     ret_cast[i] = sinf(a[k]) + vec_b_cast[i];
///   }
///   return vec_ret;
/// }
///
/// // Masked variant
///
/// <VL x float> "_ZGVbM4uvl_dowork(float *a, <VL x float> b, int k, <VL x int>
/// mask) {
///   alloc <VL x float> vec_ret;
///   alloc <VL x float> vec_b;
///   ret_cast = bitcast <VL x float>* vec_ret to float*;
///   vec_b_cast = bitcast <VL x float>* vec_b to float*;
///   store <VL x float> b, <VL x float>* vec_b;
///   for (int i = 0; i < VL; i++, k++) {
///     if (mask[i] != 0)
///       ret_cast[i] = sinf(a[k]) + vec_b_cast[i];
///   }
///   return vec_ret;
/// }
///
// ===--------------------------------------------------------------------=== //

// This pass is flexible enough to recognize whether or not parameters have been
// registerized so that the users of the parameter can be properly updated. For
// instance, we need to know where the users of linear parameters are so that
// the stride can be added to them.
//
// In the following example, %i and %x are used directly by %add directly, so
// in this case the pass can just look for users of %i and %x.
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
//   %add = add nsw i32 %x, %i
//   ret i32 %add
// }
//
// When parameters have not been registerized, parameters are used indirectly
// through a store/load of the parameter to/from memory that has been allocated
// for them in the function. Thus, in this case, the pass looks for users of
// %0 and %1.
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
// %i.addr = alloca i32, align 4
// %x.addr = alloca i32, align 4
// store i32 %i, i32* %i.addr, align 4
// store i32 %x, i32* %x.addr, align 4
// %0 = load i32, i32* %x.addr, align 4
// %1 = load i32, i32* %i.addr, align 4
// %add = add nsw i32 %0, %1
//  ret i32 %add
// }
//
// The pass must run at all optimization levels because it is possible that
// a loop calling the vector function is vectorized, but the vector function
// itself is not vectorized. For example, above main.cpp may be compiled at
// -O2, but dowork.cpp may be compiled at -O0. Therefore, it is required that
// the attribute list for the vector function specify all variants that must
// be generated by this pass so as to avoid any linking problems. This pass
// also serves to canonicalize the input IR to the loop vectorizer.

#include "llvm/Transforms/Utils/VecCloneVP.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/Analysis/Passes.h"
#include "llvm/Analysis/TargetTransformInfo.h"
#include "llvm/Analysis/VectorUtils.h"
#include "llvm/IR/BasicBlock.h"
#include "llvm/IR/Constants.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/IR/VectorBuilder.h"
#include "llvm/InitializePasses.h"
#include "llvm/PassRegistry.h"
#include "llvm/Support/Debug.h"
#include "llvm/Support/raw_ostream.h"
#include "llvm/Transforms/Utils/Cloning.h"
#include <map>
#include <set>

#define SV_NAME "vec-clone-vp"
#define DEBUG_TYPE "vec-clone-vp"

using namespace llvm;

VecCloneVP::VecCloneVP() : ModulePass(ID) {}

void VecCloneVP::getAnalysisUsage(AnalysisUsage &AU) const {
  AU.addRequired<TargetTransformInfoWrapperPass>();
}

void VecCloneVPPass::getFunctionsToVectorize(llvm::Module &M,
                                             FunctionVariants &FuncVars) {

  // FuncVars will contain a one-to-many mapping between the original scalar
  // function and the vector variant encoding strings (represented as
  // attributes). The encodings correspond to functions that will be created by
  // the caller of this function as vector versions of the original function.
  // For example, if foo() is a function marked as a simd function, it will have
  // several vector variant encodings like: "_ZGVbM4_foo", "_ZGVbN4_foo",
  // "_ZGVcM8_foo", "_ZGVcN8_foo", "_ZGVdM8_foo", "_ZGVdN8_foo", "_ZGVeM16_foo",
  // "_ZGVeN16_foo". The caller of this function will then clone foo() and name
  // the clones using the above name manglings. The variant encodings correspond
  // to differences in masked/non-masked execution, vector length, and target
  // vector register size, etc. For more details, please refer to the vector
  // function abi references listed at the top of this file.

  for (auto &F : M.functions()) {
    auto AttrSet = F.getAttributes().getFnAttrs();
    // parse SIMD signatures
    for (const auto &Attr : AttrSet) {
      if (!Attr.isStringAttribute())
        continue;
      StringRef AttrText = Attr.getKindAsString();
      Optional<VFInfo> VFInfo = VFABI::tryDemangleForVFABI(
          AttrText, M, /* RequireDeclaration */ false);
      if (!VFInfo) {
        LLVM_DEBUG(llvm::dbgs()
                   << "[VecCloneVP] Discarding |" << AttrText << "|\n");
        continue;
      }
      LLVM_DEBUG(llvm::dbgs()
                 << "[VecCloneVP] Considering |" << AttrText << "|\n");
      FuncVars[&F].push_back(*VFInfo);
    }
  }
}

template Constant *
VecCloneVPPass::getConstantValue<int>(Type *Ty, LLVMContext &Context, int Val);
template Constant *VecCloneVPPass::getConstantValue<float>(Type *Ty,
                                                           LLVMContext &Context,
                                                           float Val);
template Constant *
VecCloneVPPass::getConstantValue<double>(Type *Ty, LLVMContext &Context,
                                         double Val);
template <typename T>
Constant *VecCloneVPPass::getConstantValue(Type *Ty, LLVMContext &Context,
                                           T Val) {
  Constant *ConstVal = nullptr;
  if (Ty->isIntegerTy())
    ConstVal = ConstantInt::get(Ty, Val);
  else if (Ty->isFloatTy())
    ConstVal = ConstantFP::get(Ty, Val);

  assert(ConstVal && "Could not generate constant for type");
  return ConstVal;
}

// FIXME
static bool isMasked(const VFShape &Shape) {
  return std::find_if(Shape.Parameters.begin(), Shape.Parameters.end(),
                      [](const VFParameter &P) {
                        return P.ParamKind == VFParamKind::GlobalPredicate;
                      }) != Shape.Parameters.end();
}

static bool hasVL(const VFShape &Shape) {
  return std::find_if(Shape.Parameters.begin(), Shape.Parameters.end(),
                      [](const VFParameter &P) {
                        return P.ParamKind == VFParamKind::GlobalVL;
                      }) != Shape.Parameters.end();
}

Function *VecCloneVPPass::cloneFunction(Module &M, Function &F,
                                        const VFInfo &V) {
  std::string VariantName = V.VectorName;
  if (M.getFunction(VariantName))
    return nullptr;

  FunctionType *OrigFunctionType = F.getFunctionType();
  Type *ReturnType = F.getReturnType();

  // Expand return type to vector.
  if (!ReturnType->isVoidTy())
    ReturnType = VectorType::get(ReturnType, V.Shape.VF);

  const auto &ParmKinds = V.Shape.Parameters;
  SmallVector<Type *, 4> ParmTypes;
  const auto *VKIt = ParmKinds.begin();
  for (auto *ParamTy : OrigFunctionType->params()) {
    if (VKIt->ParamKind == VFParamKind::Vector)
      ParmTypes.push_back(
          VectorType::get(ParamTy->getScalarType(), V.Shape.VF));
    else
      ParmTypes.push_back(ParamTy);
    ++VKIt;
  }

  if (isMasked(V.Shape)) {
    Type *MaskVecTy =
        VectorType::get(Type::getInt1Ty(M.getContext()), V.Shape.VF);
    ParmTypes.push_back(MaskVecTy);
  }

  if (hasVL(V.Shape))
    ParmTypes.push_back(Type::getInt32Ty(M.getContext()));

  FunctionType *CloneFuncType = FunctionType::get(ReturnType, ParmTypes, false);
  Function *Clone = Function::Create(
      CloneFuncType, GlobalValue::ExternalLinkage, VariantName, F.getParent());

  ValueToValueMapTy Vmap;
  Function::arg_iterator NewArgIt = Clone->arg_begin();
  for (auto &Arg : F.args()) {
    NewArgIt->setName(Arg.getName());
    Vmap[&Arg] = &*NewArgIt;
    ++NewArgIt;
  }

  if (isMasked(V.Shape)) {
    Argument &MaskArg = *NewArgIt;
    MaskArg.setName("mask");
    ++NewArgIt;
  }

  if (hasVL(V.Shape)) {
    Argument &VLArg = *NewArgIt;
    VLArg.setName("vl");
  }

  SmallVector<ReturnInst *, 8> Returns;
  CloneFunctionInto(Clone, &F, Vmap, CloneFunctionChangeType::LocalChangesOnly,
                    Returns);

  // Remove incompatible argument attributes (applied to the scalar argument,
  // does not apply to its vector counterpart). This must be done after cloning
  // the function because CloneFunctionInto() transfers parameter attributes
  // from the original parameters in the Vmap.
  uint64_t Idx = 0;
  for (auto &Arg : Clone->args()) {
    Type *ArgType = Arg.getType();
    auto AM = AttributeFuncs::typeIncompatible(ArgType);
    Clone->removeParamAttrs(Idx, AM);
    ++Idx;
  }

  auto AM = AttributeFuncs::typeIncompatible(ReturnType);
  Clone->removeRetAttrs(AM);

  LLVM_DEBUG(
      dbgs() << "[VecCloneVP] After Cloning and Function Signature widening\n");
  LLVM_DEBUG(Clone->dump());

  return Clone;
}

PHINode *VecCloneVPPass::generateLoopForFunctionBody(
    Function *Clone, BasicBlock *EntryBlock, BasicBlock *LoopBlock,
    BasicBlock *LoopExitBlock, BasicBlock *ReturnBlock, VFShape Shape) {
  auto &Context = Clone->getContext();

  // Create the phi node for the top of the loop block and add the back
  // edge to the loop from the loop exit.
  IRBuilder<> Builder(LoopBlock->getFirstNonPHI());
  PHINode *Phi = Builder.CreatePHI(Type::getInt32Ty(Context), 2, "index");

  Builder.SetInsertPoint(LoopExitBlock->getTerminator());
  Constant *Inc = ConstantInt::get(Type::getInt32Ty(Context), 1);
  auto *Induction = cast<Instruction>(Builder.CreateNSWAdd(Phi, Inc, "indvar"));

  // FIXME
  Value *VL;
  if (hasVL(Shape)) {
    Function::arg_iterator VLParam = Clone->arg_end();
    VLParam--;
    VL = VLParam;
  } else
    VL = ConstantInt::get(Type::getInt32Ty(Context),
                          Shape.VF.getKnownMinValue());

  auto *VLCmp =
      cast<Instruction>(Builder.CreateICmpSLT(Induction, VL, "vl.cond"));
  Builder.CreateCondBr(VLCmp, LoopBlock, ReturnBlock);

  LoopExitBlock->getTerminator()->eraseFromParent();

  Constant *IndInit = ConstantInt::get(Type::getInt32Ty(Context), 0);
  Phi->addIncoming(IndInit, EntryBlock);
  Phi->addIncoming(Induction, LoopExitBlock);

  LLVM_DEBUG(dbgs() << "[VecCloneVP] After Loop Insertion\n");
  LLVM_DEBUG(Clone->dump());

  return Phi;
}

bool VecCloneVPPass::isSimpleFunction(Function *Clone, BasicBlock &EntryBlock) {
  // For really simple functions, there is no need to go through the process
  // of inserting a loop.

  // Example:
  //
  // void foo(void) {
  //   return;
  // }
  //
  // No need to insert a loop for this case since it's basically a no-op. Just
  // clone the function and return. It's possible that we could have some code
  // inside of a vector function that modifies global memory. Let that case go
  // through.
  ReturnInst *RetInst = dyn_cast<ReturnInst>(EntryBlock.getTerminator());
  if (RetInst && Clone->getReturnType()->isVoidTy())
    return true;

  return false;
}

void VecCloneVPPass::removeIncompatibleAttributes(Function *Clone) {
  for (auto &Arg : Clone->args()) {
    // For functions that only have a return instruction and are not void,
    // the return type is widened to vector. For this case, the returned
    // attribute becomes incompatible and must be removed.
    if (Clone->hasParamAttribute(Arg.getArgNo(), Attribute::Returned))
      Clone->removeParamAttr(Arg.getArgNo(), Attribute::Returned);
  }
}

// void VecCloneVPPass::insertSplitForMaskedVariant(Function *Clone,
//                                                  BasicBlock *LoopBlock,
//                                                  BasicBlock *LoopExitBlock,
//                                                  Instruction *Mask,
//                                                  PHINode *Phi) {
//   auto &Context = Clone->getContext();
//   IRBuilder<> Builder(Context);

//   BasicBlock *LoopThenBlock =
//       LoopBlock->splitBasicBlock(LoopBlock->getFirstNonPHI(),
//       "simd.loop.then");
//   BasicBlock *LoopElseBlock = BasicBlock::Create(
//       Context, "simd.loop.else", Clone, LoopExitBlock);

//   Instruction *ElseTerm = LoopElseBlock->getTerminator();
//   auto *BrInst = Builder.CreateBr(LoopExitBlock);
//   BrInst->insertBefore(ElseTerm);
//   ElseTerm->removeFromParent();

//   BitCastInst *BitCast = dyn_cast<BitCastInst>(Mask);
//   PointerType *BitCastType = dyn_cast<PointerType>(BitCast->getType());
//   // FIXME!
//   Type *PointeeType = BitCastType->getPointerElementType();
//   Instruction *Term = LoopBlock->getTerminator();
//   auto *MaskGEP = cast<Instruction>(Builder.CreateGEP(PointeeType, Mask, Phi,
//   "mask.gep")); MaskGEP->insertBefore(Term);

//   LoadInst *MaskLoad = Builder.CreateLoad(PointeeType, MaskGEP, "mask.parm");
//   MaskLoad->insertBefore(Term);

//   Type *CompareTy = MaskLoad->getType();
//   Instruction *MaskCmp;
//   Constant *Zero;

//   // Generate the compare instruction to see if the mask bit is on. In ICC,
//   we
//   // use the movemask intrinsic which takes both float/int mask registers and
//   // converts to an integer scalar value, one bit representing each element.
//   // AVR construction will be complicated if this intrinsic is introduced
//   here,
//   // so the current solution is to just generate either an integer or
//   floating
//   // point compare instruction for now. This may change anyway if we decide
//   to
//   // go to a vector of i1 values for the mask. I suppose this would be one
//   // positive reason to use vector of i1.
//   if (CompareTy->isIntegerTy()) {
//     Zero = getConstantValue(CompareTy, Context, 0);
//     MaskCmp = cast<Instruction>(Builder.CreateICmpNE(MaskLoad, Zero,
//     "mask.cond"));
//   } else if (CompareTy->isFloatingPointTy()) {
//     Zero = getConstantValue(CompareTy, Context, 0.0);
//     MaskCmp = cast<Instruction>(Builder.CreateFCmpUNE(MaskLoad, Zero,
//     "mask.cond"));
//   } else {
//     llvm::report_fatal_error("Unsupported mask compare");
//   }
//   MaskCmp->insertBefore(Term);

//   auto *CondBrInst = Builder.CreateCondBr(MaskCmp, LoopThenBlock,
//   LoopElseBlock); CondBrInst->insertBefore(Term);

//   Term->eraseFromParent();

//   LLVM_DEBUG(
//       dbgs() << "[VecCloneVP] After Split Insertion For Masked Variant\n");
//   LLVM_DEBUG(Clone->dump());
// }

void VecCloneVPPass::addLoopMetadata(BasicBlock *Latch, ElementCount VF) {
  // This function sets the loop metadata for the new loop inserted around
  // the simd function body. This metadata includes disabling unrolling just
  // in case for some reason that unrolling occurs in between this pass and
  // the vectorizer. Also, the loop vectorization metadata is set to try
  // and force vectorization at the specified VF of the simd function.
  //
  // Set disable unroll metadata on the conditional branch of the loop latch
  // for the simd loop. The following is an example of what the loop latch
  // and Metadata will look like. The !llvm.loop marks the beginning of the
  // loop Metadata and is always placed on the terminator of the loop latch.
  // (i.e., simd.loop.exit in this case). According to LLVM documentation, to
  // properly set the loop Metadata, the 1st operand of !16 must be a self-
  // reference to avoid some type of Metadata merging conflicts that have
  // apparently arisen in the past. This is part of LLVM history that I do not
  // know. Also, according to LLVM documentation, any Metadata nodes referring
  // to themselves are marked as distinct. As such, all Metadata corresponding
  // to a loop belongs to that loop alone and no sharing of Metadata can be
  // done across different loops.
  //
  // simd.loop.exit:        ; preds = %simd.loop, %if.else, %if.then
  //  %indvar = add nuw i32 %index, 1
  //  %vl.cond = icmp ult i32 %indvar, 2
  //  br i1 %vl.cond, label %simd.loop, label %simd.end.region, !llvm.loop !16
  //
  // !16 = distinct !{!16, !17}
  // !17 = !{!"llvm.loop.unroll.disable"}

  SmallVector<Metadata *, 4> MDs;

  // Reserve first location for self reference to the LoopID metadata node.
  MDs.push_back(nullptr);

  // Add unroll(disable) metadata to disable future unrolling.
  LLVMContext &Context = Latch->getContext();
  MDs.push_back(
      MDNode::get(Context, MDString::get(Context, "llvm.loop.unroll.disable")));
  MDs.push_back(MDNode::get(
      Context, MDString::get(Context, "llvm.loop.vectorize.enable")));
  Metadata *Vals[] = {MDString::get(Context, "llvm.loop.vectorize.width"),
                      ConstantAsMetadata::get(ConstantInt::get(
                          Type::getInt32Ty(Context), VF.getKnownMinValue()))};
  MDs.push_back(MDNode::get(Context, Vals));
  if (VF.isScalable()) {
    MDs.push_back(MDNode::get(
        Context, {MDString::get(Context, "llvm.loop.vectorize.scalable.enable"),
                  ConstantAsMetadata::get(
                      ConstantInt::get(Type::getInt1Ty(Context), 1))}));
  }
  MDs.push_back(MDNode::get(
      Context, {MDString::get(Context, "llvm.loop.vectorize.predicate.enable"),
                ConstantAsMetadata::get(
                    ConstantInt::get(Type::getInt1Ty(Context), 1))}));
  // // This loop won't need any tail.
  // MDs.push_back(MDNode::get(
  //     Context, MDString::get(Context, "llvm.loop.vectorize.skip_tail")));

  MDNode *NewLoopID = MDNode::get(Context, MDs);
  // Set operand 0 to refer to the loop id itself.
  NewLoopID->replaceOperandWith(0, NewLoopID);
  Latch->getTerminator()->setMetadata("llvm.loop", NewLoopID);
}

void VecCloneVPPass::widenAllocaInstructions(
    Function *Clone, DenseMap<AllocaInst *, Instruction *> &AllocaMap,
    BasicBlock &EntryBlock, const VFInfo &Variant, const DataLayout &DL) {
  DenseMap<AllocaInst *, Instruction *>::iterator AllocaMapIt;
  SmallVector<StoreInst *, 4> StoresToRemove;

  Value *MaskArg = nullptr;
  if (isMasked(Variant.Shape))
    MaskArg = Clone->getArg(Clone->arg_size() - 2);
  Value *VLArg = nullptr;
  if (hasVL(Variant.Shape))
    VLArg = Clone->getArg(Clone->arg_size() - 1);

  for (auto &Arg : Clone->args()) {
    SmallVector<User *, 4> ArgUsers;
    for (auto *U : Arg.users()) {
      // Only update parameter users in the loop.
      if (Instruction *Inst = dyn_cast<Instruction>(U))
        if (Inst->getParent() != &EntryBlock)
          ArgUsers.push_back(U);
    }

    Type *ArgTy = Arg.getType();
    StringRef ArgName = Arg.getName();
    for (auto *U : ArgUsers) {
      // For non-optimized parameters, i.e., for parameters that are loads and
      // stores through memory (allocas), we need to know which alloca belongs
      // to which parameter. This can be done by finding the store of the
      // parameter to an alloca. Set up a map that maintains this relationship
      // so that we can update the users of the original allocas with the new
      // widened ones. When widening the allocas, vector parameters will be
      // stored to a vector alloca, and linear/uniform parameters will be
      // stored to an array, using the loop index as the "lane". Nothing else
      // needs to be done for optimized parameters. Later, this map will be
      // used to update all alloca users.
      StoreInst *StoreUser = dyn_cast<StoreInst>(U);
      LoadInst *LoadUser = dyn_cast<LoadInst>(U);
      AllocaInst *Alloca = nullptr;

      if (LoadUser)
        Alloca = dyn_cast<AllocaInst>(LoadUser->getPointerOperand());

      if (StoreUser)
        Alloca = dyn_cast<AllocaInst>(StoreUser->getPointerOperand());

      if (StoreUser && Alloca) {
        AllocaMapIt = AllocaMap.find(Alloca);
        if (AllocaMapIt == AllocaMap.end()) {
          IRBuilder<> Builder(EntryBlock.getTerminator());

          if (VectorType *VecArgType = dyn_cast<VectorType>(ArgTy)) {
            AllocaInst *VecAlloca = Builder.CreateAlloca(
                VecArgType, DL.getAllocaAddrSpace(), nullptr, "vec." + ArgName);

            if (VLArg) {
              auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
              if (!MaskArg) {
                auto *MaskTy = VectorType::get(Builder.getInt1Ty(), VecArgType);
                MaskArg = ConstantInt::getAllOnesValue(MaskTy);
              }
              VPBuilder.setMask(MaskArg);
              VPBuilder.setEVL(VLArg);
              VPBuilder.createVectorInstruction(Instruction::Store, nullptr,
                                                {&Arg, VecAlloca});
            } else
              Builder.CreateAlignedStore(&Arg, VecAlloca,
                                         DL.getABITypeAlign(VecArgType), false);

            PointerType *ElemTypePtr =
                PointerType::get(VecArgType->getElementType(),
                                 VecAlloca->getType()->getAddressSpace());
            auto *VecAllocaCast = cast<Instruction>(Builder.CreateBitCast(
                VecAlloca, ElemTypePtr, VecAlloca->getName() + ".cast"));

            AllocaMap[Alloca] = VecAllocaCast;
            StoresToRemove.push_back(StoreUser);
          } else {
            llvm_unreachable(
                "ArgTy is not a VectorType: this shouldn't happen");
            // FIXME - This won't fly at all.
            // ArrayType *ArrType =
            //     ArrayType::get(ArgTy, Variant.Shape.VF.getKnownMinValue());
            // AllocaInst *ArrAlloca =
            //     new AllocaInst(ArrType, DL.getAllocaAddrSpace(),
            //                    "arr." + ArgName, EntryBlock.getTerminator());
            // AllocaMap[Alloca] = ArrAlloca;
          }
        }
      }
    }
  }

  // Remove the store of the parameter to the original alloca. A new one
  // was just created for the new alloca.
  for (auto *Store : StoresToRemove)
    Store->eraseFromParent();
}

void VecCloneVPPass::updateAllocaUsers(
    Function *Clone, PHINode *Phi,
    DenseMap<AllocaInst *, Instruction *> &AllocaMap) {

  SmallVector<User *, 10> AllocaUsers;
  for (auto Pair : AllocaMap) {
    AllocaInst *OldAlloca = Pair.first;
    for (auto *U : OldAlloca->users()) {
      if (isa<Instruction>(U))
        AllocaUsers.push_back(U);
    }
  }

  // Update all alloca users by doing an a -> &a[i] transformation. This
  // involves inserting a gep just before each use of the alloca. The only
  // exception is for vector stores to an alloca. These are moved to the
  // entry block of the function just after the widened alloca.
  for (auto *U : AllocaUsers) {
    IRBuilder<> Builder(cast<Instruction>(U));
    unsigned NumOps = U->getNumOperands();
    for (unsigned K = 0; K < NumOps; K++) {
      if (AllocaInst *OldAlloca = dyn_cast<AllocaInst>(U->getOperand(K))) {
        if (AllocaInst *NewAlloca =
                dyn_cast<AllocaInst>(AllocaMap[OldAlloca])) {
          // If this is an alloca for a linear/uniform parameter, then insert
          // a gep for the load/store and use the loop index to reference the
          // proper value for each "lane".
          SmallVector<Value *, 2> GepIndices;
          Constant *Idx0 =
              ConstantInt::get(Type::getInt32Ty(Clone->getContext()), 0);
          GepIndices.push_back(Idx0);
          GepIndices.push_back(Phi);
          Type *SeqTy = NewAlloca->getAllocatedType();
          auto *AllocaGep = Builder.CreateGEP(SeqTy, NewAlloca, GepIndices,
                                              NewAlloca->getName() + ".gep");
          U->setOperand(K, AllocaGep);
        } else if (BitCastInst *NewAllocaCast =
                       dyn_cast<BitCastInst>(AllocaMap[OldAlloca])) {
          SmallVector<Value *, 2> GepIndices;
          GepIndices.push_back(Phi);
          auto *AllocaCastGep =
              Builder.CreateGEP(OldAlloca->getAllocatedType(), NewAllocaCast,
                                GepIndices, NewAllocaCast->getName() + ".gep");
          U->setOperand(K, AllocaCastGep);
        } else {
          llvm_unreachable(
              "Expected array alloca for linear/uniform parameters or a "
              "cast of vector alloca for vector parameters");
        }
      }
    }
  }
}

void VecCloneVPPass::updateParameterUsers(Function *Clone,
                                          const VFInfo &Variant,
                                          BasicBlock &EntryBlock, PHINode *Phi,
                                          const DataLayout &DL) {
  // Update non-alloca parameter users based on type of parameter. Any users of
  // the parameters that are also users of an alloca will not be updated again
  // here since this has already been done.
  const auto &ParmKinds = Variant.Shape.Parameters;
  DenseMap<Argument *, BitCastInst *> VecParmCasts;
  DenseMap<Argument *, BitCastInst *>::iterator VecParmCastsIt;

  Value *MaskArg = nullptr;
  if (isMasked(Variant.Shape))
    MaskArg = Clone->getArg(Clone->arg_size() - 2);
  Value *VLArg = nullptr;
  if (hasVL(Variant.Shape))
    VLArg = Clone->getArg(Clone->arg_size() - 1);

  for (auto &Arg : Clone->args()) {
    SmallVector<User *, 4> ArgUsers;
    for (auto *U : Arg.users()) {
      // Only update parameter users in the loop.
      if (Instruction *Inst = dyn_cast<Instruction>(U))
        if (Inst->getParent() != &EntryBlock)
          ArgUsers.push_back(U);
    }

    Type *ArgTy = Arg.getType();
    unsigned ArgNo = Arg.getArgNo();
    StringRef ArgName = Arg.getName();
    VectorType *VecArgType = dyn_cast<VectorType>(ArgTy);
    for (unsigned J = 0; J < ArgUsers.size(); J++) {
      User *U = ArgUsers[J];
      IRBuilder<> Builder(cast<Instruction>(U));
      if (ParmKinds[ArgNo].ParamKind == VFParamKind::Vector) {
        VecParmCastsIt = VecParmCasts.find(&Arg);
        if (VecParmCastsIt == VecParmCasts.end()) {
          Builder.SetInsertPoint(EntryBlock.getTerminator());
          AllocaInst *VecAlloca = Builder.CreateAlloca(
              VecArgType, DL.getAllocaAddrSpace(), nullptr, "vec." + ArgName);

          if (VLArg) {
            auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
            if (!MaskArg) {
              auto *MaskTy = VectorType::get(Builder.getInt1Ty(), VecArgType);
              MaskArg = ConstantInt::getAllOnesValue(MaskTy);
            }
            VPBuilder.setMask(MaskArg);
            VPBuilder.setEVL(VLArg);
            VPBuilder.createVectorInstruction(Instruction::Store, nullptr,
                                              {&Arg, VecAlloca});
          } else
            Builder.CreateAlignedStore(&Arg, VecAlloca,
                                       DL.getABITypeAlign(VecArgType), false);

          PointerType *ElemTypePtr =
              PointerType::get(VecArgType->getElementType(),
                               VecAlloca->getType()->getAddressSpace());
          auto *VecAllocaCast = cast<Instruction>(Builder.CreateBitCast(
              VecAlloca, ElemTypePtr, VecAlloca->getName() + ".cast"));
          VecParmCasts[&Arg] = cast<BitCastInst>(VecAllocaCast);

          Builder.SetInsertPoint(cast<Instruction>(U));
        }

        auto *VecAllocaCastGep =
            Builder.CreateGEP(VecArgType->getElementType(), VecParmCasts[&Arg],
                              Phi, VecParmCasts[&Arg]->getName() + ".gep");

        Value *ArgElemLoad = nullptr;
        if (VLArg) {
          auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
          if (!MaskArg) {
            auto *MaskTy = VectorType::get(Builder.getInt1Ty(), VecArgType);
            MaskArg = ConstantInt::getAllOnesValue(MaskTy);
          }
          VPBuilder.setMask(MaskArg);
          VPBuilder.setEVL(VLArg);
          ArgElemLoad = VPBuilder.createVectorInstruction(
              Instruction::Load, VecArgType, {VecAllocaCastGep},
              "vec." + ArgName + ".elem");
        } else
          ArgElemLoad = Builder.CreateAlignedLoad(
              VecArgType->getElementType(), VecAllocaCastGep,
              DL.getABITypeAlign(VecArgType), false,
              "vec." + ArgName + ".elem");

        unsigned NumOps = U->getNumOperands();
        for (unsigned Op = 0; Op < NumOps; Op++) {
          if (U->getOperand(Op) == &Arg)
            U->setOperand(Op, ArgElemLoad);
        }
      } else if (ParmKinds[ArgNo].ParamKind == VFParamKind::OMP_Linear) {
        // FIXME: There is a plethora of linear stuff that will probably need
        // different ways of handling it.
        int Stride = ParmKinds[ArgNo].LinearStepOrPos;
        Constant *StrideConst =
            ConstantInt::get(Type::getInt32Ty(Clone->getContext()), Stride);
        auto *Mul = Builder.CreateMul(StrideConst, Phi, "stride.mul");

        Value *UserOp = nullptr;
        if (PointerType *ParmPtrType = dyn_cast<PointerType>(ArgTy)) {
          if (ParmPtrType->isOpaque())
            llvm::report_fatal_error("Can't retrieve missing type info from the pointer itself.");

          UserOp = Builder.CreateGEP(ParmPtrType->getNonOpaquePointerElementType(), &Arg,
                                     Mul, ArgName + ".gep");
        } else {
          if (Mul->getType() != ArgTy)
            Mul = Builder.CreateSExtOrBitCast(Mul, ArgTy,
                                              Mul->getName() + ".cast");

          UserOp = Builder.CreateAdd(&Arg, Mul, "stride.add");
        }

        unsigned NumOps = U->getNumOperands();
        for (unsigned Op = 0; Op < NumOps; Op++) {
          if (U->getOperand(Op) == &Arg)
            U->setOperand(Op, UserOp);
        }
      }
    }
  }
}

bool VecCloneVPPass::runImpl(Module &M, Function &F, const VFInfo &Variant) {

  LLVM_DEBUG(dbgs() << "[VecCloneVP] Before SIMD Function Cloning\n");
  LLVM_DEBUG(F.dump());
  LLVM_DEBUG(dbgs() << "[VecCloneVP] Generating variant '" << Variant.VectorName
                    << "'\nClone->getContext();n");

  // Clone the original function.
  Function *Clone = cloneFunction(M, F, Variant);
  if (!Clone) {
    LLVM_DEBUG(dbgs() << "[VecCloneVP] Could not clone\n");
    return false;
  }

  BasicBlock &EntryBlock = Clone->getEntryBlock();
  if (isSimpleFunction(Clone, EntryBlock)) {
    LLVM_DEBUG(dbgs() << "[VecCloneVP] Function is too simple\n");
    return false;
  }

  // Remove any incompatible attributes that happen as part of widening
  // function vector parameters.
  removeIncompatibleAttributes(Clone);

  const DataLayout &DL = Clone->getParent()->getDataLayout();
  DenseMap<AllocaInst *, Instruction *> AllocaMap;
  // Split the entry block at the beginning and create a block for the
  // loop entry.
  BasicBlock *LoopBlock =
      EntryBlock.splitBasicBlock(EntryBlock.begin(), "simd.loop");

  // On the split, the alloca instructions are moved into LoopBlock. Move
  // them back to the entry block.
  SmallVector<AllocaInst *, 4> Allocas;
  SmallVector<StoreInst *, 4> VecStores;
  BasicBlock::iterator BBIt = LoopBlock->begin();
  BasicBlock::iterator BBEnd = LoopBlock->end();
  for (; BBIt != BBEnd; ++BBIt)
    if (AllocaInst *Alloca = dyn_cast<AllocaInst>(&*BBIt))
      Allocas.push_back(Alloca);
  for (auto *Alloca : Allocas)
    Alloca->moveBefore(EntryBlock.getTerminator());

  widenAllocaInstructions(Clone, AllocaMap, EntryBlock, Variant, DL);

  // Create a vector alloca for the return. The return type of the clone
  // has already been widened, so the type can be used directly.
  AllocaInst *VecRetAlloca = nullptr;
  IRBuilder<> Builder(EntryBlock.getTerminator());
  Type *VecRetTy = Clone->getReturnType();
  if (!VecRetTy->isVoidTy())
    VecRetAlloca = Builder.CreateAlloca(VecRetTy, DL.getAllocaAddrSpace(),
                                        nullptr, "vec.ret");

  // Find the basic block containing the return. We need to know where
  // to replace the return instruction with a store to the return vector
  // and where to split off a loop exit block containing the loop exit
  // condition.
  Function::iterator FuncIt = Clone->begin();
  Function::iterator FuncEnd = Clone->end();
  BasicBlock *ReturnBlock = nullptr;
  Instruction *RetInst = nullptr;
  unsigned NumRets = 1;
  for (; FuncIt != FuncEnd; ++FuncIt) {
    if (isa<ReturnInst>(FuncIt->getTerminator())) {
      // TODO: Haven't yet found (or created) a test case where there are
      // multiple ret instructions. Assert for now.
      assert(NumRets == 1 &&
             "Unsupported function due to multiple return instructions");
      ReturnBlock = &*FuncIt;
      RetInst = FuncIt->getTerminator();
      NumRets++;
    }
  }

  // Create a basic block that will contain the loop exit condition.
  BasicBlock *LoopExitBlock =
      ReturnBlock->splitBasicBlock(RetInst, "simd.loop.exit");

  // Create a new return block that will contain the load of the return
  // vector and the new return instruction.
  BasicBlock *NewReturnBlock =
      LoopExitBlock->splitBasicBlock(LoopExitBlock->getTerminator(), "return");

  // Generate the phi for the loop index, the loop index increment, and
  // loop exit condition and put these instructions in LoopExitBlock.
  PHINode *Phi =
      generateLoopForFunctionBody(Clone, &EntryBlock, LoopBlock, LoopExitBlock,
                                  NewReturnBlock, Variant.Shape);

  // Generate the load from the return vector and new return instruction
  // and put them in the new return basic block.
  Builder.SetInsertPoint(NewReturnBlock->getTerminator());

  Value *VecReturn = nullptr;
  if (hasVL(Variant.Shape)) {
    Value *MaskArg = nullptr;
    if (isMasked(Variant.Shape))
      MaskArg = Clone->getArg(Clone->arg_size() - 2);
    else
      MaskArg = ConstantInt::getAllOnesValue(
          VectorType::get(Builder.getInt1Ty(), cast<VectorType>(VecRetTy)));

    Value *VLArg = Clone->getArg(Clone->arg_size() - 1);

    auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
    VPBuilder.setMask(MaskArg);
    VPBuilder.setEVL(VLArg);
    VecReturn = VPBuilder.createVectorInstruction(Instruction::Load, VecRetTy,
                                                  {VecRetAlloca}, "vec.ret");
  } else
    VecReturn = Builder.CreateLoad(VecRetTy, VecRetAlloca, "vec.ret");

  Builder.CreateRet(VecReturn);

  // Change the return instruction to a store to the return vector.
  Value *StoreVal = RetInst->getOperand(0);
  Type *StoreValTy = StoreVal->getType();
  PointerType *ElemTypePtr =
      PointerType::get(StoreValTy, DL.getAllocaAddrSpace());

  Builder.SetInsertPoint(EntryBlock.getTerminator());
  auto *RetAllocaCast = Builder.CreateBitCast(
      VecRetAlloca, ElemTypePtr, VecRetAlloca->getName() + ".cast");

  Builder.SetInsertPoint(ReturnBlock->getTerminator());
  auto *RetAllocaCastGep = Builder.CreateGEP(StoreValTy, RetAllocaCast, Phi,
                                             RetAllocaCast->getName() + ".gep");
  Builder.CreateAlignedStore(RetInst->getOperand(0), RetAllocaCastGep,
                             DL.getABITypeAlign(StoreValTy), false);
  RetInst->eraseFromParent();

  updateAllocaUsers(Clone, Phi, AllocaMap);

  updateParameterUsers(Clone, Variant, EntryBlock, Phi, DL);

  // For masked variants, create a vector mask parameter and insert the mask
  // bit checks.
  // FIXME !!!
  if (isMasked(Variant.Shape)) {
    llvm::report_fatal_error("Masked variants unsupported");

    // Create a vector alloca for the mask parameter.
    // Function::arg_iterator MaskParam = Clone->arg_end();
    // MaskParam--;
    // if (hasVL(Variant.Shape))
    //   MaskParam--;
    // AllocaInst *MaskAlloca = new AllocaInst(
    //     MaskParam->getType(), DL.getAllocaAddrSpace(),
    //     "vec." + MaskParam->getName(), EntryBlock.getTerminator());
    // StoreInst *MaskStore = new StoreInst(
    //     MaskParam, MaskAlloca, false,
    //     DL.getABITypeAlign(MaskParam->getType()));
    // MaskStore->insertAfter(MaskAlloca);

    // VectorType *MaskTy = cast<VectorType>(MaskParam->getType());
    // PointerType *ElemTypePtr =
    //     PointerType::get(MaskTy->getElementType(), DL.getAllocaAddrSpace());
    // BitCastInst *MaskAllocaCast =
    //     new BitCastInst(MaskAlloca, ElemTypePtr, "mask.cast");
    // MaskAllocaCast->insertAfter(MaskStore);

    // insertSplitForMaskedVariant(Clone, LoopBlock, LoopExitBlock,
    // MaskAllocaCast,
    //                             Phi);
  }

  // Remove old allocas
  for (auto Pair : AllocaMap) {
    AllocaInst *OldAlloca = Pair.first;
    OldAlloca->eraseFromParent();
  }

  // Prevent unrolling from kicking in before loop vectorization and force
  // vectorization of the loop to the VF of the simd function.
  addLoopMetadata(LoopExitBlock, Variant.Shape.VF);

  LLVM_DEBUG(dbgs() << "[VecCloneVP] After SIMD Function Cloning\n");
  LLVM_DEBUG(Clone->dump());

  return true; // LLVM IR has been modified
}

bool VecCloneVP::runOnModule(Module &M) {
  bool Changed = false;
  FunctionVariants FunctionsToVectorize;
  Impl.getFunctionsToVectorize(M, FunctionsToVectorize);
  for (auto Pair : FunctionsToVectorize) {
    Function &F = *(Pair.first);
    std::vector<VFInfo> Variants = Pair.second;
    // TargetTransformInfo *TTI =
    //   &getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
    for (auto V : Variants) {
      Changed |= Impl.runImpl(M, F, V);
    }
  }

  return Changed;
}

PreservedAnalyses VecCloneVPPass::run(Module &M, ModuleAnalysisManager &AM) {
  bool Changed = false;
  // auto &FAM =
  // AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();
  FunctionVariants FunctionsToVectorize;
  getFunctionsToVectorize(M, FunctionsToVectorize);
  for (auto Pair : FunctionsToVectorize) {
    Function &F = *(Pair.first);
    std::vector<VFInfo> Variants = Pair.second;
    // TargetTransformInfo *TTI = &FAM.getResult<TargetIRAnalysis>(F);
    for (auto V : Variants) {
      Changed |= runImpl(M, F, V);
    }
  }

  if (Changed)
    return PreservedAnalyses::none();
  return PreservedAnalyses::all();
}

void VecCloneVP::print(raw_ostream &OS, const Module *M) const {
  // TODO
}

ModulePass *llvm::createVecCloneVPPass() { return new llvm::VecCloneVP(); }

char VecCloneVP::ID = 0;

static const char LVNAME[] = "VecCloneVP";
INITIALIZE_PASS_BEGIN(VecCloneVP, SV_NAME, LVNAME, false /* modifies CFG */,
                      false /* transform pass */)
INITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)
INITIALIZE_PASS_END(VecCloneVP, SV_NAME, LVNAME, false /* modififies CFG */,
                    false /* transform pass */)
