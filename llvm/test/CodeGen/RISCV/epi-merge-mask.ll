; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+v -verify-machineinstrs -O0 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+v -verify-machineinstrs -O2 < %s \
; RUN:    | FileCheck --check-prefix=CHECK-O2 %s

declare i64 @llvm.epi.vsetvl(i64, i64, i64)

declare <vscale x 1 x double> @llvm.epi.vload.v1f64(
  <vscale x 1 x double>*,
  i64)

declare <vscale x 1 x i1> @llvm.epi.vmflt.v1i1.v1f64.v1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64)

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.v1f64.v1f64.v1i1(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64)

declare void @llvm.epi.vstore.v1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64)

define void @merge_mask(i64 %vl, double* %c, double* %a, double* %b) nounwind {
; CHECK-O0-LABEL: merge_mask:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvtype a6
; CHECK-O0-NEXT:    rdvl a5
; CHECK-O0-NEXT:    vsetvli a4, zero, e8,m1
; CHECK-O0-NEXT:    vsetvl zero, a5, a6
; CHECK-O0-NEXT:    sub sp, sp, a4
; CHECK-O0-NEXT:    andi sp, sp, -16
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    vsetvli a0, a0, e64,m1
; CHECK-O0-NEXT:    vle.v v0, (a2)
; CHECK-O0-NEXT:    vle.v v1, (a3)
; CHECK-O0-NEXT:    vmflt.vv v2, v0, v1
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a0)
; CHECK-O0-NEXT:    rdvtype ra
; CHECK-O0-NEXT:    rdvl t0
; CHECK-O0-NEXT:    vsetvli t1, zero, e64,m1
; CHECK-O0-NEXT:    vmv.v.v v0, v2
; CHECK-O0-NEXT:    vsetvl zero, t0, ra
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v2, (a0)
; CHECK-O0-NEXT:    vfsub.vv v1, v1, v2, v0.t
; CHECK-O0-NEXT:    vse.v v1, (a1)
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: merge_mask:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    vsetvli a0, a0, e64,m1
; CHECK-O2-NEXT:    vle.v v1, (a2)
; CHECK-O2-NEXT:    vle.v v2, (a3)
; CHECK-O2-NEXT:    vmflt.vv v0, v1, v2
; CHECK-O2-NEXT:    vfsub.vv v2, v2, v1, v0.t
; CHECK-O2-NEXT:    vse.v v2, (a1)
; CHECK-O2-NEXT:    ret
entry:

  %gvl = call i64 @llvm.epi.vsetvl(i64 %vl, i64 3, i64 0)

  %addr_a = bitcast double* %a to <vscale x 1 x double>*
  %vec_a = call <vscale x 1 x double> @llvm.epi.vload.v1f64(
    <vscale x 1 x double>* %addr_a,
    i64 %gvl)

  %addr_b = bitcast double* %b to <vscale x 1 x double>*
  %vec_b = call <vscale x 1 x double> @llvm.epi.vload.v1f64(
    <vscale x 1 x double>* %addr_b,
    i64 %gvl)

  %cmp = call <vscale x 1 x i1> @llvm.epi.vmflt.v1i1.v1f64.v1f64(
    <vscale x 1 x double> %vec_a,
    <vscale x 1 x double> %vec_b,
    i64 %gvl)

  %sub = call <vscale x 1 x double> @llvm.epi.vfsub.mask.v1f64.v1f64.v1i1(
    <vscale x 1 x double> %vec_b,
    <vscale x 1 x double> %vec_b,
    <vscale x 1 x double> %vec_a,
    <vscale x 1 x i1> %cmp,
    i64 %gvl)

  %addr_c = bitcast double* %c to <vscale x 1 x double>*
  call void @llvm.epi.vstore.v1f64(
    <vscale x 1 x double> %sub,
    <vscale x 1 x double>* %addr_c,
    i64 %gvl)

  ret void
}
