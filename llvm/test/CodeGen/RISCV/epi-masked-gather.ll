; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+experimental-v < %s | FileCheck %s

define <vscale x 1 x i64> @nxv1i64(i64* %ptr, <vscale x 1 x i64> %offsets, <vscale x 1 x i1> %mask, <vscale x 1 x i64> %passthru) {
; CHECK-LABEL: nxv1i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e64,m1
; CHECK-NEXT:    vsll.vx v1, v16, 3
; CHECK-NEXT:    vlxe.v v17, (a0), v1, v0.t
; CHECK-NEXT:    vmv1r.v v16, v17
; CHECK-NEXT:    ret
  %1 = getelementptr i64, i64* %ptr, <vscale x 1 x i64> %offsets
  %2 = call <vscale x 1 x i64> @llvm.masked.gather.nxv1i64.nxv1p0i64(<vscale x 1 x i64*> %1, i32 8, <vscale x 1 x i1> %mask, <vscale x 1 x i64> %passthru)
  ret <vscale x 1 x i64> %2
}

define <vscale x 2 x float> @nxv2f32(float* %ptr, <vscale x 2 x i32> %offsets, <vscale x 2 x i1> %mask, <vscale x 2 x float> %passthru) {
; CHECK-LABEL: nxv2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e32,m1
; CHECK-NEXT:    vsll.vx v1, v16, 2
; CHECK-NEXT:    vlxe.v v17, (a0), v1, v0.t
; CHECK-NEXT:    vmv1r.v v16, v17
; CHECK-NEXT:    ret
  %1 = getelementptr float, float* %ptr, <vscale x 2 x i32> %offsets
  %2 = call <vscale x 2 x float> @llvm.masked.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*> %1, i32 4, <vscale x 2 x i1> %mask, <vscale x 2 x float> %passthru)
  ret <vscale x 2 x float> %2
}

define <vscale x 16 x i8> @nxv16i8(i8* %ptr, <vscale x 16 x i8> %offsets, <vscale x 16 x i1> %mask, <vscale x 16 x i8> %passthru) {
; CHECK-LABEL: nxv16i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e8,m2
; CHECK-NEXT:    vsll.vx v2, v16, 0
; CHECK-NEXT:    vlxe.v v18, (a0), v2, v0.t
; CHECK-NEXT:    vmv2r.v v16, v18
; CHECK-NEXT:    ret
  %1 = getelementptr i8, i8* %ptr, <vscale x 16 x i8> %offsets
  %2 = call <vscale x 16 x i8> @llvm.masked.gather.nxv16i8.nxv16p0i8(<vscale x 16 x i8*> %1, i32 1, <vscale x 16 x i1> %mask, <vscale x 16 x i8> %passthru)
  ret <vscale x 16 x i8> %2
}

define <vscale x 16 x i16> @nxv16i16(i16* %ptr, <vscale x 16 x i16> %offsets, <vscale x 16 x i1> %mask, <vscale x 16 x i16> %passthru) {
; CHECK-LABEL: nxv16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e16,m4
; CHECK-NEXT:    vsll.vx v4, v16, 1
; CHECK-NEXT:    vlxe.v v20, (a0), v4, v0.t
; CHECK-NEXT:    vmv4r.v v16, v20
; CHECK-NEXT:    ret
  %1 = getelementptr i16, i16* %ptr, <vscale x 16 x i16> %offsets
  %2 = call <vscale x 16 x i16> @llvm.masked.gather.nxv16i16.nxv16p0i16(<vscale x 16 x i16*> %1, i32 2, <vscale x 16 x i1> %mask, <vscale x 16 x i16> %passthru)
  ret <vscale x 16 x i16> %2
}

define <vscale x 8 x double> @nxv8f64(double* %ptr, <vscale x 8 x i64> %offsets, <vscale x 8 x i1> %mask, <vscale x 8 x double> %passthru) {
; CHECK-LABEL: nxv8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a2, zero, e64,m8
; CHECK-NEXT:    vle.v v8, (a1)
; CHECK-NEXT:    vsll.vx v24, v16, 3
; CHECK-NEXT:    vlxe.v v8, (a0), v24, v0.t
; CHECK-NEXT:    vmv8r.v v16, v8
; CHECK-NEXT:    ret
  %1 = getelementptr double, double* %ptr, <vscale x 8 x i64> %offsets
  %2 = call <vscale x 8 x double> @llvm.masked.gather.nxv8f64.nxv8p0f64(<vscale x 8 x double*> %1, i32 8, <vscale x 8 x i1> %mask, <vscale x 8 x double> %passthru)
  ret <vscale x 8 x double> %2
}

; LMUL=1
declare <vscale x 1 x i64> @llvm.masked.gather.nxv1i64.nxv1p0i64(<vscale x 1 x i64*>, i32, <vscale x 1 x i1>, <vscale x 1 x i64>)
declare <vscale x 2 x float> @llvm.masked.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*>, i32, <vscale x 2 x i1>, <vscale x 2 x float>)

; LMUL>1
declare <vscale x 16 x i8> @llvm.masked.gather.nxv16i8.nxv16p0i8(<vscale x 16 x i8*>, i32, <vscale x 16 x i1>, <vscale x 16 x i8>)
declare <vscale x 16 x i16> @llvm.masked.gather.nxv16i16.nxv16p0i16(<vscale x 16 x i16*>, i32, <vscale x 16 x i1>, <vscale x 16 x i16>)
declare <vscale x 8 x double> @llvm.masked.gather.nxv8f64.nxv8p0f64(<vscale x 8 x double*>, i32, <vscale x 8 x i1>, <vscale x 8 x double>)
