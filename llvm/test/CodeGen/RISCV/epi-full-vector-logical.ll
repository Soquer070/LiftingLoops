; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+experimental-v < %s | FileCheck %s

@scratch = global i8 0, align 16

define void @nxv1i1(<vscale x 1 x i1> %a, <vscale x 1 x i1> %b) nounwind {
; CHECK-LABEL: nxv1i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1
; CHECK-NEXT:    vmand.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1
; CHECK-NEXT:    vmor.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1
; CHECK-NEXT:    vmxor.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i64>*

  %log_1 = and <vscale x 1 x i1> %a, %b
  %val_1 = zext <vscale x 1 x i1> %log_1 to <vscale x 1 x i64>
  store <vscale x 1 x i64> %val_1, <vscale x 1 x i64>* %store_addr

  %log_2 = or <vscale x 1 x i1> %a, %b
  %val_2 = zext <vscale x 1 x i1> %log_2 to <vscale x 1 x i64>
  store <vscale x 1 x i64> %val_2, <vscale x 1 x i64>* %store_addr

  %log_3 = xor <vscale x 1 x i1> %a, %b
  %val_3 = zext <vscale x 1 x i1> %log_3 to <vscale x 1 x i64>
  store <vscale x 1 x i64> %val_3, <vscale x 1 x i64>* %store_addr

  ret void
}

define void @nxv2i1(<vscale x 2 x i1> %a, <vscale x 2 x i1> %b) nounwind {
; CHECK-LABEL: nxv2i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e32,m1
; CHECK-NEXT:    vmand.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m1
; CHECK-NEXT:    vmor.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m1
; CHECK-NEXT:    vmxor.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i32>*

  %log_1 = and <vscale x 2 x i1> %a, %b
  %val_1 = zext <vscale x 2 x i1> %log_1 to <vscale x 2 x i32>
  store <vscale x 2 x i32> %val_1, <vscale x 2 x i32>* %store_addr

  %log_2 = or <vscale x 2 x i1> %a, %b
  %val_2 = zext <vscale x 2 x i1> %log_2 to <vscale x 2 x i32>
  store <vscale x 2 x i32> %val_2, <vscale x 2 x i32>* %store_addr

  %log_3 = xor <vscale x 2 x i1> %a, %b
  %val_3 = zext <vscale x 2 x i1> %log_3 to <vscale x 2 x i32>
  store <vscale x 2 x i32> %val_3, <vscale x 2 x i32>* %store_addr

  ret void
}

define void @nxv4i1(<vscale x 4 x i1> %a, <vscale x 4 x i1> %b) nounwind {
; CHECK-LABEL: nxv4i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e16,m1
; CHECK-NEXT:    vmand.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m1
; CHECK-NEXT:    vmor.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m1
; CHECK-NEXT:    vmxor.mm v1, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 4 x i16>*

  %log_1 = and <vscale x 4 x i1> %a, %b
  %val_1 = zext <vscale x 4 x i1> %log_1 to <vscale x 4 x i16>
  store <vscale x 4 x i16> %val_1, <vscale x 4 x i16>* %store_addr

  %log_2 = or <vscale x 4 x i1> %a, %b
  %val_2 = zext <vscale x 4 x i1> %log_2 to <vscale x 4 x i16>
  store <vscale x 4 x i16> %val_2, <vscale x 4 x i16>* %store_addr

  %log_3 = xor <vscale x 4 x i1> %a, %b
  %val_3 = zext <vscale x 4 x i1> %log_3 to <vscale x 4 x i16>
  store <vscale x 4 x i16> %val_3, <vscale x 4 x i16>* %store_addr

  ret void
}

define void @nxv8i1(<vscale x 8 x i1> %a, <vscale x 8 x i1> %b) nounwind {
; CHECK-LABEL: nxv8i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e8,m1
; CHECK-NEXT:    vmand.mm v1, v0, v16
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vmor.mm v1, v0, v16
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    vmxor.mm v1, v0, v16
; CHECK-NEXT:    vse.v v1, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 8 x i8>*

  %log_1 = and <vscale x 8 x i1> %a, %b
  %val_1 = zext <vscale x 8 x i1> %log_1 to <vscale x 8 x i8>
  store <vscale x 8 x i8> %val_1, <vscale x 8 x i8>* %store_addr

  %log_2 = or <vscale x 8 x i1> %a, %b
  %val_2 = zext <vscale x 8 x i1> %log_2 to <vscale x 8 x i8>
  store <vscale x 8 x i8> %val_2, <vscale x 8 x i8>* %store_addr

  %log_3 = xor <vscale x 8 x i1> %a, %b
  %val_3 = zext <vscale x 8 x i1> %log_3 to <vscale x 8 x i8>
  store <vscale x 8 x i8> %val_3, <vscale x 8 x i8>* %store_addr

  ret void
}

; FIXME: Enable the following tests when storing whole LMUL > 1 masks is
; supported.

;define void @nxv16i1(<vscale x 16 x i1> %a, <vscale x 16 x i1> %b) nounwind {
;  %store_addr = bitcast i8* @scratch to <vscale x 16 x i4>*
;
;  %log_1 = and <vscale x 16 x i1> %a, %b
;  %val_1 = zext <vscale x 16 x i1> %log_1 to <vscale x 16 x i4>
;  store <vscale x 16 x i4> %val_1, <vscale x 16 x i4>* %store_addr
;
;  %log_2 = or <vscale x 16 x i1> %a, %b
;  %val_2 = zext <vscale x 16 x i1> %log_2 to <vscale x 16 x i4>
;  store <vscale x 16 x i4> %val_2, <vscale x 16 x i4>* %store_addr
;
;  %log_3 = xor <vscale x 16 x i1> %a, %b
;  %val_3 = zext <vscale x 16 x i1> %log_3 to <vscale x 16 x i4>
;  store <vscale x 16 x i4> %val_3, <vscale x 16 x i4>* %store_addr
;
;  ret void
;}

;define void @nxv32i1(<vscale x 32 x i1> %a, <vscale x 32 x i1> %b) nounwind {
;  %store_addr = bitcast i8* @scratch to <vscale x 32 x i2>*
;
;  %log_1 = and <vscale x 32 x i1> %a, %b
;  %val_1 = zext <vscale x 32 x i1> %log_1 to <vscale x 32 x i2>
;  store <vscale x 32 x i2> %val_1, <vscale x 32 x i2>* %store_addr
;
;  %log_2 = or <vscale x 32 x i1> %a, %b
;  %val_2 = zext <vscale x 32 x i1> %log_2 to <vscale x 32 x i2>
;  store <vscale x 32 x i2> %val_2, <vscale x 32 x i2>* %store_addr
;
;  %log_3 = xor <vscale x 32 x i1> %a, %b
;  %val_3 = zext <vscale x 32 x i1> %log_3 to <vscale x 32 x i2>
;  store <vscale x 32 x i2> %val_3, <vscale x 32 x i2>* %store_addr
;
;  ret void
;}

;define void @nxv64i1(<vscale x 64 x i1> %a, <vscale x 64 x i1> %b) nounwind {
;  %store_addr = bitcast i8* @scratch to <vscale x 64 x i1>*
;
;  %log_1 = and <vscale x 64 x i1> %a, %b
;  store <vscale x 64 x i1> %log_1, <vscale x 64 x i1>* %store_addr
;
;  %log_2 = or <vscale x 64 x i1> %a, %b
;  store <vscale x 64 x i1> %log_2, <vscale x 64 x i1>* %store_addr
;
;  %log_3 = xor <vscale x 64 x i1> %a, %b
;  store <vscale x 64 x i1> %log_3, <vscale x 64 x i1>* %store_addr
;
;  ret void
;}
