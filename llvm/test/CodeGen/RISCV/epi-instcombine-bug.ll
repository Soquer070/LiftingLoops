; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+epi -verify-machineinstrs < %s \
; RUN:    | FileCheck %s

%struct.timeval = type { i64, i64 }
%struct.timezone = type { i32, i32 }

@.str = private unnamed_addr constant [20 x i8] c"\0A\0A RISCV - Serial \0A\00", align 1
@__const.main.input = private unnamed_addr constant [8 x double] [double 1.000000e+00, double 2.000000e+00, double 3.000000e+00, double 4.000000e+00, double 5.000000e+00, double 6.000000e+00, double 7.000000e+00, double 8.000000e+00], align 8
@.str.1 = private unnamed_addr constant [34 x i8] c"\0A\0ALog Kernel took %8.8lf secs   \0A\00", align 1
@.str.2 = private unnamed_addr constant [33 x i8] c"i = %i, x = %0.15f, y = %0.15f \0A\00", align 1
@.str.3 = private unnamed_addr constant [20 x i8] c"\0A\0A RISCV - Vector \0A\00", align 1

; Function Attrs: nounwind
define dso_local <vscale x 1 x double> @_Z10log_vectorDv1_dm(<vscale x 1 x double> %x, i64 %gvl) #0 {
; CHECK-LABEL: _Z10log_vectorDv1_dm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -192
; CHECK-NEXT:    sd ra, 184(sp)
; CHECK-NEXT:    sd s0, 176(sp)
; CHECK-NEXT:    sd s1, 168(sp)
; CHECK-NEXT:    sd s2, 160(sp)
; CHECK-NEXT:    sd s3, 152(sp)
; CHECK-NEXT:    sd s4, 144(sp)
; CHECK-NEXT:    sd s5, 136(sp)
; CHECK-NEXT:    sd s6, 128(sp)
; CHECK-NEXT:    sd s7, 120(sp)
; CHECK-NEXT:    sd s8, 112(sp)
; CHECK-NEXT:    sd s9, 104(sp)
; CHECK-NEXT:    sd s10, 96(sp)
; CHECK-NEXT:    sd s11, 88(sp)
; CHECK-NEXT:    addi s0, sp, 192
; CHECK-NEXT:    andi sp, sp, -64
; CHECK-NEXT:    add s1, zero, sp
; CHECK-NEXT:    sd a0, 72(s1)
; CHECK-NEXT:    vsetvli a0, zero, e64, m1
; CHECK-NEXT:    lui a0, 0
; CHECK-NEXT:    addi a0, a0, 80
; CHECK-NEXT:    add a0, a0, s1
; CHECK-NEXT:    vse.v v16, (a0)
; CHECK-NEXT:    rdvl s4
; CHECK-NEXT:    slli a0, s4, 3
; CHECK-NEXT:    addi a0, a0, 15
; CHECK-NEXT:    andi a0, a0, -16
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi s2, a1, -64
; CHECK-NEXT:    add sp, zero, s2
; CHECK-NEXT:    lui a1, %hi(.LCPI0_0)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_0)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s2)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_1)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_1)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi a3, a1, -64
; CHECK-NEXT:    sd a3, 56(s1)
; CHECK-NEXT:    add sp, zero, a3
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a3)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_2)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_2)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi a7, a1, -64
; CHECK-NEXT:    add sp, zero, a7
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a7)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_3)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_3)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi t0, a1, -64
; CHECK-NEXT:    add sp, zero, t0
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (t0)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_4)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_4)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi t1, a1, -64
; CHECK-NEXT:    add sp, zero, t1
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (t1)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_5)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_5)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi t2, a1, -64
; CHECK-NEXT:    add sp, zero, t2
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (t2)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_6)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_6)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi t3, a1, -64
; CHECK-NEXT:    add sp, zero, t3
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (t3)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_7)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_7)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi t4, a1, -64
; CHECK-NEXT:    add sp, zero, t4
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (t4)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_8)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_8)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi t5, a1, -64
; CHECK-NEXT:    add sp, zero, t5
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (t5)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_9)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_9)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi t6, a1, -64
; CHECK-NEXT:    add sp, zero, t6
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (t6)
; CHECK-NEXT:    addi a1, zero, 1
; CHECK-NEXT:    slli a1, a1, 52
; CHECK-NEXT:    sub a2, sp, a0
; CHECK-NEXT:    andi a2, a2, -64
; CHECK-NEXT:    add sp, zero, a2
; CHECK-NEXT:    ld a3, 72(s1)
; CHECK-NEXT:    vsetvli a4, a3, e64, m1
; CHECK-NEXT:    vmv.v.x v0, a1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi s10, a1, -64
; CHECK-NEXT:    add sp, zero, s10
; CHECK-NEXT:    vle.v v0, (a2)
; CHECK-NEXT:    vse.v v0, (s10)
; CHECK-NEXT:    addi a2, zero, -2047
; CHECK-NEXT:    slli a2, a2, 52
; CHECK-NEXT:    addi a3, a2, -1
; CHECK-NEXT:    sub a2, sp, a0
; CHECK-NEXT:    andi s7, a2, -64
; CHECK-NEXT:    add sp, zero, s7
; CHECK-NEXT:    sub a2, sp, a0
; CHECK-NEXT:    andi s11, a2, -64
; CHECK-NEXT:    add sp, zero, s11
; CHECK-NEXT:    ld a4, 72(s1)
; CHECK-NEXT:    vsetvli a5, a4, e64, m1
; CHECK-NEXT:    vmv.v.x v0, a3
; CHECK-NEXT:    vsetvli a3, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s11)
; CHECK-NEXT:    sub a3, sp, a0
; CHECK-NEXT:    andi s9, a3, -64
; CHECK-NEXT:    add sp, zero, s9
; CHECK-NEXT:    addi a3, zero, 1023
; CHECK-NEXT:    ld a4, 72(s1)
; CHECK-NEXT:    vsetvli a5, a4, e64, m1
; CHECK-NEXT:    vmv.v.x v0, a3
; CHECK-NEXT:    vsetvli a3, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s9)
; CHECK-NEXT:    lui a3, %hi(.LCPI0_10)
; CHECK-NEXT:    addi a3, a3, %lo(.LCPI0_10)
; CHECK-NEXT:    fld ft0, 0(a3)
; CHECK-NEXT:    sub a3, sp, a0
; CHECK-NEXT:    andi a5, a3, -64
; CHECK-NEXT:    add sp, zero, a5
; CHECK-NEXT:    sub a3, sp, a0
; CHECK-NEXT:    andi a3, a3, -64
; CHECK-NEXT:    add sp, zero, a3
; CHECK-NEXT:    ld a4, 72(s1)
; CHECK-NEXT:    vsetvli a1, a4, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a3)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_11)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_11)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi s6, a1, -64
; CHECK-NEXT:    add sp, zero, s6
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s6)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_12)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_12)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi s5, a1, -64
; CHECK-NEXT:    add sp, zero, s5
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s5)
; CHECK-NEXT:    lui a1, %hi(.LCPI0_13)
; CHECK-NEXT:    addi a1, a1, %lo(.LCPI0_13)
; CHECK-NEXT:    fld ft0, 0(a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi s3, a1, -64
; CHECK-NEXT:    add sp, zero, s3
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s3)
; CHECK-NEXT:    addi a1, s4, 15
; CHECK-NEXT:    lui a4, %hi(.LCPI0_14)
; CHECK-NEXT:    addi a4, a4, %lo(.LCPI0_14)
; CHECK-NEXT:    fld ft0, 0(a4)
; CHECK-NEXT:    andi a6, a1, -16
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi s8, a1, -64
; CHECK-NEXT:    add sp, zero, s8
; CHECK-NEXT:    sub s4, sp, a6
; CHECK-NEXT:    add sp, zero, s4
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmv.v.f v0, ft0
; CHECK-NEXT:    vsetvli a2, zero, e64, m1
; CHECK-NEXT:    lui a2, 0
; CHECK-NEXT:    addi a2, a2, 80
; CHECK-NEXT:    add a2, a2, s1
; CHECK-NEXT:    vle.v v1, (a2)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vmfle.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e8, m1
; CHECK-NEXT:    vse.v v0, (s4)
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vle.v v0, (s10)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a2, 0
; CHECK-NEXT:    addi a2, a2, 80
; CHECK-NEXT:    add a2, a2, s1
; CHECK-NEXT:    vle.v v1, (a2)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfmax.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a2, zero, e64, m1
; CHECK-NEXT:    lui a2, 0
; CHECK-NEXT:    addi a2, a2, 80
; CHECK-NEXT:    add a2, a2, s1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    addi a2, zero, 52
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vmv.v.x v0, a2
; CHECK-NEXT:    vsetvli a2, zero, e64, m1
; CHECK-NEXT:    lui a2, 0
; CHECK-NEXT:    addi a2, a2, 80
; CHECK-NEXT:    add a2, a2, s1
; CHECK-NEXT:    vle.v v1, (a2)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vsrl.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a5)
; CHECK-NEXT:    vle.v v0, (s11)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a2, 0
; CHECK-NEXT:    addi a2, a2, 80
; CHECK-NEXT:    add a2, a2, s1
; CHECK-NEXT:    vle.v v1, (a2)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vand.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s7)
; CHECK-NEXT:    vle.v v1, (s6)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vor.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s7)
; CHECK-NEXT:    lui a1, 0
; CHECK-NEXT:    addi a1, a1, 80
; CHECK-NEXT:    add a1, a1, s1
; CHECK-NEXT:    vse.v v0, (a1)
; CHECK-NEXT:    vle.v v0, (s9)
; CHECK-NEXT:    vle.v v1, (a5)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vsub.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a5)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfcvt.f.x.v v0, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s8)
; CHECK-NEXT:    vle.v v1, (a3)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a2, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s8)
; CHECK-NEXT:    sub a1, sp, a6
; CHECK-NEXT:    add sp, zero, a1
; CHECK-NEXT:    vle.v v0, (s2)
; CHECK-NEXT:    ld a2, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a2, e64, m1
; CHECK-NEXT:    vmflt.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a2, zero, e8, m1
; CHECK-NEXT:    vse.v v0, (a1)
; CHECK-NEXT:    sub a2, sp, a0
; CHECK-NEXT:    andi a2, a2, -64
; CHECK-NEXT:    add sp, zero, a2
; CHECK-NEXT:    ld a4, 72(s1)
; CHECK-NEXT:    vsetvli a5, a4, e64, m1
; CHECK-NEXT:    vfmv.v.f v1, ft0
; CHECK-NEXT:    vsetvli a5, zero, e8, m1
; CHECK-NEXT:    vle.v v0, (a1)
; CHECK-NEXT:    vsetvli a5, zero, e64, m1
; CHECK-NEXT:    lui a5, 0
; CHECK-NEXT:    addi a5, a5, 80
; CHECK-NEXT:    add a5, a5, s1
; CHECK-NEXT:    vle.v v2, (a5)
; CHECK-NEXT:    vsetvli a5, a4, e64, m1
; CHECK-NEXT:    vmerge.vvm v0, v1, v2, v0
; CHECK-NEXT:    vsetvli a4, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    vle.v v0, (a3)
; CHECK-NEXT:    ld a4, 72(s1)
; CHECK-NEXT:    lui a5, 0
; CHECK-NEXT:    addi a5, a5, 80
; CHECK-NEXT:    add a5, a5, s1
; CHECK-NEXT:    vle.v v1, (a5)
; CHECK-NEXT:    vsetvli a5, a4, e64, m1
; CHECK-NEXT:    vfsub.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a5, zero, e64, m1
; CHECK-NEXT:    lui a5, 0
; CHECK-NEXT:    addi a5, a5, 80
; CHECK-NEXT:    add a5, a5, s1
; CHECK-NEXT:    vse.v v0, (a5)
; CHECK-NEXT:    vsetvli a5, a4, e64, m1
; CHECK-NEXT:    vfmv.v.f v1, ft0
; CHECK-NEXT:    vsetvli a5, zero, e8, m1
; CHECK-NEXT:    vle.v v0, (a1)
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vle.v v2, (a3)
; CHECK-NEXT:    vsetvli a1, a4, e64, m1
; CHECK-NEXT:    vmerge.vvm v0, v1, v2, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vle.v v1, (s8)
; CHECK-NEXT:    vsetvli a1, a4, e64, m1
; CHECK-NEXT:    vfsub.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (s8)
; CHECK-NEXT:    vle.v v0, (a2)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a3, 0
; CHECK-NEXT:    addi a3, a3, 80
; CHECK-NEXT:    add a3, a3, s1
; CHECK-NEXT:    vle.v v1, (a3)
; CHECK-NEXT:    vsetvli a3, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    lui a1, 0
; CHECK-NEXT:    addi a1, a1, 80
; CHECK-NEXT:    add a1, a1, s1
; CHECK-NEXT:    vse.v v0, (a1)
; CHECK-NEXT:    sub a1, sp, a0
; CHECK-NEXT:    andi a3, a1, -64
; CHECK-NEXT:    add sp, zero, a3
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v0, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a3)
; CHECK-NEXT:    sub a0, sp, a0
; CHECK-NEXT:    andi a0, a0, -64
; CHECK-NEXT:    add sp, zero, a0
; CHECK-NEXT:    ld a1, 56(s1)
; CHECK-NEXT:    vle.v v0, (a1)
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (a7)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (t0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (t1)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (t2)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (t3)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (t4)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (t5)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (t6)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    lui a4, 0
; CHECK-NEXT:    addi a4, a4, 80
; CHECK-NEXT:    add a4, a4, s1
; CHECK-NEXT:    vle.v v1, (a4)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v1, (a3)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v0, (s5)
; CHECK-NEXT:    vle.v v1, (s8)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    vle.v v1, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a4, a1, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v0, (s6)
; CHECK-NEXT:    vle.v v1, (a3)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a3, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    vle.v v1, (a0)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a3, a1, e64, m1
; CHECK-NEXT:    vfsub.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    vle.v v0, (s3)
; CHECK-NEXT:    vle.v v1, (s8)
; CHECK-NEXT:    ld a1, 72(s1)
; CHECK-NEXT:    vsetvli a3, a1, e64, m1
; CHECK-NEXT:    vfmul.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    vse.v v0, (a2)
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    ld a0, 72(s1)
; CHECK-NEXT:    lui a1, 0
; CHECK-NEXT:    addi a1, a1, 80
; CHECK-NEXT:    add a1, a1, s1
; CHECK-NEXT:    vle.v v1, (a1)
; CHECK-NEXT:    vsetvli a1, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v0, v1, v0
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    lui a1, 0
; CHECK-NEXT:    addi a1, a1, 80
; CHECK-NEXT:    add a1, a1, s1
; CHECK-NEXT:    vse.v v0, (a1)
; CHECK-NEXT:    vle.v v1, (a2)
; CHECK-NEXT:    vsetvli a1, a0, e64, m1
; CHECK-NEXT:    vfadd.vv v1, v0, v1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1
; CHECK-NEXT:    lui a1, 0
; CHECK-NEXT:    addi a1, a1, 80
; CHECK-NEXT:    add a1, a1, s1
; CHECK-NEXT:    vse.v v1, (a1)
; CHECK-NEXT:    vsetvli a1, zero, e8, m1
; CHECK-NEXT:    vle.v v0, (s4)
; CHECK-NEXT:    vsetvli a1, a0, e64, m1
; CHECK-NEXT:    vmv.v.i v2, -1
; CHECK-NEXT:    vmerge.vvm v16, v1, v2, v0
; CHECK-NEXT:    vsetvli a0, zero, e64, m1
; CHECK-NEXT:    lui a0, 0
; CHECK-NEXT:    addi a0, a0, 80
; CHECK-NEXT:    add a0, a0, s1
; CHECK-NEXT:    vse.v v16, (a0)
; CHECK-NEXT:    addi sp, s0, -192
; CHECK-NEXT:    ld s11, 88(sp)
; CHECK-NEXT:    ld s10, 96(sp)
; CHECK-NEXT:    ld s9, 104(sp)
; CHECK-NEXT:    ld s8, 112(sp)
; CHECK-NEXT:    ld s7, 120(sp)
; CHECK-NEXT:    ld s6, 128(sp)
; CHECK-NEXT:    ld s5, 136(sp)
; CHECK-NEXT:    ld s4, 144(sp)
; CHECK-NEXT:    ld s3, 152(sp)
; CHECK-NEXT:    ld s2, 160(sp)
; CHECK-NEXT:    ld s1, 168(sp)
; CHECK-NEXT:    ld s0, 176(sp)
; CHECK-NEXT:    ld ra, 184(sp)
; CHECK-NEXT:    addi sp, sp, 192
; CHECK-NEXT:    ret
entry:
  %x.addr = alloca <vscale x 1 x double>, align 8
  %gvl.addr = alloca i64, align 8
  store <vscale x 1 x double> %x, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  store i64 %gvl, i64* %gvl.addr, align 8, !tbaa !5
  %0 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_SQRTHF = alloca double, i64 %0, align 64
  %1 = bitcast double* %_ps256_cephes_SQRTHF to <vscale x 1 x double>*
  %2 = bitcast double* %_ps256_cephes_SQRTHF to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %2) #6
  %3 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %4 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FE6A09E667F3BCD, i64 %3)
  store <vscale x 1 x double> %4, <vscale x 1 x double>* %1, align 64, !tbaa !2
  %5 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p0 = alloca double, i64 %5, align 64
  %6 = bitcast double* %_ps256_cephes_log_p0 to <vscale x 1 x double>*
  %7 = bitcast double* %_ps256_cephes_log_p0 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %7) #6
  %8 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %9 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FB204376245245A, i64 %8)
  store <vscale x 1 x double> %9, <vscale x 1 x double>* %6, align 64, !tbaa !2
  %10 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p1 = alloca double, i64 %10, align 64
  %11 = bitcast double* %_ps256_cephes_log_p1 to <vscale x 1 x double>*
  %12 = bitcast double* %_ps256_cephes_log_p1 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %12) #6
  %13 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %14 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0xBFBD7A370B138B4B, i64 %13)
  store <vscale x 1 x double> %14, <vscale x 1 x double>* %11, align 64, !tbaa !2
  %15 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p2 = alloca double, i64 %15, align 64
  %16 = bitcast double* %_ps256_cephes_log_p2 to <vscale x 1 x double>*
  %17 = bitcast double* %_ps256_cephes_log_p2 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %17) #6
  %18 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %19 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FBDE4A34D098E98, i64 %18)
  store <vscale x 1 x double> %19, <vscale x 1 x double>* %16, align 64, !tbaa !2
  %20 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p3 = alloca double, i64 %20, align 64
  %21 = bitcast double* %_ps256_cephes_log_p3 to <vscale x 1 x double>*
  %22 = bitcast double* %_ps256_cephes_log_p3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %22) #6
  %23 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %24 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0xBFBFCBA9DB73ED2C, i64 %23)
  store <vscale x 1 x double> %24, <vscale x 1 x double>* %21, align 64, !tbaa !2
  %25 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p4 = alloca double, i64 %25, align 64
  %26 = bitcast double* %_ps256_cephes_log_p4 to <vscale x 1 x double>*
  %27 = bitcast double* %_ps256_cephes_log_p4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %27) #6
  %28 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %29 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FC23D37D4CD3339, i64 %28)
  store <vscale x 1 x double> %29, <vscale x 1 x double>* %26, align 64, !tbaa !2
  %30 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p5 = alloca double, i64 %30, align 64
  %31 = bitcast double* %_ps256_cephes_log_p5 to <vscale x 1 x double>*
  %32 = bitcast double* %_ps256_cephes_log_p5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %32) #6
  %33 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %34 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0xBFC555CA04CB8ABB, i64 %33)
  store <vscale x 1 x double> %34, <vscale x 1 x double>* %31, align 64, !tbaa !2
  %35 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p6 = alloca double, i64 %35, align 64
  %36 = bitcast double* %_ps256_cephes_log_p6 to <vscale x 1 x double>*
  %37 = bitcast double* %_ps256_cephes_log_p6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %37) #6
  %38 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %39 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FC999D58F0FBE3E, i64 %38)
  store <vscale x 1 x double> %39, <vscale x 1 x double>* %36, align 64, !tbaa !2
  %40 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p7 = alloca double, i64 %40, align 64
  %41 = bitcast double* %_ps256_cephes_log_p7 to <vscale x 1 x double>*
  %42 = bitcast double* %_ps256_cephes_log_p7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %42) #6
  %43 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %44 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0xBFCFFFFF7F002B13, i64 %43)
  store <vscale x 1 x double> %44, <vscale x 1 x double>* %41, align 64, !tbaa !2
  %45 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_p8 = alloca double, i64 %45, align 64
  %46 = bitcast double* %_ps256_cephes_log_p8 to <vscale x 1 x double>*
  %47 = bitcast double* %_ps256_cephes_log_p8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %47) #6
  %48 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %49 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FD555553E25CD96, i64 %48)
  store <vscale x 1 x double> %49, <vscale x 1 x double>* %46, align 64, !tbaa !2
  %50 = call i64 @llvm.experimental.vector.vscale.i64()
  %_256_min_norm_pos = alloca i64, i64 %50, align 64
  %51 = bitcast i64* %_256_min_norm_pos to <vscale x 1 x i64>*
  %52 = bitcast i64* %_256_min_norm_pos to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %52) #6
  %53 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %54 = call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 4503599627370496, i64 %53)
  store <vscale x 1 x i64> %54, <vscale x 1 x i64>* %51, align 64, !tbaa !2
  %55 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_min_norm_pos = alloca double, i64 %55, align 64
  %56 = bitcast double* %_ps256_min_norm_pos to <vscale x 1 x double>*
  %57 = bitcast double* %_ps256_min_norm_pos to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %57) #6
  %58 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %51, align 64, !tbaa !2
  %59 = bitcast <vscale x 1 x i64> %58 to <vscale x 1 x double>
  store <vscale x 1 x double> %59, <vscale x 1 x double>* %56, align 64, !tbaa !2
  %60 = call i64 @llvm.experimental.vector.vscale.i64()
  %_x_i = alloca i64, i64 %60, align 64
  %61 = bitcast i64* %_x_i to <vscale x 1 x i64>*
  %62 = bitcast i64* %_x_i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %62) #6
  %63 = call i64 @llvm.experimental.vector.vscale.i64()
  %_256_inv_mant_mask = alloca i64, i64 %63, align 64
  %64 = bitcast i64* %_256_inv_mant_mask to <vscale x 1 x i64>*
  %65 = bitcast i64* %_256_inv_mant_mask to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %65) #6
  %66 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %67 = call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 -9218868437227405313, i64 %66)
  store <vscale x 1 x i64> %67, <vscale x 1 x i64>* %64, align 64, !tbaa !2
  %68 = call i64 @llvm.experimental.vector.vscale.i64()
  %_256_0x7f = alloca i64, i64 %68, align 64
  %69 = bitcast i64* %_256_0x7f to <vscale x 1 x i64>*
  %70 = bitcast i64* %_256_0x7f to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %70) #6
  %71 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %72 = call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 1023, i64 %71)
  store <vscale x 1 x i64> %72, <vscale x 1 x i64>* %69, align 64, !tbaa !2
  %73 = call i64 @llvm.experimental.vector.vscale.i64()
  %imm0 = alloca i64, i64 %73, align 64
  %74 = bitcast i64* %imm0 to <vscale x 1 x i64>*
  %75 = bitcast i64* %imm0 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %75) #6
  %76 = call i64 @llvm.experimental.vector.vscale.i64()
  %one = alloca double, i64 %76, align 64
  %77 = bitcast double* %one to <vscale x 1 x double>*
  %78 = bitcast double* %one to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %78) #6
  %79 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %80 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 1.000000e+00, i64 %79)
  store <vscale x 1 x double> %80, <vscale x 1 x double>* %77, align 64, !tbaa !2
  %81 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_0p5 = alloca double, i64 %81, align 64
  %82 = bitcast double* %_ps256_0p5 to <vscale x 1 x double>*
  %83 = bitcast double* %_ps256_0p5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %83) #6
  %84 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %85 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 5.000000e-01, i64 %84)
  store <vscale x 1 x double> %85, <vscale x 1 x double>* %82, align 64, !tbaa !2
  %86 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_q1 = alloca double, i64 %86, align 64
  %87 = bitcast double* %_ps256_cephes_log_q1 to <vscale x 1 x double>*
  %88 = bitcast double* %_ps256_cephes_log_q1 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %88) #6
  %89 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %90 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0xBF2BD0105C4242EA, i64 %89)
  store <vscale x 1 x double> %90, <vscale x 1 x double>* %87, align 64, !tbaa !2
  %91 = call i64 @llvm.experimental.vector.vscale.i64()
  %_ps256_cephes_log_q2 = alloca double, i64 %91, align 64
  %92 = bitcast double* %_ps256_cephes_log_q2 to <vscale x 1 x double>*
  %93 = bitcast double* %_ps256_cephes_log_q2 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %93) #6
  %94 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %95 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0x3FE6300000000000, i64 %94)
  store <vscale x 1 x double> %95, <vscale x 1 x double>* %92, align 64, !tbaa !2
  %96 = call i64 @llvm.experimental.vector.vscale.i64()
  %e = alloca double, i64 %96, align 64
  %97 = bitcast double* %e to <vscale x 1 x double>*
  %98 = bitcast double* %e to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %98) #6
  %99 = call i64 @llvm.experimental.vector.vscale.i64()
  %invalid_mask = alloca i1, i64 %99, align 1
  %100 = bitcast i1* %invalid_mask to <vscale x 1 x i1>*
  %101 = bitcast i1* %invalid_mask to i8*
  call void @llvm.lifetime.start.p0i8(i64 1, i8* %101) #6
  %102 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %103 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %104 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0.000000e+00, i64 %103)
  %105 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %106 = call <vscale x 1 x i1> @llvm.epi.vmfle.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> %102, <vscale x 1 x double> %104, i64 %105)
  store <vscale x 1 x i1> %106, <vscale x 1 x i1>* %100, align 1, !tbaa !2
  %107 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %108 = load <vscale x 1 x double>, <vscale x 1 x double>* %56, align 64, !tbaa !2
  %109 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %110 = call <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(<vscale x 1 x double> %107, <vscale x 1 x double> %108, i64 %109)
  store <vscale x 1 x double> %110, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %111 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %112 = bitcast <vscale x 1 x double> %111 to <vscale x 1 x i64>
  %113 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %114 = call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 52, i64 %113)
  %115 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %116 = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64> %112, <vscale x 1 x i64> %114, i64 %115)
  store <vscale x 1 x i64> %116, <vscale x 1 x i64>* %74, align 64, !tbaa !2
  %117 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %118 = bitcast <vscale x 1 x double> %117 to <vscale x 1 x i64>
  %119 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %64, align 64, !tbaa !2
  %120 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %121 = call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64> %118, <vscale x 1 x i64> %119, i64 %120)
  store <vscale x 1 x i64> %121, <vscale x 1 x i64>* %61, align 64, !tbaa !2
  %122 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %61, align 64, !tbaa !2
  %123 = load <vscale x 1 x double>, <vscale x 1 x double>* %82, align 64, !tbaa !2
  %124 = bitcast <vscale x 1 x double> %123 to <vscale x 1 x i64>
  %125 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %126 = call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(<vscale x 1 x i64> %122, <vscale x 1 x i64> %124, i64 %125)
  store <vscale x 1 x i64> %126, <vscale x 1 x i64>* %61, align 64, !tbaa !2
  %127 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %61, align 64, !tbaa !2
  %128 = bitcast <vscale x 1 x i64> %127 to <vscale x 1 x double>
  store <vscale x 1 x double> %128, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %129 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %74, align 64, !tbaa !2
  %130 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %69, align 64, !tbaa !2
  %131 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %132 = call <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(<vscale x 1 x i64> %129, <vscale x 1 x i64> %130, i64 %131)
  store <vscale x 1 x i64> %132, <vscale x 1 x i64>* %74, align 64, !tbaa !2
  %133 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %74, align 64, !tbaa !2
  %134 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %135 = call <vscale x 1 x double> @llvm.epi.vfcvt.f.x.nxv1f64.nxv1i64(<vscale x 1 x i64> %133, i64 %134)
  store <vscale x 1 x double> %135, <vscale x 1 x double>* %97, align 64, !tbaa !2
  %136 = load <vscale x 1 x double>, <vscale x 1 x double>* %97, align 64, !tbaa !2
  %137 = load <vscale x 1 x double>, <vscale x 1 x double>* %77, align 64, !tbaa !2
  %138 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %139 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %136, <vscale x 1 x double> %137, i64 %138)
  store <vscale x 1 x double> %139, <vscale x 1 x double>* %97, align 64, !tbaa !2
  %140 = call i64 @llvm.experimental.vector.vscale.i64()
  %mask = alloca i1, i64 %140, align 1
  %141 = bitcast i1* %mask to <vscale x 1 x i1>*
  %142 = bitcast i1* %mask to i8*
  call void @llvm.lifetime.start.p0i8(i64 1, i8* %142) #6
  %143 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %144 = load <vscale x 1 x double>, <vscale x 1 x double>* %1, align 64, !tbaa !2
  %145 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %146 = call <vscale x 1 x i1> @llvm.epi.vmflt.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> %143, <vscale x 1 x double> %144, i64 %145)
  store <vscale x 1 x i1> %146, <vscale x 1 x i1>* %141, align 1, !tbaa !2
  %147 = call i64 @llvm.experimental.vector.vscale.i64()
  %tmp = alloca double, i64 %147, align 64
  %148 = bitcast double* %tmp to <vscale x 1 x double>*
  %149 = bitcast double* %tmp to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %149) #6
  %150 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %151 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0.000000e+00, i64 %150)
  %152 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %153 = load <vscale x 1 x i1>, <vscale x 1 x i1>* %141, align 1, !tbaa !2
  %154 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %155 = call <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %151, <vscale x 1 x double> %152, <vscale x 1 x i1> %153, i64 %154)
  store <vscale x 1 x double> %155, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %156 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %157 = load <vscale x 1 x double>, <vscale x 1 x double>* %77, align 64, !tbaa !2
  %158 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %159 = call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %156, <vscale x 1 x double> %157, i64 %158)
  store <vscale x 1 x double> %159, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %160 = load <vscale x 1 x double>, <vscale x 1 x double>* %97, align 64, !tbaa !2
  %161 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %162 = call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double 0.000000e+00, i64 %161)
  %163 = load <vscale x 1 x double>, <vscale x 1 x double>* %77, align 64, !tbaa !2
  %164 = load <vscale x 1 x i1>, <vscale x 1 x i1>* %141, align 1, !tbaa !2
  %165 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %166 = call <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %162, <vscale x 1 x double> %163, <vscale x 1 x i1> %164, i64 %165)
  %167 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %168 = call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %160, <vscale x 1 x double> %166, i64 %167)
  store <vscale x 1 x double> %168, <vscale x 1 x double>* %97, align 64, !tbaa !2
  %169 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %170 = load <vscale x 1 x double>, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %171 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %172 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %169, <vscale x 1 x double> %170, i64 %171)
  store <vscale x 1 x double> %172, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %173 = call i64 @llvm.experimental.vector.vscale.i64()
  %z = alloca double, i64 %173, align 64
  %174 = bitcast double* %z to <vscale x 1 x double>*
  %175 = bitcast double* %z to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %175) #6
  %176 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %177 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %178 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %179 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %176, <vscale x 1 x double> %177, i64 %178)
  store <vscale x 1 x double> %179, <vscale x 1 x double>* %174, align 64, !tbaa !2
  %180 = call i64 @llvm.experimental.vector.vscale.i64()
  %y = alloca double, i64 %180, align 64
  %181 = bitcast double* %y to <vscale x 1 x double>*
  %182 = bitcast double* %y to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* %182) #6
  %183 = load <vscale x 1 x double>, <vscale x 1 x double>* %6, align 64, !tbaa !2
  store <vscale x 1 x double> %183, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %184 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %185 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %186 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %187 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %184, <vscale x 1 x double> %185, i64 %186)
  store <vscale x 1 x double> %187, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %188 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %189 = load <vscale x 1 x double>, <vscale x 1 x double>* %11, align 64, !tbaa !2
  %190 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %191 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %188, <vscale x 1 x double> %189, i64 %190)
  store <vscale x 1 x double> %191, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %192 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %193 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %194 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %195 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %192, <vscale x 1 x double> %193, i64 %194)
  store <vscale x 1 x double> %195, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %196 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %197 = load <vscale x 1 x double>, <vscale x 1 x double>* %16, align 64, !tbaa !2
  %198 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %199 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %196, <vscale x 1 x double> %197, i64 %198)
  store <vscale x 1 x double> %199, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %200 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %201 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %202 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %203 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %200, <vscale x 1 x double> %201, i64 %202)
  store <vscale x 1 x double> %203, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %204 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %205 = load <vscale x 1 x double>, <vscale x 1 x double>* %21, align 64, !tbaa !2
  %206 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %207 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %204, <vscale x 1 x double> %205, i64 %206)
  store <vscale x 1 x double> %207, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %208 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %209 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %210 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %211 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %208, <vscale x 1 x double> %209, i64 %210)
  store <vscale x 1 x double> %211, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %212 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %213 = load <vscale x 1 x double>, <vscale x 1 x double>* %26, align 64, !tbaa !2
  %214 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %215 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %212, <vscale x 1 x double> %213, i64 %214)
  store <vscale x 1 x double> %215, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %216 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %217 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %218 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %219 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %216, <vscale x 1 x double> %217, i64 %218)
  store <vscale x 1 x double> %219, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %220 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %221 = load <vscale x 1 x double>, <vscale x 1 x double>* %31, align 64, !tbaa !2
  %222 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %223 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %220, <vscale x 1 x double> %221, i64 %222)
  store <vscale x 1 x double> %223, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %224 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %225 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %226 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %227 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %224, <vscale x 1 x double> %225, i64 %226)
  store <vscale x 1 x double> %227, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %228 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %229 = load <vscale x 1 x double>, <vscale x 1 x double>* %36, align 64, !tbaa !2
  %230 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %231 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %228, <vscale x 1 x double> %229, i64 %230)
  store <vscale x 1 x double> %231, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %232 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %233 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %234 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %235 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %232, <vscale x 1 x double> %233, i64 %234)
  store <vscale x 1 x double> %235, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %236 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %237 = load <vscale x 1 x double>, <vscale x 1 x double>* %41, align 64, !tbaa !2
  %238 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %239 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %236, <vscale x 1 x double> %237, i64 %238)
  store <vscale x 1 x double> %239, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %240 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %241 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %242 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %243 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %240, <vscale x 1 x double> %241, i64 %242)
  store <vscale x 1 x double> %243, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %244 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %245 = load <vscale x 1 x double>, <vscale x 1 x double>* %46, align 64, !tbaa !2
  %246 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %247 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %244, <vscale x 1 x double> %245, i64 %246)
  store <vscale x 1 x double> %247, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %248 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %249 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %250 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %251 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %248, <vscale x 1 x double> %249, i64 %250)
  store <vscale x 1 x double> %251, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %252 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %253 = load <vscale x 1 x double>, <vscale x 1 x double>* %174, align 64, !tbaa !2
  %254 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %255 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %252, <vscale x 1 x double> %253, i64 %254)
  store <vscale x 1 x double> %255, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %256 = load <vscale x 1 x double>, <vscale x 1 x double>* %97, align 64, !tbaa !2
  %257 = load <vscale x 1 x double>, <vscale x 1 x double>* %87, align 64, !tbaa !2
  %258 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %259 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %256, <vscale x 1 x double> %257, i64 %258)
  store <vscale x 1 x double> %259, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %260 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %261 = load <vscale x 1 x double>, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %262 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %263 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %260, <vscale x 1 x double> %261, i64 %262)
  store <vscale x 1 x double> %263, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %264 = load <vscale x 1 x double>, <vscale x 1 x double>* %174, align 64, !tbaa !2
  %265 = load <vscale x 1 x double>, <vscale x 1 x double>* %82, align 64, !tbaa !2
  %266 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %267 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %264, <vscale x 1 x double> %265, i64 %266)
  store <vscale x 1 x double> %267, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %268 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %269 = load <vscale x 1 x double>, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %270 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %271 = call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %268, <vscale x 1 x double> %269, i64 %270)
  store <vscale x 1 x double> %271, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %272 = load <vscale x 1 x double>, <vscale x 1 x double>* %97, align 64, !tbaa !2
  %273 = load <vscale x 1 x double>, <vscale x 1 x double>* %92, align 64, !tbaa !2
  %274 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %275 = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %272, <vscale x 1 x double> %273, i64 %274)
  store <vscale x 1 x double> %275, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %276 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %277 = load <vscale x 1 x double>, <vscale x 1 x double>* %181, align 64, !tbaa !2
  %278 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %279 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %276, <vscale x 1 x double> %277, i64 %278)
  store <vscale x 1 x double> %279, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %280 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %281 = load <vscale x 1 x double>, <vscale x 1 x double>* %148, align 64, !tbaa !2
  %282 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %283 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %280, <vscale x 1 x double> %281, i64 %282)
  store <vscale x 1 x double> %283, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %284 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %285 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %286 = call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 -1, i64 %285)
  %287 = bitcast <vscale x 1 x i64> %286 to <vscale x 1 x double>
  %288 = load <vscale x 1 x i1>, <vscale x 1 x i1>* %100, align 1, !tbaa !2
  %289 = load i64, i64* %gvl.addr, align 8, !tbaa !5
  %290 = call <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %284, <vscale x 1 x double> %287, <vscale x 1 x i1> %288, i64 %289)
  store <vscale x 1 x double> %290, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %291 = load <vscale x 1 x double>, <vscale x 1 x double>* %x.addr, align 8, !tbaa !2
  %292 = bitcast double* %y to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %292) #6
  %293 = bitcast double* %z to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %293) #6
  %294 = bitcast double* %tmp to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %294) #6
  %295 = bitcast i1* %mask to i8*
  call void @llvm.lifetime.end.p0i8(i64 1, i8* %295) #6
  %296 = bitcast i1* %invalid_mask to i8*
  call void @llvm.lifetime.end.p0i8(i64 1, i8* %296) #6
  %297 = bitcast double* %e to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %297) #6
  %298 = bitcast double* %_ps256_cephes_log_q2 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %298) #6
  %299 = bitcast double* %_ps256_cephes_log_q1 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %299) #6
  %300 = bitcast double* %_ps256_0p5 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %300) #6
  %301 = bitcast double* %one to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %301) #6
  %302 = bitcast i64* %imm0 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %302) #6
  %303 = bitcast i64* %_256_0x7f to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %303) #6
  %304 = bitcast i64* %_256_inv_mant_mask to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %304) #6
  %305 = bitcast i64* %_x_i to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %305) #6
  %306 = bitcast double* %_ps256_min_norm_pos to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %306) #6
  %307 = bitcast i64* %_256_min_norm_pos to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %307) #6
  %308 = bitcast double* %_ps256_cephes_log_p8 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %308) #6
  %309 = bitcast double* %_ps256_cephes_log_p7 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %309) #6
  %310 = bitcast double* %_ps256_cephes_log_p6 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %310) #6
  %311 = bitcast double* %_ps256_cephes_log_p5 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %311) #6
  %312 = bitcast double* %_ps256_cephes_log_p4 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %312) #6
  %313 = bitcast double* %_ps256_cephes_log_p3 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %313) #6
  %314 = bitcast double* %_ps256_cephes_log_p2 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %314) #6
  %315 = bitcast double* %_ps256_cephes_log_p1 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %315) #6
  %316 = bitcast double* %_ps256_cephes_log_p0 to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %316) #6
  %317 = bitcast double* %_ps256_cephes_SQRTHF to i8*
  call void @llvm.lifetime.end.p0i8(i64 8, i8* %317) #6
  ret <vscale x 1 x double> %291
}

; Function Attrs: nounwind readnone
declare i64 @llvm.experimental.vector.vscale.i64() #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i1> @llvm.epi.vmfle.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfcvt.f.x.nxv1f64.nxv1i64(<vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i1> @llvm.epi.vmflt.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #2

declare dso_local signext i32 @printf(i8*, ...) #4

; Function Attrs: nounwind
declare dso_local signext i32 @gettimeofday(%struct.timeval*, %struct.timezone*) #5

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #2

; Function Attrs: nounwind
declare dso_local double @log(double) #5

; Function Attrs: nounwind
declare i64 @llvm.epi.vsetvl(i64, i64, i64) #6

; Function Attrs: nounwind readonly
declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64) #7

; Function Attrs: nounwind writeonly
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64) #8

attributes #0 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="64" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-features"="+a,+c,+d,+epi,+f,+m,-relax" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind readnone }
attributes #2 = { argmemonly nounwind }
attributes #3 = { norecurse "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="64" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-features"="+a,+c,+d,+epi,+f,+m,-relax" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-features"="+a,+c,+d,+epi,+f,+m,-relax" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-features"="+a,+c,+d,+epi,+f,+m,-relax" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind }
attributes #7 = { nounwind readonly }
attributes #8 = { nounwind writeonly }

!llvm.module.flags = !{!0}
!llvm.ident = !{!1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{!"clang version 9.0.0 (git@repo.hca.bsc.es:EPI/System-Software/llvm-mono.git 9c73d502eeef50316a792a82589c6f8bb1f31f24)"}
!2 = !{!3, !3, i64 0}
!3 = !{!"omnipotent char", !4, i64 0}
!4 = !{!"Simple C++ TBAA"}
!5 = !{!6, !6, i64 0}
!6 = !{!"long", !3, i64 0}
!7 = !{!8, !8, i64 0}
!8 = !{!"double", !3, i64 0}
!9 = !{!10, !10, i64 0}
!10 = !{!"int", !3, i64 0}
!11 = !{!12, !6, i64 0}
!12 = !{!"_ZTS7timeval", !6, i64 0, !6, i64 8}
!13 = !{!12, !6, i64 8}
!14 = !{i32 -2146452286}
!15 = !{i32 -2146452230}
