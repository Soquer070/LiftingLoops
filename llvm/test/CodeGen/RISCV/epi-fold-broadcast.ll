; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -S < %s | FileCheck --check-prefix=NOFOLD %s
; RUN: opt -S -epi-fold-broadcast < %s | FileCheck --check-prefix=FOLD %s

define <vscale x 1 x double> @add1(<vscale x 1 x double> %a, i64 %gvl) nounwind {
; NOFOLD-LABEL: @add1(
; NOFOLD-NEXT:    [[BCAST:%.*]] = call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 1.000000e+00, i64 [[GVL:%.*]])
; NOFOLD-NEXT:    [[RET:%.*]] = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> [[A:%.*]], <vscale x 1 x double> [[BCAST]], i64 [[GVL]])
; NOFOLD-NEXT:    ret <vscale x 1 x double> [[RET]]
;
; FOLD-LABEL: @add1(
; FOLD-NEXT:    [[TMP1:%.*]] = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.f64(<vscale x 1 x double> [[A:%.*]], double 1.000000e+00, i64 [[GVL:%.*]])
; FOLD-NEXT:    ret <vscale x 1 x double> [[TMP1]]
;
  %bcast = call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 1.0, i64 %gvl)
  %ret = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %bcast, i64 %gvl)
  ret <vscale x 1 x double> %ret
}

declare <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double, i64)
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

define <vscale x 1 x double> @add1_mask(<vscale x 1 x double> %a, <vscale x 1 x i1> %mask, i64 %gvl) nounwind {
; NOFOLD-LABEL: @add1_mask(
; NOFOLD-NEXT:    [[BCAST:%.*]] = call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 1.000000e+00, i64 [[GVL:%.*]])
; NOFOLD-NEXT:    [[RET:%.*]] = call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> [[A:%.*]], <vscale x 1 x double> [[A]], <vscale x 1 x double> [[BCAST]], <vscale x 1 x i1> [[MASK:%.*]], i64 [[GVL]])
; NOFOLD-NEXT:    ret <vscale x 1 x double> [[RET]]
;
; FOLD-LABEL: @add1_mask(
; FOLD-NEXT:    [[TMP1:%.*]] = call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.f64.nxv1i1(<vscale x 1 x double> [[A:%.*]], <vscale x 1 x double> [[A]], double 1.000000e+00, <vscale x 1 x i1> [[MASK:%.*]], i64 [[GVL:%.*]])
; FOLD-NEXT:    ret <vscale x 1 x double> [[TMP1]]
;
  %bcast = call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 1.0, i64 %gvl)
  %ret = call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %a, <vscale x 1 x double> %bcast, <vscale x 1 x i1> %mask, i64 %gvl)
  ret <vscale x 1 x double> %ret
}

declare <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i64)

define dso_local <vscale x 1 x double> @vfmacc(<vscale x 1 x double> %va, double %b, <vscale x 1 x double> %vc, i64 %gvl) nounwind {
; NOFOLD-LABEL: @vfmacc(
; NOFOLD-NEXT:  entry:
; NOFOLD-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double [[B:%.*]], i64 [[GVL:%.*]])
; NOFOLD-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> [[VA:%.*]], <vscale x 1 x double> [[TMP0]], <vscale x 1 x double> [[VC:%.*]], i64 [[GVL]])
; NOFOLD-NEXT:    ret <vscale x 1 x double> [[TMP1]]
;
; FOLD-LABEL: @vfmacc(
; FOLD-NEXT:  entry:
; FOLD-NEXT:    [[TMP0:%.*]] = call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.f64(<vscale x 1 x double> [[VA:%.*]], double [[B:%.*]], <vscale x 1 x double> [[VC:%.*]], i64 [[GVL:%.*]])
; FOLD-NEXT:    ret <vscale x 1 x double> [[TMP0]]
;
entry:
  %0 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double %b, i64 %gvl)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %va, <vscale x 1 x double> %0, <vscale x 1 x double> %vc, i64 %gvl)
  ret <vscale x 1 x double> %1
}

declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, i64)

define dso_local <vscale x 1 x double> @vfmacc_mask(<vscale x 1 x double> %va, double %b, <vscale x 1 x double> %vc, <vscale x 1 x i1> %vm, i64 %gvl) nounwind {
; NOFOLD-LABEL: @vfmacc_mask(
; NOFOLD-NEXT:  entry:
; NOFOLD-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double [[B:%.*]], i64 [[GVL:%.*]])
; NOFOLD-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> [[VA:%.*]], <vscale x 1 x double> [[TMP0]], <vscale x 1 x double> [[VC:%.*]], <vscale x 1 x i1> [[VM:%.*]], i64 [[GVL]])
; NOFOLD-NEXT:    ret <vscale x 1 x double> [[TMP1]]
;
; FOLD-LABEL: @vfmacc_mask(
; FOLD-NEXT:  entry:
; FOLD-NEXT:    [[TMP0:%.*]] = call <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.f64.nxv1i1(<vscale x 1 x double> [[VA:%.*]], double [[B:%.*]], <vscale x 1 x double> [[VC:%.*]], <vscale x 1 x i1> [[VM:%.*]], i64 [[GVL:%.*]])
; FOLD-NEXT:    ret <vscale x 1 x double> [[TMP0]]
;
entry:
  %0 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double %b, i64 %gvl)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %va, <vscale x 1 x double> %0, <vscale x 1 x double> %vc, <vscale x 1 x i1> %vm, i64 %gvl)
  ret <vscale x 1 x double> %1
}

declare <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i64)

define dso_local <vscale x 1 x i64> @test_broadcast_twice(i64 %a, i64 %gvl) nounwind {
; NOFOLD-LABEL: @test_broadcast_twice(
; NOFOLD-NEXT:  entry:
; NOFOLD-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64 [[A:%.*]], i64 [[GVL:%.*]])
; NOFOLD-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64> [[TMP0]], i64 [[GVL]])
; NOFOLD-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
;
; FOLD-LABEL: @test_broadcast_twice(
; FOLD-NEXT:  entry:
; FOLD-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64 [[A:%.*]], i64 [[GVL:%.*]])
; FOLD-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64> [[TMP0]], i64 [[GVL]])
; FOLD-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
;
entry:
  %0 = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64 %a, i64 %gvl)
  %1 = tail call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %0, i64 %gvl)
  ret <vscale x 1 x i64> %1
}

declare <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64, i64)

declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64)
