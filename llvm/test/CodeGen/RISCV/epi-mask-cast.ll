; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+epi < %s | FileCheck %s

; We do not want loads and stores be optimized, hence optnone and noinline
define dso_local void @to_mask(i8* %p) optnone noinline {
; CHECK-LABEL: to_mask:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a1, zero, e32, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ret
entry:
  %pi = bitcast i8* %p to <vscale x 2 x i32>*
  %a = load <vscale x 2 x i32>, <vscale x 2 x i32>* %pi
  %b = call <vscale x 2 x i1> @llvm.epi.mask.cast.nxv2i1.nxv2i32(<vscale x 2 x i32> %a)
  %pm = bitcast i8* %p to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %b, <vscale x 2 x i1>* %pm
  ret void
}
declare <vscale x 2 x i1> @llvm.epi.mask.cast.nxv2i1.nxv2i32(<vscale x 2 x i32>)

define dso_local void @from_mask(i8* %p) optnone noinline {
; CHECK-LABEL: from_mask:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a1, zero, e8, m1
; CHECK-NEXT:    vle.v v0, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32, m1
; CHECK-NEXT:    vse.v v0, (a0)
; CHECK-NEXT:    ret
entry:
  %pi = bitcast i8* %p to <vscale x 2 x i1>*
  %a = load <vscale x 2 x i1>, <vscale x 2 x i1>* %pi
  %b = call <vscale x 2 x i32> @llvm.epi.mask.cast.nxv2i32.nxv2i1(<vscale x 2 x i1> %a)
  %pm = bitcast i8* %p to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %b, <vscale x 2 x i32>* %pm
  ret void
}
declare <vscale x 2 x i32> @llvm.epi.mask.cast.nxv2i32.nxv2i1(<vscale x 2 x i1>)
