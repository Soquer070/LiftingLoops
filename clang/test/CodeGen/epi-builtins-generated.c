// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv64-unknown-linux-gnu -emit-llvm -O2 -o - \
// RUN:       -target-feature +m -target-feature +a -target-feature +f \
// RUN:       -target-feature +d -target-feature +c \
// RUN:       -target-feature +epi -target-abi lp64d %s \
// RUN:       | FileCheck --check-prefix=CHECK-O2 %s



// CHECK-O2-LABEL: @test_vreadvl(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vreadvl()
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vreadvl()
{
    return __builtin_epi_vreadvl();
}


// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m1(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m1);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m2(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m2);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m4(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m4);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e64__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 3, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e64__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e64, __epi_m8);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e32__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 2, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e32__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e32, __epi_m8);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e16__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 1, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e16__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e16, __epi_m8);
    return gvl;
}

// CHECK-O2-LABEL: @test_vsetvl__epi_e8__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvl(i64 [[RVL:%.*]], i64 0, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvl__epi_e8__epi_m8(unsigned long rvl)
{
    unsigned long gvl = __builtin_epi_vsetvl(rvl, __epi_e8, __epi_m8);
    return gvl;
}


// CHECK-O2-LABEL: @test_vsetvlmax__epi_e64__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 3, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e64__epi_m1()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e64, __epi_m1);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e32__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 2, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e32__epi_m1()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e32, __epi_m1);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e16__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 1, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e16__epi_m1()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e16, __epi_m1);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e8__epi_m1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 0, i64 0)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e8__epi_m1()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e8, __epi_m1);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e64__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 3, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e64__epi_m2()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e64, __epi_m2);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e32__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 2, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e32__epi_m2()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e32, __epi_m2);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e16__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 1, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e16__epi_m2()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e16, __epi_m2);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e8__epi_m2(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 0, i64 1)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e8__epi_m2()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e8, __epi_m2);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e64__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 3, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e64__epi_m4()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e64, __epi_m4);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e32__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 2, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e32__epi_m4()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e32, __epi_m4);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e16__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 1, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e16__epi_m4()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e16, __epi_m4);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e8__epi_m4(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 0, i64 2)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e8__epi_m4()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e8, __epi_m4);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e64__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 3, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e64__epi_m8()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e64, __epi_m8);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e32__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 2, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e32__epi_m8()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e32, __epi_m8);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e16__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 1, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e16__epi_m8()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e16, __epi_m8);
    return vlmax;
}

// CHECK-O2-LABEL: @test_vsetvlmax__epi_e8__epi_m8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vsetvlmax(i64 0, i64 3)
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
unsigned long test_vsetvlmax__epi_e8__epi_m8()
{
    unsigned long vlmax = __builtin_epi_vsetvlmax(__epi_e8, __epi_m8);
    return vlmax;
}


// CHECK-O2-LABEL: @test_vload_8xi8_vstore_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.nxv8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_8xi8_vstore_8xi8(const signed char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_8xi8(addr, gvl);
  __builtin_epi_vstore_8xi8((signed char* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_4xi16_vstore_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.nxv4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_4xi16_vstore_4xi16(const signed short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_4xi16(addr, gvl);
  __builtin_epi_vstore_4xi16((signed short int* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_2xi32_vstore_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xi32_vstore_2xi32(const signed int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_2xi32(addr, gvl);
  __builtin_epi_vstore_2xi32((signed int* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_1xi64_vstore_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_1xi64_vstore_1xi64(const signed long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_1xi64(addr, gvl);
  __builtin_epi_vstore_1xi64((signed long int* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_8xi8_vstore_strided_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.strided.nxv8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_8xi8_vstore_strided_8xi8(const signed char*  addr, signed long stride, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_strided_8xi8(addr, stride, gvl);
  __builtin_epi_vstore_strided_8xi8((signed char* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_4xi16_vstore_strided_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.strided.nxv4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_4xi16_vstore_strided_4xi16(const signed short int*  addr, signed long stride, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_strided_4xi16(addr, stride, gvl);
  __builtin_epi_vstore_strided_4xi16((signed short int* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_2xi32_vstore_strided_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.strided.nxv2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_2xi32_vstore_strided_2xi32(const signed int*  addr, signed long stride, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_strided_2xi32(addr, stride, gvl);
  __builtin_epi_vstore_strided_2xi32((signed int* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_1xi64_vstore_strided_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.strided.nxv1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_1xi64_vstore_strided_1xi64(const signed long int*  addr, signed long stride, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_strided_1xi64(addr, stride, gvl);
  __builtin_epi_vstore_strided_1xi64((signed long int* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_8xi8_vstore_indexed_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.indexed.nxv8i8.nxv8i8(<vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv8i8.nxv8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_8xi8_vstore_indexed_8xi8(const signed char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 index;
  result = __builtin_epi_vload_indexed_8xi8(addr, index, gvl);
  __builtin_epi_vstore_indexed_8xi8((signed char* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_4xi16_vstore_indexed_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.indexed.nxv4i16.nxv4i16(<vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv4i16.nxv4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_4xi16_vstore_indexed_4xi16(const signed short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 index;
  result = __builtin_epi_vload_indexed_4xi16(addr, index, gvl);
  __builtin_epi_vstore_indexed_4xi16((signed short int* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_2xi32_vstore_indexed_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.indexed.nxv2i32.nxv2i32(<vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv2i32.nxv2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_2xi32_vstore_indexed_2xi32(const signed int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 index;
  result = __builtin_epi_vload_indexed_2xi32(addr, index, gvl);
  __builtin_epi_vstore_indexed_2xi32((signed int* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_1xi64_vstore_indexed_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.indexed.nxv1i64.nxv1i64(<vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv1i64.nxv1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_1xi64_vstore_indexed_1xi64(const signed long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 index;
  result = __builtin_epi_vload_indexed_1xi64(addr, index, gvl);
  __builtin_epi_vstore_indexed_1xi64((signed long int* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_8xi8_vstore_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.nxv8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_8xi8_vstore_unsigned_8xi8(const unsigned char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_unsigned_8xi8(addr, gvl);
  __builtin_epi_vstore_unsigned_8xi8((unsigned char* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_4xi16_vstore_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.nxv4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_4xi16_vstore_unsigned_4xi16(const unsigned short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_unsigned_4xi16(addr, gvl);
  __builtin_epi_vstore_unsigned_4xi16((unsigned short int* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_2xi32_vstore_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_2xi32_vstore_unsigned_2xi32(const unsigned int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_unsigned_2xi32(addr, gvl);
  __builtin_epi_vstore_unsigned_2xi32((unsigned int* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_unsigned_1xi64_vstore_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_unsigned_1xi64_vstore_unsigned_1xi64(const unsigned long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_unsigned_1xi64(addr, gvl);
  __builtin_epi_vstore_unsigned_1xi64((unsigned long int* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi8_vstore_strided_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.strided.nxv8i8(<vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_8xi8_vstore_strided_unsigned_8xi8(const unsigned char*  addr, signed long stride, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vload_strided_unsigned_8xi8(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_8xi8((unsigned char* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi16_vstore_strided_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.strided.nxv4i16(<vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_4xi16_vstore_strided_unsigned_4xi16(const unsigned short int*  addr, signed long stride, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vload_strided_unsigned_4xi16(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_4xi16((unsigned short int* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_2xi32_vstore_strided_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.strided.nxv2i32(<vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_2xi32_vstore_strided_unsigned_2xi32(const unsigned int*  addr, signed long stride, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vload_strided_unsigned_2xi32(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_2xi32((unsigned int* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_unsigned_1xi64_vstore_strided_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.strided.nxv1i64(<vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_unsigned_1xi64_vstore_strided_unsigned_1xi64(const unsigned long int*  addr, signed long stride, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vload_strided_unsigned_1xi64(addr, stride, gvl);
  __builtin_epi_vstore_strided_unsigned_1xi64((unsigned long int* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi8_vstore_indexed_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ADDR:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vload.indexed.nxv8i8.nxv8i8(<vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv8i8.nxv8i8(<vscale x 8 x i8> [[TMP1]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_8xi8_vstore_indexed_unsigned_8xi8(const unsigned char*  addr, unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 index;
  result = __builtin_epi_vload_indexed_unsigned_8xi8(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_8xi8((unsigned char* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi16_vstore_indexed_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ADDR:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vload.indexed.nxv4i16.nxv4i16(<vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv4i16.nxv4i16(<vscale x 4 x i16> [[TMP1]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_4xi16_vstore_indexed_unsigned_4xi16(const unsigned short int*  addr, unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 index;
  result = __builtin_epi_vload_indexed_unsigned_4xi16(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_4xi16((unsigned short int* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_2xi32_vstore_indexed_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ADDR:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vload.indexed.nxv2i32.nxv2i32(<vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv2i32.nxv2i32(<vscale x 2 x i32> [[TMP1]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_2xi32_vstore_indexed_unsigned_2xi32(const unsigned int*  addr, unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 index;
  result = __builtin_epi_vload_indexed_unsigned_2xi32(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_2xi32((unsigned int* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_unsigned_1xi64_vstore_indexed_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ADDR:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vload.indexed.nxv1i64.nxv1i64(<vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv1i64.nxv1i64(<vscale x 1 x i64> [[TMP1]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_unsigned_1xi64_vstore_indexed_unsigned_1xi64(const unsigned long int*  addr, unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 index;
  result = __builtin_epi_vload_indexed_unsigned_1xi64(addr, index, gvl);
  __builtin_epi_vstore_indexed_unsigned_1xi64((unsigned long int* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_2xf32_vstore_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ADDR:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vload.nxv2f32(<vscale x 2 x float>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP1]], <vscale x 2 x float>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xf32_vstore_2xf32(const float*  addr, unsigned long gvl)
{
  __epi_2xf32 result;
  result = __builtin_epi_vload_2xf32(addr, gvl);
  __builtin_epi_vstore_2xf32((float* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_1xf64_vstore_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP1]], <vscale x 1 x double>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_1xf64_vstore_1xf64(const double*  addr, unsigned long gvl)
{
  __epi_1xf64 result;
  result = __builtin_epi_vload_1xf64(addr, gvl);
  __builtin_epi_vstore_1xf64((double* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_2xf64_vstore_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.epi.vload.nxv2f64(<vscale x 2 x double>* [[TMP0]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f64(<vscale x 2 x double> [[TMP1]], <vscale x 2 x double>* [[TMP0]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xf64_vstore_2xf64(const double*  addr, unsigned long gvl)
{
  __epi_2xf64 result;
  result = __builtin_epi_vload_2xf64(addr, gvl);
  __builtin_epi_vstore_2xf64((double* )addr, result, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_2xf32_vstore_strided_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ADDR:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vload.strided.nxv2f32(<vscale x 2 x float>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv2f32(<vscale x 2 x float> [[TMP1]], <vscale x 2 x float>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_2xf32_vstore_strided_2xf32(const float*  addr, signed long stride, unsigned long gvl)
{
  __epi_2xf32 result;
  result = __builtin_epi_vload_strided_2xf32(addr, stride, gvl);
  __builtin_epi_vstore_strided_2xf32((float* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_strided_1xf64_vstore_strided_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vload.strided.nxv1f64(<vscale x 1 x double>* [[TMP0]], i64 [[STRIDE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.strided.nxv1f64(<vscale x 1 x double> [[TMP1]], <vscale x 1 x double>* [[TMP0]], i64 [[STRIDE]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_strided_1xf64_vstore_strided_1xf64(const double*  addr, signed long stride, unsigned long gvl)
{
  __epi_1xf64 result;
  result = __builtin_epi_vload_strided_1xf64(addr, stride, gvl);
  __builtin_epi_vstore_strided_1xf64((double* )addr, result, stride, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_2xf32_vstore_indexed_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ADDR:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vload.indexed.nxv2f32.nxv2i32(<vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv2f32.nxv2i32(<vscale x 2 x float> [[TMP1]], <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_2xf32_vstore_indexed_2xf32(const float*  addr, unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 index;
  result = __builtin_epi_vload_indexed_2xf32(addr, index, gvl);
  __builtin_epi_vstore_indexed_2xf32((float* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_indexed_1xf64_vstore_indexed_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> [[TMP1]], <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> undef, i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vload_indexed_1xf64_vstore_indexed_1xf64(const double*  addr, unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 index;
  result = __builtin_epi_vload_indexed_1xf64(addr, index, gvl);
  __builtin_epi_vstore_indexed_1xf64((double* )addr, result, index, gvl);
}


// CHECK-O2-LABEL: @test_vload_1xi1_vstore_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_1xi1_vstore_1xi1(const unsigned long int*  addr)
{
  __epi_1xi1 result;
  result = __builtin_epi_vload_1xi1(addr);
  __builtin_epi_vstore_1xi1((unsigned long int* )addr, result);
}


// CHECK-O2-LABEL: @test_vload_2xi1_vstore_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_2xi1_vstore_2xi1(const unsigned int*  addr)
{
  __epi_2xi1 result;
  result = __builtin_epi_vload_2xi1(addr);
  __builtin_epi_vstore_2xi1((unsigned int* )addr, result);
}


// CHECK-O2-LABEL: @test_vload_4xi1_vstore_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_4xi1_vstore_4xi1(const unsigned short int*  addr)
{
  __epi_4xi1 result;
  result = __builtin_epi_vload_4xi1(addr);
  __builtin_epi_vstore_4xi1((unsigned short int* )addr, result);
}


// CHECK-O2-LABEL: @test_vload_8xi1_vstore_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    ret void
//
void test_vload_8xi1_vstore_8xi1(const unsigned char*  addr)
{
  __epi_8xi1 result;
  result = __builtin_epi_vload_8xi1(addr);
  __builtin_epi_vstore_8xi1((unsigned char* )addr, result);
}


signed char* p0;
// CHECK-O2-LABEL: @test_vadd_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p0 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vadd_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p0, result, gvl);
}


short* p1;
// CHECK-O2-LABEL: @test_vadd_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p1 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vadd_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p1, result, gvl);
}


int* p2;
// CHECK-O2-LABEL: @test_vadd_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p2 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vadd_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p2, result, gvl);
}


long* p3;
// CHECK-O2-LABEL: @test_vadd_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p3 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vadd_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p3, result, gvl);
}


signed char* p4;
// CHECK-O2-LABEL: @test_vadd_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p4 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vadd_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p4, result, gvl);
}


short* p5;
// CHECK-O2-LABEL: @test_vadd_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p5 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vadd_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p5, result, gvl);
}


int* p6;
// CHECK-O2-LABEL: @test_vadd_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p6 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vadd_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p6, result, gvl);
}


long* p7;
// CHECK-O2-LABEL: @test_vadd_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p7 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vadd_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vadd_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p7, result, gvl);
}


signed char* p8;
// CHECK-O2-LABEL: @test_vsub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p8 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p8, result, gvl);
}


short* p9;
// CHECK-O2-LABEL: @test_vsub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p9 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p9, result, gvl);
}


int* p10;
// CHECK-O2-LABEL: @test_vsub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p10 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p10, result, gvl);
}


long* p11;
// CHECK-O2-LABEL: @test_vsub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p11 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p11, result, gvl);
}


signed char* p12;
// CHECK-O2-LABEL: @test_vsub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p12 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsub_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p12, result, gvl);
}


short* p13;
// CHECK-O2-LABEL: @test_vsub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p13 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsub_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p13, result, gvl);
}


int* p14;
// CHECK-O2-LABEL: @test_vsub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p14 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsub_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p14, result, gvl);
}


long* p15;
// CHECK-O2-LABEL: @test_vsub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p15 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsub_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p15, result, gvl);
}


signed char* p16;
// CHECK-O2-LABEL: @test_vrsub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrsub.nxv8i8.i8(<vscale x 8 x i8> undef, i8 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p16 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  signed char rhs;
  result = __builtin_epi_vrsub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p16, result, gvl);
}


short* p17;
// CHECK-O2-LABEL: @test_vrsub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrsub.nxv4i16.i16(<vscale x 4 x i16> undef, i16 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p17 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  signed short int rhs;
  result = __builtin_epi_vrsub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p17, result, gvl);
}


int* p18;
// CHECK-O2-LABEL: @test_vrsub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrsub.nxv2i32.i32(<vscale x 2 x i32> undef, i32 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p18 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  signed int rhs;
  result = __builtin_epi_vrsub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p18, result, gvl);
}


long* p19;
// CHECK-O2-LABEL: @test_vrsub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p19 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  signed long int rhs;
  result = __builtin_epi_vrsub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p19, result, gvl);
}


signed char* p20;
// CHECK-O2-LABEL: @test_vrsub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrsub.mask.nxv8i8.i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i8 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p20 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  signed char rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vrsub_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p20, result, gvl);
}


short* p21;
// CHECK-O2-LABEL: @test_vrsub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrsub.mask.nxv4i16.i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i16 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p21 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  signed short int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vrsub_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p21, result, gvl);
}


int* p22;
// CHECK-O2-LABEL: @test_vrsub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrsub.mask.nxv2i32.i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i32 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p22 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  signed int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrsub_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p22, result, gvl);
}


long* p23;
// CHECK-O2-LABEL: @test_vrsub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p23 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrsub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  signed long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrsub_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p23, result, gvl);
}


signed char* p24;
// CHECK-O2-LABEL: @test_vand_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p24 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vand_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p24, result, gvl);
}


short* p25;
// CHECK-O2-LABEL: @test_vand_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p25 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vand_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p25, result, gvl);
}


int* p26;
// CHECK-O2-LABEL: @test_vand_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p26 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vand_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p26, result, gvl);
}


long* p27;
// CHECK-O2-LABEL: @test_vand_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p27 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vand_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p27, result, gvl);
}


signed char* p28;
// CHECK-O2-LABEL: @test_vand_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p28 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vand_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p28, result, gvl);
}


short* p29;
// CHECK-O2-LABEL: @test_vand_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p29 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vand_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p29, result, gvl);
}


int* p30;
// CHECK-O2-LABEL: @test_vand_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p30 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vand_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p30, result, gvl);
}


long* p31;
// CHECK-O2-LABEL: @test_vand_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p31 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vand_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vand_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p31, result, gvl);
}


signed char* p32;
// CHECK-O2-LABEL: @test_vor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p32 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p32, result, gvl);
}


short* p33;
// CHECK-O2-LABEL: @test_vor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p33 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p33, result, gvl);
}


int* p34;
// CHECK-O2-LABEL: @test_vor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p34 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p34, result, gvl);
}


long* p35;
// CHECK-O2-LABEL: @test_vor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p35 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p35, result, gvl);
}


signed char* p36;
// CHECK-O2-LABEL: @test_vor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p36 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vor_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p36, result, gvl);
}


short* p37;
// CHECK-O2-LABEL: @test_vor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p37 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vor_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p37, result, gvl);
}


int* p38;
// CHECK-O2-LABEL: @test_vor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p38 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vor_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p38, result, gvl);
}


long* p39;
// CHECK-O2-LABEL: @test_vor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p39 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vor_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p39, result, gvl);
}


signed char* p40;
// CHECK-O2-LABEL: @test_vxor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p40 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vxor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p40, result, gvl);
}


short* p41;
// CHECK-O2-LABEL: @test_vxor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p41 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vxor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p41, result, gvl);
}


int* p42;
// CHECK-O2-LABEL: @test_vxor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p42 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vxor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p42, result, gvl);
}


long* p43;
// CHECK-O2-LABEL: @test_vxor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p43 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vxor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p43, result, gvl);
}


signed char* p44;
// CHECK-O2-LABEL: @test_vxor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p44 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vxor_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p44, result, gvl);
}


short* p45;
// CHECK-O2-LABEL: @test_vxor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p45 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vxor_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p45, result, gvl);
}


int* p46;
// CHECK-O2-LABEL: @test_vxor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p46 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vxor_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p46, result, gvl);
}


long* p47;
// CHECK-O2-LABEL: @test_vxor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p47 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vxor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vxor_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p47, result, gvl);
}


signed char* p48;
// CHECK-O2-LABEL: @test_vsll_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p48 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsll_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p48, result, gvl);
}


short* p49;
// CHECK-O2-LABEL: @test_vsll_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p49 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsll_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p49, result, gvl);
}


int* p50;
// CHECK-O2-LABEL: @test_vsll_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p50 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsll_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p50, result, gvl);
}


long* p51;
// CHECK-O2-LABEL: @test_vsll_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p51 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsll_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p51, result, gvl);
}


signed char* p52;
// CHECK-O2-LABEL: @test_vsll_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p52 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsll_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p52, result, gvl);
}


short* p53;
// CHECK-O2-LABEL: @test_vsll_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p53 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsll_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p53, result, gvl);
}


int* p54;
// CHECK-O2-LABEL: @test_vsll_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p54 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsll_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p54, result, gvl);
}


long* p55;
// CHECK-O2-LABEL: @test_vsll_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p55 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsll_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsll_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p55, result, gvl);
}


signed char* p56;
// CHECK-O2-LABEL: @test_vsrl_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p56 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsrl_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p56, result, gvl);
}


short* p57;
// CHECK-O2-LABEL: @test_vsrl_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p57 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsrl_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p57, result, gvl);
}


int* p58;
// CHECK-O2-LABEL: @test_vsrl_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p58 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsrl_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p58, result, gvl);
}


long* p59;
// CHECK-O2-LABEL: @test_vsrl_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p59 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsrl_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p59, result, gvl);
}


signed char* p60;
// CHECK-O2-LABEL: @test_vsrl_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p60 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsrl_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p60, result, gvl);
}


short* p61;
// CHECK-O2-LABEL: @test_vsrl_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p61 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsrl_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p61, result, gvl);
}


int* p62;
// CHECK-O2-LABEL: @test_vsrl_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p62 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsrl_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p62, result, gvl);
}


long* p63;
// CHECK-O2-LABEL: @test_vsrl_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p63 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsrl_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsrl_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p63, result, gvl);
}


signed char* p64;
// CHECK-O2-LABEL: @test_vsra_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p64 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsra_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p64, result, gvl);
}


short* p65;
// CHECK-O2-LABEL: @test_vsra_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p65 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsra_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p65, result, gvl);
}


int* p66;
// CHECK-O2-LABEL: @test_vsra_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p66 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsra_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p66, result, gvl);
}


long* p67;
// CHECK-O2-LABEL: @test_vsra_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p67 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsra_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p67, result, gvl);
}


signed char* p68;
// CHECK-O2-LABEL: @test_vsra_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p68 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsra_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p68, result, gvl);
}


short* p69;
// CHECK-O2-LABEL: @test_vsra_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p69 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsra_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p69, result, gvl);
}


int* p70;
// CHECK-O2-LABEL: @test_vsra_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p70 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsra_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p70, result, gvl);
}


long* p71;
// CHECK-O2-LABEL: @test_vsra_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p71 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsra_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsra_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p71, result, gvl);
}


unsigned char* p72;
// CHECK-O2-LABEL: @test_vmseq_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p72 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmseq_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p72, result);
}


unsigned short* p73;
// CHECK-O2-LABEL: @test_vmseq_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p73 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmseq_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p73, result);
}


unsigned int* p74;
// CHECK-O2-LABEL: @test_vmseq_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p74 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmseq_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p74, result);
}


unsigned long* p75;
// CHECK-O2-LABEL: @test_vmseq_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmseq.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p75 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmseq_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p75, result);
}


unsigned char* p76;
// CHECK-O2-LABEL: @test_vmseq_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p76 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmseq_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p76, result);
}


unsigned short* p77;
// CHECK-O2-LABEL: @test_vmseq_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p77 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmseq_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p77, result);
}


unsigned int* p78;
// CHECK-O2-LABEL: @test_vmseq_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p78 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmseq_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p78, result);
}


unsigned long* p79;
// CHECK-O2-LABEL: @test_vmseq_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmseq.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p79 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmseq_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmseq_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p79, result);
}


unsigned char* p80;
// CHECK-O2-LABEL: @test_vmsne_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p80 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmsne_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p80, result);
}


unsigned short* p81;
// CHECK-O2-LABEL: @test_vmsne_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p81 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmsne_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p81, result);
}


unsigned int* p82;
// CHECK-O2-LABEL: @test_vmsne_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p82 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmsne_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p82, result);
}


unsigned long* p83;
// CHECK-O2-LABEL: @test_vmsne_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsne.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p83 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmsne_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p83, result);
}


unsigned char* p84;
// CHECK-O2-LABEL: @test_vmsne_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p84 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsne_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p84, result);
}


unsigned short* p85;
// CHECK-O2-LABEL: @test_vmsne_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p85 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsne_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p85, result);
}


unsigned int* p86;
// CHECK-O2-LABEL: @test_vmsne_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p86 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsne_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p86, result);
}


unsigned long* p87;
// CHECK-O2-LABEL: @test_vmsne_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsne.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p87 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsne_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsne_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p87, result);
}


unsigned char* p88;
// CHECK-O2-LABEL: @test_vmsltu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p88 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmsltu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p88, result);
}


unsigned short* p89;
// CHECK-O2-LABEL: @test_vmsltu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p89 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmsltu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p89, result);
}


unsigned int* p90;
// CHECK-O2-LABEL: @test_vmsltu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p90 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmsltu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p90, result);
}


unsigned long* p91;
// CHECK-O2-LABEL: @test_vmsltu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsltu.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p91 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmsltu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p91, result);
}


unsigned char* p92;
// CHECK-O2-LABEL: @test_vmsltu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p92 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsltu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p92, result);
}


unsigned short* p93;
// CHECK-O2-LABEL: @test_vmsltu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p93 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsltu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p93, result);
}


unsigned int* p94;
// CHECK-O2-LABEL: @test_vmsltu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p94 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsltu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p94, result);
}


unsigned long* p95;
// CHECK-O2-LABEL: @test_vmsltu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsltu.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p95 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsltu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsltu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p95, result);
}


unsigned char* p96;
// CHECK-O2-LABEL: @test_vmslt_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p96 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmslt_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p96, result);
}


unsigned short* p97;
// CHECK-O2-LABEL: @test_vmslt_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p97 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmslt_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p97, result);
}


unsigned int* p98;
// CHECK-O2-LABEL: @test_vmslt_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p98 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmslt_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p98, result);
}


unsigned long* p99;
// CHECK-O2-LABEL: @test_vmslt_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmslt.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p99 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmslt_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p99, result);
}


unsigned char* p100;
// CHECK-O2-LABEL: @test_vmslt_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p100 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmslt_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p100, result);
}


unsigned short* p101;
// CHECK-O2-LABEL: @test_vmslt_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p101 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmslt_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p101, result);
}


unsigned int* p102;
// CHECK-O2-LABEL: @test_vmslt_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p102 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmslt_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p102, result);
}


unsigned long* p103;
// CHECK-O2-LABEL: @test_vmslt_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmslt.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p103 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmslt_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmslt_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p103, result);
}


unsigned char* p104;
// CHECK-O2-LABEL: @test_vmsleu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p104 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmsleu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p104, result);
}


unsigned short* p105;
// CHECK-O2-LABEL: @test_vmsleu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p105 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmsleu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p105, result);
}


unsigned int* p106;
// CHECK-O2-LABEL: @test_vmsleu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p106 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmsleu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p106, result);
}


unsigned long* p107;
// CHECK-O2-LABEL: @test_vmsleu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsleu.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p107 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmsleu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p107, result);
}


unsigned char* p108;
// CHECK-O2-LABEL: @test_vmsleu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p108 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsleu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p108, result);
}


unsigned short* p109;
// CHECK-O2-LABEL: @test_vmsleu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p109 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsleu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p109, result);
}


unsigned int* p110;
// CHECK-O2-LABEL: @test_vmsleu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p110 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsleu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p110, result);
}


unsigned long* p111;
// CHECK-O2-LABEL: @test_vmsleu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsleu.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p111 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsleu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsleu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p111, result);
}


unsigned char* p112;
// CHECK-O2-LABEL: @test_vmsle_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p112 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmsle_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p112, result);
}


unsigned short* p113;
// CHECK-O2-LABEL: @test_vmsle_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p113 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmsle_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p113, result);
}


unsigned int* p114;
// CHECK-O2-LABEL: @test_vmsle_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p114 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmsle_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p114, result);
}


unsigned long* p115;
// CHECK-O2-LABEL: @test_vmsle_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsle.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p115 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmsle_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p115, result);
}


unsigned char* p116;
// CHECK-O2-LABEL: @test_vmsle_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p116 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsle_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p116, result);
}


unsigned short* p117;
// CHECK-O2-LABEL: @test_vmsle_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p117 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsle_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p117, result);
}


unsigned int* p118;
// CHECK-O2-LABEL: @test_vmsle_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p118 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsle_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p118, result);
}


unsigned long* p119;
// CHECK-O2-LABEL: @test_vmsle_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsle.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p119 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsle_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsle_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p119, result);
}


unsigned char* p120;
// CHECK-O2-LABEL: @test_vmsgtu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p120 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmsgtu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p120, result);
}


unsigned short* p121;
// CHECK-O2-LABEL: @test_vmsgtu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p121 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmsgtu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p121, result);
}


unsigned int* p122;
// CHECK-O2-LABEL: @test_vmsgtu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p122 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmsgtu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p122, result);
}


unsigned long* p123;
// CHECK-O2-LABEL: @test_vmsgtu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsgtu.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p123 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmsgtu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p123, result);
}


unsigned char* p124;
// CHECK-O2-LABEL: @test_vmsgtu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p124 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsgtu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p124, result);
}


unsigned short* p125;
// CHECK-O2-LABEL: @test_vmsgtu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p125 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsgtu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p125, result);
}


unsigned int* p126;
// CHECK-O2-LABEL: @test_vmsgtu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p126 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsgtu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p126, result);
}


unsigned long* p127;
// CHECK-O2-LABEL: @test_vmsgtu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsgtu.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p127 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsgtu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsgtu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p127, result);
}


unsigned char* p128;
// CHECK-O2-LABEL: @test_vmsgt_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p128 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmsgt_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p128, result);
}


unsigned short* p129;
// CHECK-O2-LABEL: @test_vmsgt_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p129 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmsgt_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p129, result);
}


unsigned int* p130;
// CHECK-O2-LABEL: @test_vmsgt_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p130 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmsgt_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p130, result);
}


unsigned long* p131;
// CHECK-O2-LABEL: @test_vmsgt_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsgt.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p131 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmsgt_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p131, result);
}


unsigned char* p132;
// CHECK-O2-LABEL: @test_vmsgt_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i8.nxv8i8(<vscale x 8 x i1> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p132 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_8xi8_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsgt_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p132, result);
}


unsigned short* p133;
// CHECK-O2-LABEL: @test_vmsgt_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.nxv4i16.nxv4i16(<vscale x 4 x i1> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p133 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_4xi16_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsgt_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p133, result);
}


unsigned int* p134;
// CHECK-O2-LABEL: @test_vmsgt_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.nxv2i32.nxv2i32(<vscale x 2 x i1> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p134 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_2xi32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsgt_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p134, result);
}


unsigned long* p135;
// CHECK-O2-LABEL: @test_vmsgt_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsgt.mask.nxv1i1.nxv1i64.nxv1i64(<vscale x 1 x i1> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p135 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsgt_1xi64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsgt_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p135, result);
}


signed char* p136;
// CHECK-O2-LABEL: @test_vminu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p136 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vminu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p136, result, gvl);
}


short* p137;
// CHECK-O2-LABEL: @test_vminu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p137 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vminu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p137, result, gvl);
}


int* p138;
// CHECK-O2-LABEL: @test_vminu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p138 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vminu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p138, result, gvl);
}


long* p139;
// CHECK-O2-LABEL: @test_vminu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p139 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vminu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p139, result, gvl);
}


signed char* p140;
// CHECK-O2-LABEL: @test_vminu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p140 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vminu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p140, result, gvl);
}


short* p141;
// CHECK-O2-LABEL: @test_vminu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p141 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vminu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p141, result, gvl);
}


int* p142;
// CHECK-O2-LABEL: @test_vminu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p142 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vminu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p142, result, gvl);
}


long* p143;
// CHECK-O2-LABEL: @test_vminu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p143 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vminu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vminu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p143, result, gvl);
}


signed char* p144;
// CHECK-O2-LABEL: @test_vmin_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p144 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmin_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p144, result, gvl);
}


short* p145;
// CHECK-O2-LABEL: @test_vmin_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p145 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmin_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p145, result, gvl);
}


int* p146;
// CHECK-O2-LABEL: @test_vmin_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p146 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmin_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p146, result, gvl);
}


long* p147;
// CHECK-O2-LABEL: @test_vmin_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p147 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmin_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p147, result, gvl);
}


signed char* p148;
// CHECK-O2-LABEL: @test_vmin_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p148 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmin_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p148, result, gvl);
}


short* p149;
// CHECK-O2-LABEL: @test_vmin_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p149 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmin_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p149, result, gvl);
}


int* p150;
// CHECK-O2-LABEL: @test_vmin_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p150 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmin_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p150, result, gvl);
}


long* p151;
// CHECK-O2-LABEL: @test_vmin_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p151 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmin_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmin_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p151, result, gvl);
}


signed char* p152;
// CHECK-O2-LABEL: @test_vmaxu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p152 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmaxu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p152, result, gvl);
}


short* p153;
// CHECK-O2-LABEL: @test_vmaxu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p153 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmaxu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p153, result, gvl);
}


int* p154;
// CHECK-O2-LABEL: @test_vmaxu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p154 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmaxu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p154, result, gvl);
}


long* p155;
// CHECK-O2-LABEL: @test_vmaxu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p155 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmaxu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p155, result, gvl);
}


signed char* p156;
// CHECK-O2-LABEL: @test_vmaxu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p156 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmaxu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p156, result, gvl);
}


short* p157;
// CHECK-O2-LABEL: @test_vmaxu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p157 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmaxu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p157, result, gvl);
}


int* p158;
// CHECK-O2-LABEL: @test_vmaxu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p158 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmaxu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p158, result, gvl);
}


long* p159;
// CHECK-O2-LABEL: @test_vmaxu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p159 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmaxu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmaxu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p159, result, gvl);
}


signed char* p160;
// CHECK-O2-LABEL: @test_vmax_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p160 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmax_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p160, result, gvl);
}


short* p161;
// CHECK-O2-LABEL: @test_vmax_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p161 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmax_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p161, result, gvl);
}


int* p162;
// CHECK-O2-LABEL: @test_vmax_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p162 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmax_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p162, result, gvl);
}


long* p163;
// CHECK-O2-LABEL: @test_vmax_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p163 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmax_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p163, result, gvl);
}


signed char* p164;
// CHECK-O2-LABEL: @test_vmax_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p164 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmax_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p164, result, gvl);
}


short* p165;
// CHECK-O2-LABEL: @test_vmax_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p165 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmax_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p165, result, gvl);
}


int* p166;
// CHECK-O2-LABEL: @test_vmax_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p166 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmax_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p166, result, gvl);
}


long* p167;
// CHECK-O2-LABEL: @test_vmax_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p167 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmax_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmax_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p167, result, gvl);
}


signed char* p168;
// CHECK-O2-LABEL: @test_vmul_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p168 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmul_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p168, result, gvl);
}


short* p169;
// CHECK-O2-LABEL: @test_vmul_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p169 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmul_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p169, result, gvl);
}


int* p170;
// CHECK-O2-LABEL: @test_vmul_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p170 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmul_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p170, result, gvl);
}


long* p171;
// CHECK-O2-LABEL: @test_vmul_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p171 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmul_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p171, result, gvl);
}


signed char* p172;
// CHECK-O2-LABEL: @test_vmul_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p172 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmul_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p172, result, gvl);
}


short* p173;
// CHECK-O2-LABEL: @test_vmul_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p173 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmul_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p173, result, gvl);
}


int* p174;
// CHECK-O2-LABEL: @test_vmul_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p174 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmul_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p174, result, gvl);
}


long* p175;
// CHECK-O2-LABEL: @test_vmul_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p175 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmul_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmul_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p175, result, gvl);
}


signed char* p176;
// CHECK-O2-LABEL: @test_vmulh_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p176 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmulh_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p176, result, gvl);
}


short* p177;
// CHECK-O2-LABEL: @test_vmulh_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p177 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmulh_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p177, result, gvl);
}


int* p178;
// CHECK-O2-LABEL: @test_vmulh_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p178 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmulh_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p178, result, gvl);
}


long* p179;
// CHECK-O2-LABEL: @test_vmulh_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p179 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmulh_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p179, result, gvl);
}


signed char* p180;
// CHECK-O2-LABEL: @test_vmulh_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p180 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmulh_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p180, result, gvl);
}


short* p181;
// CHECK-O2-LABEL: @test_vmulh_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p181 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmulh_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p181, result, gvl);
}


int* p182;
// CHECK-O2-LABEL: @test_vmulh_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p182 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmulh_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p182, result, gvl);
}


long* p183;
// CHECK-O2-LABEL: @test_vmulh_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p183 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulh_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmulh_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p183, result, gvl);
}


signed char* p184;
// CHECK-O2-LABEL: @test_vmulhu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p184 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmulhu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p184, result, gvl);
}


short* p185;
// CHECK-O2-LABEL: @test_vmulhu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p185 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmulhu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p185, result, gvl);
}


int* p186;
// CHECK-O2-LABEL: @test_vmulhu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p186 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmulhu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p186, result, gvl);
}


long* p187;
// CHECK-O2-LABEL: @test_vmulhu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p187 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmulhu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p187, result, gvl);
}


signed char* p188;
// CHECK-O2-LABEL: @test_vmulhu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p188 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmulhu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p188, result, gvl);
}


short* p189;
// CHECK-O2-LABEL: @test_vmulhu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p189 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmulhu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p189, result, gvl);
}


int* p190;
// CHECK-O2-LABEL: @test_vmulhu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p190 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmulhu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p190, result, gvl);
}


long* p191;
// CHECK-O2-LABEL: @test_vmulhu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p191 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmulhu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p191, result, gvl);
}


signed char* p192;
// CHECK-O2-LABEL: @test_vmulhsu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p192 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vmulhsu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p192, result, gvl);
}


short* p193;
// CHECK-O2-LABEL: @test_vmulhsu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p193 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vmulhsu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p193, result, gvl);
}


int* p194;
// CHECK-O2-LABEL: @test_vmulhsu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p194 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vmulhsu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p194, result, gvl);
}


long* p195;
// CHECK-O2-LABEL: @test_vmulhsu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p195 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vmulhsu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p195, result, gvl);
}


signed char* p196;
// CHECK-O2-LABEL: @test_vmulhsu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p196 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmulhsu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p196, result, gvl);
}


short* p197;
// CHECK-O2-LABEL: @test_vmulhsu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p197 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmulhsu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p197, result, gvl);
}


int* p198;
// CHECK-O2-LABEL: @test_vmulhsu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p198 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmulhsu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p198, result, gvl);
}


long* p199;
// CHECK-O2-LABEL: @test_vmulhsu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p199 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmulhsu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmulhsu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p199, result, gvl);
}


signed char* p200;
// CHECK-O2-LABEL: @test_vdivu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p200 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdivu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p200, result, gvl);
}


short* p201;
// CHECK-O2-LABEL: @test_vdivu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p201 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdivu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p201, result, gvl);
}


int* p202;
// CHECK-O2-LABEL: @test_vdivu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p202 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdivu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p202, result, gvl);
}


long* p203;
// CHECK-O2-LABEL: @test_vdivu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p203 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdivu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p203, result, gvl);
}


signed char* p204;
// CHECK-O2-LABEL: @test_vdivu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p204 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdivu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p204, result, gvl);
}


short* p205;
// CHECK-O2-LABEL: @test_vdivu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p205 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdivu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p205, result, gvl);
}


int* p206;
// CHECK-O2-LABEL: @test_vdivu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p206 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdivu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p206, result, gvl);
}


long* p207;
// CHECK-O2-LABEL: @test_vdivu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p207 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdivu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdivu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p207, result, gvl);
}


signed char* p208;
// CHECK-O2-LABEL: @test_vdiv_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p208 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdiv_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p208, result, gvl);
}


short* p209;
// CHECK-O2-LABEL: @test_vdiv_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p209 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdiv_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p209, result, gvl);
}


int* p210;
// CHECK-O2-LABEL: @test_vdiv_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p210 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdiv_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p210, result, gvl);
}


long* p211;
// CHECK-O2-LABEL: @test_vdiv_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p211 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdiv_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p211, result, gvl);
}


signed char* p212;
// CHECK-O2-LABEL: @test_vdiv_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p212 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdiv_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p212, result, gvl);
}


short* p213;
// CHECK-O2-LABEL: @test_vdiv_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p213 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdiv_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p213, result, gvl);
}


int* p214;
// CHECK-O2-LABEL: @test_vdiv_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p214 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdiv_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p214, result, gvl);
}


long* p215;
// CHECK-O2-LABEL: @test_vdiv_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p215 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdiv_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdiv_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p215, result, gvl);
}


signed char* p216;
// CHECK-O2-LABEL: @test_vremu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p216 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vremu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p216, result, gvl);
}


short* p217;
// CHECK-O2-LABEL: @test_vremu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p217 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vremu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p217, result, gvl);
}


int* p218;
// CHECK-O2-LABEL: @test_vremu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p218 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vremu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p218, result, gvl);
}


long* p219;
// CHECK-O2-LABEL: @test_vremu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p219 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vremu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p219, result, gvl);
}


signed char* p220;
// CHECK-O2-LABEL: @test_vremu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p220 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vremu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p220, result, gvl);
}


short* p221;
// CHECK-O2-LABEL: @test_vremu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p221 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vremu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p221, result, gvl);
}


int* p222;
// CHECK-O2-LABEL: @test_vremu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p222 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vremu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p222, result, gvl);
}


long* p223;
// CHECK-O2-LABEL: @test_vremu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p223 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vremu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vremu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p223, result, gvl);
}


signed char* p224;
// CHECK-O2-LABEL: @test_vrem_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p224 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vrem_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p224, result, gvl);
}


short* p225;
// CHECK-O2-LABEL: @test_vrem_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p225 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vrem_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p225, result, gvl);
}


int* p226;
// CHECK-O2-LABEL: @test_vrem_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p226 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vrem_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p226, result, gvl);
}


long* p227;
// CHECK-O2-LABEL: @test_vrem_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p227 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vrem_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p227, result, gvl);
}


signed char* p228;
// CHECK-O2-LABEL: @test_vrem_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p228 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vrem_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p228, result, gvl);
}


short* p229;
// CHECK-O2-LABEL: @test_vrem_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p229 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vrem_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p229, result, gvl);
}


int* p230;
// CHECK-O2-LABEL: @test_vrem_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p230 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrem_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p230, result, gvl);
}


long* p231;
// CHECK-O2-LABEL: @test_vrem_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p231 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrem_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrem_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p231, result, gvl);
}


signed char* p232;
// CHECK-O2-LABEL: @test_vmerge_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p232 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmerge_8xi8(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p232, result, gvl);
}


short* p233;
// CHECK-O2-LABEL: @test_vmerge_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p233 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmerge_4xi16(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p233, result, gvl);
}


int* p234;
// CHECK-O2-LABEL: @test_vmerge_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p234 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmerge_2xi32(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p234, result, gvl);
}


long* p235;
// CHECK-O2-LABEL: @test_vmerge_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p235 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vmerge_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmerge_1xi64(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p235, result, gvl);
}


signed char* p236;
// CHECK-O2-LABEL: @test_vsaddu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p236 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsaddu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p236, result, gvl);
}


short* p237;
// CHECK-O2-LABEL: @test_vsaddu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p237 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsaddu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p237, result, gvl);
}


int* p238;
// CHECK-O2-LABEL: @test_vsaddu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p238 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsaddu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p238, result, gvl);
}


long* p239;
// CHECK-O2-LABEL: @test_vsaddu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p239 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsaddu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p239, result, gvl);
}


signed char* p240;
// CHECK-O2-LABEL: @test_vsaddu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p240 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsaddu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p240, result, gvl);
}


short* p241;
// CHECK-O2-LABEL: @test_vsaddu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p241 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsaddu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p241, result, gvl);
}


int* p242;
// CHECK-O2-LABEL: @test_vsaddu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p242 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsaddu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p242, result, gvl);
}


long* p243;
// CHECK-O2-LABEL: @test_vsaddu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p243 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsaddu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsaddu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p243, result, gvl);
}


signed char* p244;
// CHECK-O2-LABEL: @test_vsadd_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p244 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsadd_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p244, result, gvl);
}


short* p245;
// CHECK-O2-LABEL: @test_vsadd_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p245 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsadd_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p245, result, gvl);
}


int* p246;
// CHECK-O2-LABEL: @test_vsadd_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p246 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsadd_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p246, result, gvl);
}


long* p247;
// CHECK-O2-LABEL: @test_vsadd_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p247 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsadd_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p247, result, gvl);
}


signed char* p248;
// CHECK-O2-LABEL: @test_vsadd_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p248 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsadd_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p248, result, gvl);
}


short* p249;
// CHECK-O2-LABEL: @test_vsadd_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p249 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsadd_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p249, result, gvl);
}


int* p250;
// CHECK-O2-LABEL: @test_vsadd_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p250 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsadd_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p250, result, gvl);
}


long* p251;
// CHECK-O2-LABEL: @test_vsadd_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p251 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsadd_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsadd_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p251, result, gvl);
}


signed char* p252;
// CHECK-O2-LABEL: @test_vssub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p252 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p252, result, gvl);
}


short* p253;
// CHECK-O2-LABEL: @test_vssub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p253 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p253, result, gvl);
}


int* p254;
// CHECK-O2-LABEL: @test_vssub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p254 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p254, result, gvl);
}


long* p255;
// CHECK-O2-LABEL: @test_vssub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p255 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p255, result, gvl);
}


signed char* p256;
// CHECK-O2-LABEL: @test_vssub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p256 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssub_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p256, result, gvl);
}


short* p257;
// CHECK-O2-LABEL: @test_vssub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p257 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssub_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p257, result, gvl);
}


int* p258;
// CHECK-O2-LABEL: @test_vssub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p258 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssub_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p258, result, gvl);
}


long* p259;
// CHECK-O2-LABEL: @test_vssub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p259 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssub_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p259, result, gvl);
}


signed char* p260;
// CHECK-O2-LABEL: @test_vssubu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p260 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssubu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p260, result, gvl);
}


short* p261;
// CHECK-O2-LABEL: @test_vssubu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p261 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssubu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p261, result, gvl);
}


int* p262;
// CHECK-O2-LABEL: @test_vssubu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p262 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssubu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p262, result, gvl);
}


long* p263;
// CHECK-O2-LABEL: @test_vssubu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p263 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssubu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p263, result, gvl);
}


signed char* p264;
// CHECK-O2-LABEL: @test_vssubu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p264 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssubu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p264, result, gvl);
}


short* p265;
// CHECK-O2-LABEL: @test_vssubu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p265 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssubu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p265, result, gvl);
}


int* p266;
// CHECK-O2-LABEL: @test_vssubu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p266 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssubu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p266, result, gvl);
}


long* p267;
// CHECK-O2-LABEL: @test_vssubu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p267 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssubu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssubu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p267, result, gvl);
}


signed char* p268;
// CHECK-O2-LABEL: @test_vaadd_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p268 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vaadd_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p268, result, gvl);
}


short* p269;
// CHECK-O2-LABEL: @test_vaadd_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p269 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vaadd_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p269, result, gvl);
}


int* p270;
// CHECK-O2-LABEL: @test_vaadd_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p270 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vaadd_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p270, result, gvl);
}


long* p271;
// CHECK-O2-LABEL: @test_vaadd_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p271 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vaadd_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p271, result, gvl);
}


signed char* p272;
// CHECK-O2-LABEL: @test_vaadd_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p272 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vaadd_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p272, result, gvl);
}


short* p273;
// CHECK-O2-LABEL: @test_vaadd_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p273 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vaadd_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p273, result, gvl);
}


int* p274;
// CHECK-O2-LABEL: @test_vaadd_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p274 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vaadd_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p274, result, gvl);
}


long* p275;
// CHECK-O2-LABEL: @test_vaadd_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p275 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vaadd_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vaadd_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p275, result, gvl);
}


signed char* p276;
// CHECK-O2-LABEL: @test_vasub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p276 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vasub_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p276, result, gvl);
}


short* p277;
// CHECK-O2-LABEL: @test_vasub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p277 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vasub_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p277, result, gvl);
}


int* p278;
// CHECK-O2-LABEL: @test_vasub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p278 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vasub_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p278, result, gvl);
}


long* p279;
// CHECK-O2-LABEL: @test_vasub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p279 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vasub_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p279, result, gvl);
}


signed char* p280;
// CHECK-O2-LABEL: @test_vasub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p280 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vasub_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p280, result, gvl);
}


short* p281;
// CHECK-O2-LABEL: @test_vasub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p281 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vasub_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p281, result, gvl);
}


int* p282;
// CHECK-O2-LABEL: @test_vasub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p282 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vasub_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p282, result, gvl);
}


long* p283;
// CHECK-O2-LABEL: @test_vasub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p283 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vasub_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vasub_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p283, result, gvl);
}


signed char* p284;
// CHECK-O2-LABEL: @test_vsmul_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p284 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vsmul_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p284, result, gvl);
}


short* p285;
// CHECK-O2-LABEL: @test_vsmul_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p285 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vsmul_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p285, result, gvl);
}


int* p286;
// CHECK-O2-LABEL: @test_vsmul_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p286 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vsmul_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p286, result, gvl);
}


long* p287;
// CHECK-O2-LABEL: @test_vsmul_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p287 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vsmul_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p287, result, gvl);
}


signed char* p288;
// CHECK-O2-LABEL: @test_vsmul_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p288 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vsmul_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p288, result, gvl);
}


short* p289;
// CHECK-O2-LABEL: @test_vsmul_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p289 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vsmul_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p289, result, gvl);
}


int* p290;
// CHECK-O2-LABEL: @test_vsmul_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p290 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vsmul_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p290, result, gvl);
}


long* p291;
// CHECK-O2-LABEL: @test_vsmul_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p291 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsmul_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vsmul_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p291, result, gvl);
}


signed char* p292;
// CHECK-O2-LABEL: @test_vssrl_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p292 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssrl_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p292, result, gvl);
}


short* p293;
// CHECK-O2-LABEL: @test_vssrl_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p293 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssrl_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p293, result, gvl);
}


int* p294;
// CHECK-O2-LABEL: @test_vssrl_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p294 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssrl_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p294, result, gvl);
}


long* p295;
// CHECK-O2-LABEL: @test_vssrl_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p295 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssrl_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p295, result, gvl);
}


signed char* p296;
// CHECK-O2-LABEL: @test_vssrl_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p296 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssrl_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p296, result, gvl);
}


short* p297;
// CHECK-O2-LABEL: @test_vssrl_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p297 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssrl_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p297, result, gvl);
}


int* p298;
// CHECK-O2-LABEL: @test_vssrl_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p298 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssrl_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p298, result, gvl);
}


long* p299;
// CHECK-O2-LABEL: @test_vssrl_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p299 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssrl_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssrl_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p299, result, gvl);
}


signed char* p300;
// CHECK-O2-LABEL: @test_vssra_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p300 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vssra_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p300, result, gvl);
}


short* p301;
// CHECK-O2-LABEL: @test_vssra_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p301 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vssra_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p301, result, gvl);
}


int* p302;
// CHECK-O2-LABEL: @test_vssra_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p302 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vssra_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p302, result, gvl);
}


long* p303;
// CHECK-O2-LABEL: @test_vssra_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p303 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vssra_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p303, result, gvl);
}


signed char* p304;
// CHECK-O2-LABEL: @test_vssra_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p304 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vssra_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p304, result, gvl);
}


short* p305;
// CHECK-O2-LABEL: @test_vssra_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p305 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vssra_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p305, result, gvl);
}


int* p306;
// CHECK-O2-LABEL: @test_vssra_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p306 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vssra_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p306, result, gvl);
}


long* p307;
// CHECK-O2-LABEL: @test_vssra_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p307 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vssra_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vssra_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p307, result, gvl);
}


float* p308;
// CHECK-O2-LABEL: @test_vfadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p308 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfadd_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p308, result, gvl);
}


double* p309;
// CHECK-O2-LABEL: @test_vfadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p309 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfadd_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p309, result, gvl);
}


float* p310;
// CHECK-O2-LABEL: @test_vfadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p310 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfadd_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p310, result, gvl);
}


double* p311;
// CHECK-O2-LABEL: @test_vfadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p311 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfadd_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfadd_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p311, result, gvl);
}


float* p312;
// CHECK-O2-LABEL: @test_vfsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p312 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsub_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p312, result, gvl);
}


double* p313;
// CHECK-O2-LABEL: @test_vfsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p313 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsub_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p313, result, gvl);
}


float* p314;
// CHECK-O2-LABEL: @test_vfsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p314 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsub_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p314, result, gvl);
}


double* p315;
// CHECK-O2-LABEL: @test_vfsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p315 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsub_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsub_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p315, result, gvl);
}


float* p316;
// CHECK-O2-LABEL: @test_vfmul_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p316 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfmul_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p316, result, gvl);
}


double* p317;
// CHECK-O2-LABEL: @test_vfmul_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p317 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfmul_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p317, result, gvl);
}


float* p318;
// CHECK-O2-LABEL: @test_vfmul_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p318 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmul_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p318, result, gvl);
}


double* p319;
// CHECK-O2-LABEL: @test_vfmul_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p319 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmul_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmul_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p319, result, gvl);
}


float* p320;
// CHECK-O2-LABEL: @test_vfdiv_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p320 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfdiv_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p320, result, gvl);
}


double* p321;
// CHECK-O2-LABEL: @test_vfdiv_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p321 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfdiv_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p321, result, gvl);
}


float* p322;
// CHECK-O2-LABEL: @test_vfdiv_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p322 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfdiv_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p322, result, gvl);
}


double* p323;
// CHECK-O2-LABEL: @test_vfdiv_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p323 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdiv_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfdiv_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p323, result, gvl);
}


float* p324;
// CHECK-O2-LABEL: @test_vfmadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmadd.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p324 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmadd_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p324, result, gvl);
}


double* p325;
// CHECK-O2-LABEL: @test_vfmadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p325 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmadd_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p325, result, gvl);
}


float* p326;
// CHECK-O2-LABEL: @test_vfmadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmadd.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p326 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmadd_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p326, result, gvl);
}


double* p327;
// CHECK-O2-LABEL: @test_vfmadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmadd.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p327 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmadd_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmadd_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p327, result, gvl);
}


float* p328;
// CHECK-O2-LABEL: @test_vfnmadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmadd.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p328 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmadd_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p328, result, gvl);
}


double* p329;
// CHECK-O2-LABEL: @test_vfnmadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p329 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmadd_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p329, result, gvl);
}


float* p330;
// CHECK-O2-LABEL: @test_vfnmadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmadd.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p330 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmadd_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p330, result, gvl);
}


double* p331;
// CHECK-O2-LABEL: @test_vfnmadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmadd.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p331 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmadd_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmadd_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p331, result, gvl);
}


float* p332;
// CHECK-O2-LABEL: @test_vfmsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsub.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p332 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmsub_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p332, result, gvl);
}


double* p333;
// CHECK-O2-LABEL: @test_vfmsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p333 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmsub_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p333, result, gvl);
}


float* p334;
// CHECK-O2-LABEL: @test_vfmsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsub.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p334 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmsub_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p334, result, gvl);
}


double* p335;
// CHECK-O2-LABEL: @test_vfmsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsub.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p335 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsub_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmsub_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p335, result, gvl);
}


float* p336;
// CHECK-O2-LABEL: @test_vfnmsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsub.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p336 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmsub_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p336, result, gvl);
}


double* p337;
// CHECK-O2-LABEL: @test_vfnmsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p337 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmsub_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p337, result, gvl);
}


float* p338;
// CHECK-O2-LABEL: @test_vfnmsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsub.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p338 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmsub_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p338, result, gvl);
}


double* p339;
// CHECK-O2-LABEL: @test_vfnmsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsub.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p339 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsub_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmsub_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p339, result, gvl);
}


float* p340;
// CHECK-O2-LABEL: @test_vfmacc_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmacc.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p340 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmacc_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p340, result, gvl);
}


double* p341;
// CHECK-O2-LABEL: @test_vfmacc_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p341 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmacc_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p341, result, gvl);
}


float* p342;
// CHECK-O2-LABEL: @test_vfmacc_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmacc.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p342 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmacc_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p342, result, gvl);
}


double* p343;
// CHECK-O2-LABEL: @test_vfmacc_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p343 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmacc_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmacc_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p343, result, gvl);
}


float* p344;
// CHECK-O2-LABEL: @test_vfnmacc_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmacc.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p344 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmacc_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p344, result, gvl);
}


double* p345;
// CHECK-O2-LABEL: @test_vfnmacc_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p345 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmacc_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p345, result, gvl);
}


float* p346;
// CHECK-O2-LABEL: @test_vfnmacc_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmacc.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p346 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmacc_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p346, result, gvl);
}


double* p347;
// CHECK-O2-LABEL: @test_vfnmacc_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmacc.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p347 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmacc_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmacc_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p347, result, gvl);
}


float* p348;
// CHECK-O2-LABEL: @test_vfmsac_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsac.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p348 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfmsac_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p348, result, gvl);
}


double* p349;
// CHECK-O2-LABEL: @test_vfmsac_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p349 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfmsac_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p349, result, gvl);
}


float* p350;
// CHECK-O2-LABEL: @test_vfmsac_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmsac.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p350 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmsac_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p350, result, gvl);
}


double* p351;
// CHECK-O2-LABEL: @test_vfmsac_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmsac.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p351 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmsac_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmsac_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p351, result, gvl);
}


float* p352;
// CHECK-O2-LABEL: @test_vfnmsac_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsac.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p352 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  result = __builtin_epi_vfnmsac_2xf32(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_2xf32(p352, result, gvl);
}


double* p353;
// CHECK-O2-LABEL: @test_vfnmsac_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p353 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  result = __builtin_epi_vfnmsac_1xf64(lhs, rhs, acc, gvl);
  __builtin_epi_vstore_1xf64(p353, result, gvl);
}


float* p354;
// CHECK-O2-LABEL: @test_vfnmsac_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfnmsac.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p354 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xf32 acc;
  __epi_2xi1 mask;
  result = __builtin_epi_vfnmsac_2xf32_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_2xf32(p354, result, gvl);
}


double* p355;
// CHECK-O2-LABEL: @test_vfnmsac_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p355 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfnmsac_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xf64 acc;
  __epi_1xi1 mask;
  result = __builtin_epi_vfnmsac_1xf64_mask(lhs, rhs, acc, mask, gvl);
  __builtin_epi_vstore_1xf64(p355, result, gvl);
}


float* p356;
// CHECK-O2-LABEL: @test_vfsqrt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsqrt.nxv2f32.nxv2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p356 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfsqrt_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p356, result, gvl);
}


double* p357;
// CHECK-O2-LABEL: @test_vfsqrt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsqrt.nxv1f64.nxv1f64(<vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p357 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vfsqrt_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p357, result, gvl);
}


float* p358;
// CHECK-O2-LABEL: @test_vfsqrt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsqrt.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p358 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsqrt_2xf32_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p358, result, gvl);
}


double* p359;
// CHECK-O2-LABEL: @test_vfsqrt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsqrt.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p359 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsqrt_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsqrt_1xf64_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p359, result, gvl);
}


float* p360;
// CHECK-O2-LABEL: @test_vfmin_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p360 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfmin_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p360, result, gvl);
}


double* p361;
// CHECK-O2-LABEL: @test_vfmin_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p361 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfmin_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p361, result, gvl);
}


float* p362;
// CHECK-O2-LABEL: @test_vfmin_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p362 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmin_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p362, result, gvl);
}


double* p363;
// CHECK-O2-LABEL: @test_vfmin_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p363 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmin_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmin_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p363, result, gvl);
}


float* p364;
// CHECK-O2-LABEL: @test_vfmax_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p364 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfmax_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p364, result, gvl);
}


double* p365;
// CHECK-O2-LABEL: @test_vfmax_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p365 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfmax_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p365, result, gvl);
}


float* p366;
// CHECK-O2-LABEL: @test_vfmax_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p366 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmax_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p366, result, gvl);
}


double* p367;
// CHECK-O2-LABEL: @test_vfmax_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p367 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmax_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmax_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p367, result, gvl);
}


float* p368;
// CHECK-O2-LABEL: @test_vfsgnj_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p368 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsgnj_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p368, result, gvl);
}


double* p369;
// CHECK-O2-LABEL: @test_vfsgnj_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p369 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsgnj_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p369, result, gvl);
}


float* p370;
// CHECK-O2-LABEL: @test_vfsgnj_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p370 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsgnj_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p370, result, gvl);
}


double* p371;
// CHECK-O2-LABEL: @test_vfsgnj_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p371 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnj_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsgnj_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p371, result, gvl);
}


float* p372;
// CHECK-O2-LABEL: @test_vfsgnjn_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p372 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsgnjn_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p372, result, gvl);
}


double* p373;
// CHECK-O2-LABEL: @test_vfsgnjn_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p373 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsgnjn_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p373, result, gvl);
}


float* p374;
// CHECK-O2-LABEL: @test_vfsgnjn_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p374 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsgnjn_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p374, result, gvl);
}


double* p375;
// CHECK-O2-LABEL: @test_vfsgnjn_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p375 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjn_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsgnjn_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p375, result, gvl);
}


float* p376;
// CHECK-O2-LABEL: @test_vfsgnjx_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p376 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfsgnjx_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p376, result, gvl);
}


double* p377;
// CHECK-O2-LABEL: @test_vfsgnjx_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p377 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfsgnjx_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p377, result, gvl);
}


float* p378;
// CHECK-O2-LABEL: @test_vfsgnjx_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p378 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfsgnjx_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p378, result, gvl);
}


double* p379;
// CHECK-O2-LABEL: @test_vfsgnjx_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p379 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfsgnjx_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfsgnjx_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p379, result, gvl);
}


unsigned int* p380;
// CHECK-O2-LABEL: @test_vmfeq_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p380 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfeq_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vmfeq_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p380, result);
}


unsigned long* p381;
// CHECK-O2-LABEL: @test_vmfeq_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfeq.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p381 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfeq_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vmfeq_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p381, result);
}


unsigned int* p382;
// CHECK-O2-LABEL: @test_vmfeq_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x i1> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p382 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfeq_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmfeq_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p382, result);
}


unsigned long* p383;
// CHECK-O2-LABEL: @test_vmfeq_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfeq.mask.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x i1> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p383 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfeq_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmfeq_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p383, result);
}


unsigned int* p384;
// CHECK-O2-LABEL: @test_vmfne_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p384 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfne_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vmfne_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p384, result);
}


unsigned long* p385;
// CHECK-O2-LABEL: @test_vmfne_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfne.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p385 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfne_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vmfne_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p385, result);
}


unsigned int* p386;
// CHECK-O2-LABEL: @test_vmfne_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x i1> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p386 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfne_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmfne_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p386, result);
}


unsigned long* p387;
// CHECK-O2-LABEL: @test_vmfne_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfne.mask.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x i1> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p387 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfne_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmfne_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p387, result);
}


unsigned int* p388;
// CHECK-O2-LABEL: @test_vmflt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p388 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmflt_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vmflt_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p388, result);
}


unsigned long* p389;
// CHECK-O2-LABEL: @test_vmflt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmflt.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p389 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmflt_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vmflt_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p389, result);
}


unsigned int* p390;
// CHECK-O2-LABEL: @test_vmflt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x i1> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p390 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmflt_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmflt_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p390, result);
}


unsigned long* p391;
// CHECK-O2-LABEL: @test_vmflt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmflt.mask.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x i1> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p391 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmflt_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmflt_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p391, result);
}


unsigned int* p392;
// CHECK-O2-LABEL: @test_vmfle_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p392 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfle_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vmfle_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p392, result);
}


unsigned long* p393;
// CHECK-O2-LABEL: @test_vmfle_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfle.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p393 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfle_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vmfle_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p393, result);
}


unsigned int* p394;
// CHECK-O2-LABEL: @test_vmfle_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x i1> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p394 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfle_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmfle_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p394, result);
}


unsigned long* p395;
// CHECK-O2-LABEL: @test_vmfle_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfle.mask.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x i1> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p395 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfle_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmfle_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p395, result);
}


unsigned int* p396;
// CHECK-O2-LABEL: @test_vmfgt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p396 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfgt_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vmfgt_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p396, result);
}


unsigned long* p397;
// CHECK-O2-LABEL: @test_vmfgt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfgt.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p397 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfgt_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vmfgt_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p397, result);
}


unsigned int* p398;
// CHECK-O2-LABEL: @test_vmfgt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x i1> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p398 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfgt_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmfgt_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p398, result);
}


unsigned long* p399;
// CHECK-O2-LABEL: @test_vmfgt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfgt.mask.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x i1> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p399 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfgt_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmfgt_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p399, result);
}


unsigned int* p400;
// CHECK-O2-LABEL: @test_vmfge_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p400 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfge_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vmfge_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p400, result);
}


unsigned long* p401;
// CHECK-O2-LABEL: @test_vmfge_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfge.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p401 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfge_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vmfge_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p401, result);
}


unsigned int* p402;
// CHECK-O2-LABEL: @test_vmfge_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x i1> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p402 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmfge_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmfge_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p402, result);
}


unsigned long* p403;
// CHECK-O2-LABEL: @test_vmfge_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmfge.mask.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x i1> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p403 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmfge_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmfge_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p403, result);
}


unsigned int* p404;
// CHECK-O2-LABEL: @test_vmford_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmford.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p404 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmford_2xf32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vmford_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p404, result);
}


unsigned long* p405;
// CHECK-O2-LABEL: @test_vmford_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmford.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p405 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmford_1xf64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vmford_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p405, result);
}


unsigned int* p406;
// CHECK-O2-LABEL: @test_vmford_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmford.mask.nxv2i1.nxv2f32.nxv2f32(<vscale x 2 x i1> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p406 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmford_2xf32_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmford_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p406, result);
}


unsigned long* p407;
// CHECK-O2-LABEL: @test_vmford_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmford.mask.nxv1i1.nxv1f64.nxv1f64(<vscale x 1 x i1> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p407 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmford_1xf64_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmford_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p407, result);
}


float* p408;
// CHECK-O2-LABEL: @test_vfmerge_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmerge.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p408 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmerge_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfmerge_2xf32(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p408, result, gvl);
}


double* p409;
// CHECK-O2-LABEL: @test_vfmerge_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p409 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfmerge_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfmerge_1xf64(lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p409, result, gvl);
}


int* p410;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi32_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.nxv2i32.nxv2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p410 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_2xi32_2xf32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfcvt_xu_f_2xi32_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p410, result, gvl);
}


long* p411;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_1xi64_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.nxv1i64.nxv1f64(<vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p411 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_1xi64_1xf64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vfcvt_xu_f_1xi64_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p411, result, gvl);
}


int* p412;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi32_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv2i32.nxv2f32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p412 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_2xi32_2xf32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_xu_f_2xi32_2xf32_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p412, result, gvl);
}


long* p413;
// CHECK-O2-LABEL: @test_vfcvt_xu_f_1xi64_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv1i64.nxv1f64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p413 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_xu_f_1xi64_1xf64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xf64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_xu_f_1xi64_1xf64_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p413, result, gvl);
}


int* p414;
// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi32_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.nxv2i32.nxv2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p414 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_2xi32_2xf32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfcvt_x_f_2xi32_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p414, result, gvl);
}


long* p415;
// CHECK-O2-LABEL: @test_vfcvt_x_f_1xi64_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.nxv1i64.nxv1f64(<vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p415 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_1xi64_1xf64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vfcvt_x_f_1xi64_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p415, result, gvl);
}


int* p416;
// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi32_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.mask.nxv2i32.nxv2f32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p416 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_2xi32_2xf32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_x_f_2xi32_2xf32_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p416, result, gvl);
}


long* p417;
// CHECK-O2-LABEL: @test_vfcvt_x_f_1xi64_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.mask.nxv1i64.nxv1f64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p417 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_x_f_1xi64_1xf64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xf64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_x_f_1xi64_1xf64_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p417, result, gvl);
}


float* p418;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf32_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.nxv2f32.nxv2i32(<vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p418 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_2xf32_2xi32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 lhs;
  result = __builtin_epi_vfcvt_f_xu_2xf32_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p418, result, gvl);
}


double* p419;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_1xf64_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.nxv1f64.nxv1i64(<vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p419 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_1xf64_1xi64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 lhs;
  result = __builtin_epi_vfcvt_f_xu_1xf64_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p419, result, gvl);
}


float* p420;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf32_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.mask.nxv2f32.nxv2i32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p420 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_2xf32_2xi32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xi32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_f_xu_2xf32_2xi32_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p420, result, gvl);
}


double* p421;
// CHECK-O2-LABEL: @test_vfcvt_f_xu_1xf64_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.mask.nxv1f64.nxv1i64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p421 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_xu_1xf64_1xi64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xi64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_f_xu_1xf64_1xi64_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p421, result, gvl);
}


float* p422;
// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf32_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.x.nxv2f32.nxv2i32(<vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p422 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_2xf32_2xi32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xi32 lhs;
  result = __builtin_epi_vfcvt_f_x_2xf32_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p422, result, gvl);
}


double* p423;
// CHECK-O2-LABEL: @test_vfcvt_f_x_1xf64_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.x.nxv1f64.nxv1i64(<vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p423 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_1xf64_1xi64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xi64 lhs;
  result = __builtin_epi_vfcvt_f_x_1xf64_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p423, result, gvl);
}


float* p424;
// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf32_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfcvt.f.x.mask.nxv2f32.nxv2i32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p424 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_2xf32_2xi32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xi32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfcvt_f_x_2xf32_2xi32_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p424, result, gvl);
}


double* p425;
// CHECK-O2-LABEL: @test_vfcvt_f_x_1xf64_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfcvt.f.x.mask.nxv1f64.nxv1i64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p425 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfcvt_f_x_1xf64_1xi64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xi64 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfcvt_f_x_1xf64_1xi64_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p425, result, gvl);
}


double* p426;
// CHECK-O2-LABEL: @test_vfwcvt_f_f_2xf64_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.nxv2f64.nxv2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x double>*, <vscale x 2 x double>** bitcast (double** @p426 to <vscale x 2 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f64(<vscale x 2 x double> [[TMP0]], <vscale x 2 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfwcvt_f_f_2xf64_2xf32(unsigned long gvl)
{
  __epi_2xf64 result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vfwcvt_f_f_2xf64_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xf64(p426, result, gvl);
}


double* p427;
// CHECK-O2-LABEL: @test_vfwcvt_f_f_2xf64_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.mask.nxv2f64.nxv2f32.nxv2i1(<vscale x 2 x double> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x double>*, <vscale x 2 x double>** bitcast (double** @p427 to <vscale x 2 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f64(<vscale x 2 x double> [[TMP0]], <vscale x 2 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfwcvt_f_f_2xf64_2xf32_mask(unsigned long gvl)
{
  __epi_2xf64 result;
  __epi_2xf64 merge;
  __epi_2xf32 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfwcvt_f_f_2xf64_2xf32_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_2xf64(p427, result, gvl);
}


float* p428;
// CHECK-O2-LABEL: @test_vfncvt_f_f_2xf32_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfncvt.f.f.nxv2f32.nxv2f64(<vscale x 2 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p428 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfncvt_f_f_2xf32_2xf64(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf64 lhs;
  result = __builtin_epi_vfncvt_f_f_2xf32_2xf64(lhs, gvl);
  __builtin_epi_vstore_2xf32(p428, result, gvl);
}


float* p429;
// CHECK-O2-LABEL: @test_vfncvt_f_f_2xf32_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfncvt.f.f.mask.nxv2f32.nxv2f64.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x double> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p429 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfncvt_f_f_2xf32_2xf64_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf64 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfncvt_f_f_2xf32_2xf64_mask(merge, lhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p429, result, gvl);
}


signed char* p430;
// CHECK-O2-LABEL: @test_vredsum_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredsum.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p430 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredsum_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p430, result, gvl);
}


short* p431;
// CHECK-O2-LABEL: @test_vredsum_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredsum.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p431 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredsum_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p431, result, gvl);
}


int* p432;
// CHECK-O2-LABEL: @test_vredsum_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredsum.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p432 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredsum_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p432, result, gvl);
}


long* p433;
// CHECK-O2-LABEL: @test_vredsum_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredsum.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p433 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredsum_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p433, result, gvl);
}


signed char* p434;
// CHECK-O2-LABEL: @test_vredsum_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredsum.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p434 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredsum_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p434, result, gvl);
}


short* p435;
// CHECK-O2-LABEL: @test_vredsum_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredsum.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p435 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredsum_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p435, result, gvl);
}


int* p436;
// CHECK-O2-LABEL: @test_vredsum_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredsum.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p436 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredsum_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p436, result, gvl);
}


long* p437;
// CHECK-O2-LABEL: @test_vredsum_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredsum.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p437 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredsum_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredsum_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p437, result, gvl);
}


signed char* p438;
// CHECK-O2-LABEL: @test_vredmaxu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmaxu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p438 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredmaxu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p438, result, gvl);
}


short* p439;
// CHECK-O2-LABEL: @test_vredmaxu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmaxu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p439 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredmaxu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p439, result, gvl);
}


int* p440;
// CHECK-O2-LABEL: @test_vredmaxu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmaxu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p440 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredmaxu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p440, result, gvl);
}


long* p441;
// CHECK-O2-LABEL: @test_vredmaxu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmaxu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p441 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredmaxu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p441, result, gvl);
}


signed char* p442;
// CHECK-O2-LABEL: @test_vredmaxu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmaxu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p442 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredmaxu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p442, result, gvl);
}


short* p443;
// CHECK-O2-LABEL: @test_vredmaxu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmaxu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p443 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredmaxu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p443, result, gvl);
}


int* p444;
// CHECK-O2-LABEL: @test_vredmaxu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmaxu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p444 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredmaxu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p444, result, gvl);
}


long* p445;
// CHECK-O2-LABEL: @test_vredmaxu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmaxu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p445 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmaxu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredmaxu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p445, result, gvl);
}


signed char* p446;
// CHECK-O2-LABEL: @test_vredmax_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmax.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p446 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredmax_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p446, result, gvl);
}


short* p447;
// CHECK-O2-LABEL: @test_vredmax_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmax.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p447 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredmax_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p447, result, gvl);
}


int* p448;
// CHECK-O2-LABEL: @test_vredmax_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmax.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p448 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredmax_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p448, result, gvl);
}


long* p449;
// CHECK-O2-LABEL: @test_vredmax_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmax.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p449 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredmax_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p449, result, gvl);
}


signed char* p450;
// CHECK-O2-LABEL: @test_vredmax_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmax.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p450 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredmax_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p450, result, gvl);
}


short* p451;
// CHECK-O2-LABEL: @test_vredmax_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmax.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p451 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredmax_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p451, result, gvl);
}


int* p452;
// CHECK-O2-LABEL: @test_vredmax_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmax.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p452 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredmax_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p452, result, gvl);
}


long* p453;
// CHECK-O2-LABEL: @test_vredmax_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmax.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p453 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmax_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredmax_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p453, result, gvl);
}


signed char* p454;
// CHECK-O2-LABEL: @test_vredmin_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmin.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p454 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredmin_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p454, result, gvl);
}


short* p455;
// CHECK-O2-LABEL: @test_vredmin_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmin.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p455 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredmin_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p455, result, gvl);
}


int* p456;
// CHECK-O2-LABEL: @test_vredmin_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmin.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p456 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredmin_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p456, result, gvl);
}


long* p457;
// CHECK-O2-LABEL: @test_vredmin_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmin.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p457 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredmin_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p457, result, gvl);
}


signed char* p458;
// CHECK-O2-LABEL: @test_vredmin_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredmin.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p458 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredmin_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p458, result, gvl);
}


short* p459;
// CHECK-O2-LABEL: @test_vredmin_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredmin.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p459 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredmin_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p459, result, gvl);
}


int* p460;
// CHECK-O2-LABEL: @test_vredmin_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredmin.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p460 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredmin_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p460, result, gvl);
}


long* p461;
// CHECK-O2-LABEL: @test_vredmin_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredmin.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p461 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredmin_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredmin_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p461, result, gvl);
}


signed char* p462;
// CHECK-O2-LABEL: @test_vredminu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredminu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p462 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredminu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p462, result, gvl);
}


short* p463;
// CHECK-O2-LABEL: @test_vredminu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredminu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p463 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredminu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p463, result, gvl);
}


int* p464;
// CHECK-O2-LABEL: @test_vredminu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredminu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p464 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredminu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p464, result, gvl);
}


long* p465;
// CHECK-O2-LABEL: @test_vredminu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredminu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p465 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredminu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p465, result, gvl);
}


signed char* p466;
// CHECK-O2-LABEL: @test_vredminu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredminu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p466 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredminu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p466, result, gvl);
}


short* p467;
// CHECK-O2-LABEL: @test_vredminu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredminu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p467 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredminu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p467, result, gvl);
}


int* p468;
// CHECK-O2-LABEL: @test_vredminu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredminu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p468 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredminu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p468, result, gvl);
}


long* p469;
// CHECK-O2-LABEL: @test_vredminu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredminu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p469 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredminu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredminu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p469, result, gvl);
}


signed char* p470;
// CHECK-O2-LABEL: @test_vredand_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredand.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p470 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredand_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p470, result, gvl);
}


short* p471;
// CHECK-O2-LABEL: @test_vredand_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredand.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p471 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredand_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p471, result, gvl);
}


int* p472;
// CHECK-O2-LABEL: @test_vredand_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredand.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p472 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredand_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p472, result, gvl);
}


long* p473;
// CHECK-O2-LABEL: @test_vredand_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredand.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p473 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredand_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p473, result, gvl);
}


signed char* p474;
// CHECK-O2-LABEL: @test_vredand_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredand.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p474 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredand_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p474, result, gvl);
}


short* p475;
// CHECK-O2-LABEL: @test_vredand_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredand.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p475 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredand_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p475, result, gvl);
}


int* p476;
// CHECK-O2-LABEL: @test_vredand_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredand.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p476 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredand_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p476, result, gvl);
}


long* p477;
// CHECK-O2-LABEL: @test_vredand_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredand.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p477 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredand_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredand_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p477, result, gvl);
}


signed char* p478;
// CHECK-O2-LABEL: @test_vredor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredor.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p478 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p478, result, gvl);
}


short* p479;
// CHECK-O2-LABEL: @test_vredor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredor.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p479 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p479, result, gvl);
}


int* p480;
// CHECK-O2-LABEL: @test_vredor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredor.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p480 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p480, result, gvl);
}


long* p481;
// CHECK-O2-LABEL: @test_vredor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p481 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p481, result, gvl);
}


signed char* p482;
// CHECK-O2-LABEL: @test_vredor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredor.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p482 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredor_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p482, result, gvl);
}


short* p483;
// CHECK-O2-LABEL: @test_vredor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredor.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p483 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredor_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p483, result, gvl);
}


int* p484;
// CHECK-O2-LABEL: @test_vredor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredor.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p484 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredor_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p484, result, gvl);
}


long* p485;
// CHECK-O2-LABEL: @test_vredor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredor.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p485 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredor_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p485, result, gvl);
}


signed char* p486;
// CHECK-O2-LABEL: @test_vredxor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredxor.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p486 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vredxor_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p486, result, gvl);
}


short* p487;
// CHECK-O2-LABEL: @test_vredxor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredxor.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p487 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vredxor_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p487, result, gvl);
}


int* p488;
// CHECK-O2-LABEL: @test_vredxor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredxor.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p488 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vredxor_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p488, result, gvl);
}


long* p489;
// CHECK-O2-LABEL: @test_vredxor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredxor.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p489 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vredxor_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p489, result, gvl);
}


signed char* p490;
// CHECK-O2-LABEL: @test_vredxor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vredxor.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p490 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vredxor_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p490, result, gvl);
}


short* p491;
// CHECK-O2-LABEL: @test_vredxor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vredxor.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p491 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vredxor_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p491, result, gvl);
}


int* p492;
// CHECK-O2-LABEL: @test_vredxor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vredxor.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p492 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vredxor_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p492, result, gvl);
}


long* p493;
// CHECK-O2-LABEL: @test_vredxor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vredxor.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p493 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vredxor_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vredxor_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p493, result, gvl);
}


float* p494;
// CHECK-O2-LABEL: @test_vfredosum_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredosum.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p494 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredosum_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p494, result, gvl);
}


double* p495;
// CHECK-O2-LABEL: @test_vfredosum_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredosum.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p495 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredosum_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p495, result, gvl);
}


float* p496;
// CHECK-O2-LABEL: @test_vfredosum_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredosum.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p496 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredosum_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p496, result, gvl);
}


double* p497;
// CHECK-O2-LABEL: @test_vfredosum_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredosum.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p497 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredosum_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredosum_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p497, result, gvl);
}


float* p498;
// CHECK-O2-LABEL: @test_vfredsum_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredsum.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p498 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredsum_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p498, result, gvl);
}


double* p499;
// CHECK-O2-LABEL: @test_vfredsum_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredsum.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p499 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredsum_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p499, result, gvl);
}


float* p500;
// CHECK-O2-LABEL: @test_vfredsum_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredsum.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p500 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredsum_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p500, result, gvl);
}


double* p501;
// CHECK-O2-LABEL: @test_vfredsum_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredsum.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p501 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredsum_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredsum_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p501, result, gvl);
}


float* p502;
// CHECK-O2-LABEL: @test_vfredmax_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmax.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p502 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredmax_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p502, result, gvl);
}


double* p503;
// CHECK-O2-LABEL: @test_vfredmax_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmax.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p503 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredmax_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p503, result, gvl);
}


float* p504;
// CHECK-O2-LABEL: @test_vfredmax_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmax.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p504 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredmax_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p504, result, gvl);
}


double* p505;
// CHECK-O2-LABEL: @test_vfredmax_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmax.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p505 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmax_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredmax_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p505, result, gvl);
}


float* p506;
// CHECK-O2-LABEL: @test_vfredmin_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmin.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p506 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfredmin_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p506, result, gvl);
}


double* p507;
// CHECK-O2-LABEL: @test_vfredmin_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmin.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p507 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfredmin_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p507, result, gvl);
}


float* p508;
// CHECK-O2-LABEL: @test_vfredmin_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfredmin.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p508 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfredmin_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p508, result, gvl);
}


double* p509;
// CHECK-O2-LABEL: @test_vfredmin_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfredmin.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p509 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfredmin_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfredmin_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p509, result, gvl);
}


unsigned long* p510;
// CHECK-O2-LABEL: @test_vmand_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmand.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p510 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmand_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmand_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p510, result);
}


unsigned int* p511;
// CHECK-O2-LABEL: @test_vmand_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmand.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p511 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmand_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmand_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p511, result);
}


unsigned short* p512;
// CHECK-O2-LABEL: @test_vmand_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmand.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p512 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmand_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmand_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p512, result);
}


unsigned char* p513;
// CHECK-O2-LABEL: @test_vmand_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmand.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p513 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmand_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmand_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p513, result);
}


unsigned long* p514;
// CHECK-O2-LABEL: @test_vmnand_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmnand.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p514 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmnand_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p514, result);
}


unsigned int* p515;
// CHECK-O2-LABEL: @test_vmnand_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmnand.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p515 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmnand_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p515, result);
}


unsigned short* p516;
// CHECK-O2-LABEL: @test_vmnand_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmnand.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p516 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmnand_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p516, result);
}


unsigned char* p517;
// CHECK-O2-LABEL: @test_vmnand_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmnand.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p517 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmnand_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmnand_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p517, result);
}


unsigned long* p518;
// CHECK-O2-LABEL: @test_vmandnot_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmandnot.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p518 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmandnot_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p518, result);
}


unsigned int* p519;
// CHECK-O2-LABEL: @test_vmandnot_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmandnot.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p519 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmandnot_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p519, result);
}


unsigned short* p520;
// CHECK-O2-LABEL: @test_vmandnot_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmandnot.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p520 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmandnot_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p520, result);
}


unsigned char* p521;
// CHECK-O2-LABEL: @test_vmandnot_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmandnot.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p521 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmandnot_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmandnot_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p521, result);
}


unsigned long* p522;
// CHECK-O2-LABEL: @test_vmxor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmxor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p522 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmxor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p522, result);
}


unsigned int* p523;
// CHECK-O2-LABEL: @test_vmxor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmxor.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p523 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmxor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p523, result);
}


unsigned short* p524;
// CHECK-O2-LABEL: @test_vmxor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmxor.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p524 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmxor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p524, result);
}


unsigned char* p525;
// CHECK-O2-LABEL: @test_vmxor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmxor.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p525 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmxor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmxor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p525, result);
}


unsigned long* p526;
// CHECK-O2-LABEL: @test_vmor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p526 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p526, result);
}


unsigned int* p527;
// CHECK-O2-LABEL: @test_vmor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmor.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p527 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p527, result);
}


unsigned short* p528;
// CHECK-O2-LABEL: @test_vmor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmor.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p528 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p528, result);
}


unsigned char* p529;
// CHECK-O2-LABEL: @test_vmor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmor.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p529 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p529, result);
}


unsigned long* p530;
// CHECK-O2-LABEL: @test_vmnor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmnor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p530 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmnor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p530, result);
}


unsigned int* p531;
// CHECK-O2-LABEL: @test_vmnor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmnor.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p531 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmnor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p531, result);
}


unsigned short* p532;
// CHECK-O2-LABEL: @test_vmnor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmnor.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p532 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmnor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p532, result);
}


unsigned char* p533;
// CHECK-O2-LABEL: @test_vmnor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmnor.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p533 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmnor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmnor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p533, result);
}


unsigned long* p534;
// CHECK-O2-LABEL: @test_vmornot_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmornot.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p534 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmornot_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p534, result);
}


unsigned int* p535;
// CHECK-O2-LABEL: @test_vmornot_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmornot.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p535 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmornot_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p535, result);
}


unsigned short* p536;
// CHECK-O2-LABEL: @test_vmornot_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmornot.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p536 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmornot_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p536, result);
}


unsigned char* p537;
// CHECK-O2-LABEL: @test_vmornot_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmornot.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p537 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmornot_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmornot_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p537, result);
}


unsigned long* p538;
// CHECK-O2-LABEL: @test_vmxnor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmxnor.nxv1i1.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p538 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vmxnor_1xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi1(p538, result);
}


unsigned int* p539;
// CHECK-O2-LABEL: @test_vmxnor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmxnor.nxv2i1.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p539 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vmxnor_2xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi1(p539, result);
}


unsigned short* p540;
// CHECK-O2-LABEL: @test_vmxnor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmxnor.nxv4i1.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p540 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vmxnor_4xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi1(p540, result);
}


unsigned char* p541;
// CHECK-O2-LABEL: @test_vmxnor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmxnor.nxv8i1.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p541 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmxnor_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vmxnor_8xi1(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi1(p541, result);
}


signed long int* p542;
// CHECK-O2-LABEL: @test_vmpopc_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.nxv1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p542, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_1xi1(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmpopc_1xi1(lhs, gvl);
  *p542 = result;
}


signed long int* p543;
// CHECK-O2-LABEL: @test_vmpopc_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.nxv2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p543, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_2xi1(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmpopc_2xi1(lhs, gvl);
  *p543 = result;
}


signed long int* p544;
// CHECK-O2-LABEL: @test_vmpopc_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.nxv4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p544, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_4xi1(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmpopc_4xi1(lhs, gvl);
  *p544 = result;
}


signed long int* p545;
// CHECK-O2-LABEL: @test_vmpopc_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.nxv8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p545, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_8xi1(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmpopc_8xi1(lhs, gvl);
  *p545 = result;
}


signed long int* p546;
// CHECK-O2-LABEL: @test_vmpopc_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p546, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_1xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmpopc_1xi1_mask(lhs, mask, gvl);
  *p546 = result;
}


signed long int* p547;
// CHECK-O2-LABEL: @test_vmpopc_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p547, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_2xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmpopc_2xi1_mask(lhs, mask, gvl);
  *p547 = result;
}


signed long int* p548;
// CHECK-O2-LABEL: @test_vmpopc_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p548, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_4xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmpopc_4xi1_mask(lhs, mask, gvl);
  *p548 = result;
}


signed long int* p549;
// CHECK-O2-LABEL: @test_vmpopc_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmpopc.mask.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p549, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmpopc_8xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmpopc_8xi1_mask(lhs, mask, gvl);
  *p549 = result;
}


signed long int* p550;
// CHECK-O2-LABEL: @test_vmfirst_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.nxv1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p550, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_1xi1(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmfirst_1xi1(lhs, gvl);
  *p550 = result;
}


signed long int* p551;
// CHECK-O2-LABEL: @test_vmfirst_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.nxv2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p551, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_2xi1(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmfirst_2xi1(lhs, gvl);
  *p551 = result;
}


signed long int* p552;
// CHECK-O2-LABEL: @test_vmfirst_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.nxv4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p552, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_4xi1(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmfirst_4xi1(lhs, gvl);
  *p552 = result;
}


signed long int* p553;
// CHECK-O2-LABEL: @test_vmfirst_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.nxv8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p553, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_8xi1(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmfirst_8xi1(lhs, gvl);
  *p553 = result;
}


signed long int* p554;
// CHECK-O2-LABEL: @test_vmfirst_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p554, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_1xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmfirst_1xi1_mask(lhs, mask, gvl);
  *p554 = result;
}


signed long int* p555;
// CHECK-O2-LABEL: @test_vmfirst_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p555, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_2xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmfirst_2xi1_mask(lhs, mask, gvl);
  *p555 = result;
}


signed long int* p556;
// CHECK-O2-LABEL: @test_vmfirst_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p556, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_4xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmfirst_4xi1_mask(lhs, mask, gvl);
  *p556 = result;
}


signed long int* p557;
// CHECK-O2-LABEL: @test_vmfirst_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vmfirst.mask.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load i64*, i64** @p557, align 8, !tbaa !2
// CHECK-O2-NEXT:    store i64 [[TMP0]], i64* [[TMP1]], align 8, !tbaa !6
// CHECK-O2-NEXT:    ret void
//
void test_vmfirst_8xi1_mask(unsigned long gvl)
{
  signed long int result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmfirst_8xi1_mask(lhs, mask, gvl);
  *p557 = result;
}


unsigned long* p558;
// CHECK-O2-LABEL: @test_vmsbf_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsbf.nxv1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p558 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmsbf_1xi1(lhs, gvl);
  __builtin_epi_vstore_1xi1(p558, result);
}


unsigned int* p559;
// CHECK-O2-LABEL: @test_vmsbf_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsbf.nxv2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p559 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmsbf_2xi1(lhs, gvl);
  __builtin_epi_vstore_2xi1(p559, result);
}


unsigned short* p560;
// CHECK-O2-LABEL: @test_vmsbf_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsbf.nxv4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p560 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmsbf_4xi1(lhs, gvl);
  __builtin_epi_vstore_4xi1(p560, result);
}


unsigned char* p561;
// CHECK-O2-LABEL: @test_vmsbf_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsbf.nxv8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p561 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmsbf_8xi1(lhs, gvl);
  __builtin_epi_vstore_8xi1(p561, result);
}


unsigned long* p562;
// CHECK-O2-LABEL: @test_vmsbf_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsbf.mask.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p562 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_1xi1_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsbf_1xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p562, result);
}


unsigned int* p563;
// CHECK-O2-LABEL: @test_vmsbf_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsbf.mask.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p563 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_2xi1_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsbf_2xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p563, result);
}


unsigned short* p564;
// CHECK-O2-LABEL: @test_vmsbf_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsbf.mask.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p564 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_4xi1_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsbf_4xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p564, result);
}


unsigned char* p565;
// CHECK-O2-LABEL: @test_vmsbf_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsbf.mask.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p565 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsbf_8xi1_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsbf_8xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p565, result);
}


unsigned long* p566;
// CHECK-O2-LABEL: @test_vmsif_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsif.nxv1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p566 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmsif_1xi1(lhs, gvl);
  __builtin_epi_vstore_1xi1(p566, result);
}


unsigned int* p567;
// CHECK-O2-LABEL: @test_vmsif_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsif.nxv2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p567 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmsif_2xi1(lhs, gvl);
  __builtin_epi_vstore_2xi1(p567, result);
}


unsigned short* p568;
// CHECK-O2-LABEL: @test_vmsif_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsif.nxv4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p568 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmsif_4xi1(lhs, gvl);
  __builtin_epi_vstore_4xi1(p568, result);
}


unsigned char* p569;
// CHECK-O2-LABEL: @test_vmsif_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsif.nxv8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p569 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmsif_8xi1(lhs, gvl);
  __builtin_epi_vstore_8xi1(p569, result);
}


unsigned long* p570;
// CHECK-O2-LABEL: @test_vmsif_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsif.mask.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p570 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_1xi1_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsif_1xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p570, result);
}


unsigned int* p571;
// CHECK-O2-LABEL: @test_vmsif_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsif.mask.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p571 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_2xi1_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsif_2xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p571, result);
}


unsigned short* p572;
// CHECK-O2-LABEL: @test_vmsif_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsif.mask.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p572 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_4xi1_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsif_4xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p572, result);
}


unsigned char* p573;
// CHECK-O2-LABEL: @test_vmsif_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsif.mask.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p573 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsif_8xi1_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsif_8xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p573, result);
}


unsigned long* p574;
// CHECK-O2-LABEL: @test_vmsof_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsof.nxv1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p574 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_1xi1(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_vmsof_1xi1(lhs, gvl);
  __builtin_epi_vstore_1xi1(p574, result);
}


unsigned int* p575;
// CHECK-O2-LABEL: @test_vmsof_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsof.nxv2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p575 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_2xi1(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_vmsof_2xi1(lhs, gvl);
  __builtin_epi_vstore_2xi1(p575, result);
}


unsigned short* p576;
// CHECK-O2-LABEL: @test_vmsof_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsof.nxv4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p576 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_4xi1(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_vmsof_4xi1(lhs, gvl);
  __builtin_epi_vstore_4xi1(p576, result);
}


unsigned char* p577;
// CHECK-O2-LABEL: @test_vmsof_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsof.nxv8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p577 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_8xi1(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_vmsof_8xi1(lhs, gvl);
  __builtin_epi_vstore_8xi1(p577, result);
}


unsigned long* p578;
// CHECK-O2-LABEL: @test_vmsof_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.vmsof.mask.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p578 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_1xi1_mask(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vmsof_1xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi1(p578, result);
}


unsigned int* p579;
// CHECK-O2-LABEL: @test_vmsof_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.vmsof.mask.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p579 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_2xi1_mask(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vmsof_2xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi1(p579, result);
}


unsigned short* p580;
// CHECK-O2-LABEL: @test_vmsof_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.vmsof.mask.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p580 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_4xi1_mask(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vmsof_4xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi1(p580, result);
}


unsigned char* p581;
// CHECK-O2-LABEL: @test_vmsof_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.vmsof.mask.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p581 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_vmsof_8xi1_mask(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vmsof_8xi1_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi1(p581, result);
}


signed char* p582;
// CHECK-O2-LABEL: @test_viota_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.viota.nxv8i8.nxv8i1(<vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p582 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_viota_8xi8(lhs, gvl);
  __builtin_epi_vstore_8xi8(p582, result, gvl);
}


short* p583;
// CHECK-O2-LABEL: @test_viota_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.viota.nxv4i16.nxv4i1(<vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p583 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_viota_4xi16(lhs, gvl);
  __builtin_epi_vstore_4xi16(p583, result, gvl);
}


int* p584;
// CHECK-O2-LABEL: @test_viota_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.viota.nxv2i32.nxv2i1(<vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p584 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_viota_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p584, result, gvl);
}


long* p585;
// CHECK-O2-LABEL: @test_viota_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.viota.nxv1i64.nxv1i1(<vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p585 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_viota_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p585, result, gvl);
}


signed char* p586;
// CHECK-O2-LABEL: @test_viota_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.viota.mask.nxv8i8.nxv8i1(<vscale x 8 x i1> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p586 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi1 lhs;
  __epi_8xi1 mask;
  result = __builtin_epi_viota_8xi8_mask(lhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p586, result, gvl);
}


short* p587;
// CHECK-O2-LABEL: @test_viota_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.viota.mask.nxv4i16.nxv4i1(<vscale x 4 x i1> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p587 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi1 lhs;
  __epi_4xi1 mask;
  result = __builtin_epi_viota_4xi16_mask(lhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p587, result, gvl);
}


int* p588;
// CHECK-O2-LABEL: @test_viota_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.viota.mask.nxv2i32.nxv2i1(<vscale x 2 x i1> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p588 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi1 lhs;
  __epi_2xi1 mask;
  result = __builtin_epi_viota_2xi32_mask(lhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p588, result, gvl);
}


long* p589;
// CHECK-O2-LABEL: @test_viota_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.viota.mask.nxv1i64.nxv1i1(<vscale x 1 x i1> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p589 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_viota_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi1 lhs;
  __epi_1xi1 mask;
  result = __builtin_epi_viota_1xi64_mask(lhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p589, result, gvl);
}


// CHECK-O2-LABEL: @test_vextract_8xi8_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i8 @llvm.epi.vext.x.v.i8.nxv8i8(<vscale x 8 x i8> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i8 [[TMP0]]
//
signed char test_vextract_8xi8_(unsigned long idx)
{
  signed char result;
  __epi_8xi8 lhs;
  result = __builtin_epi_vextract_8xi8(lhs, idx);
  return result;
}


// CHECK-O2-LABEL: @test_vextract_4xi16_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i16 @llvm.epi.vext.x.v.i16.nxv4i16(<vscale x 4 x i16> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i16 [[TMP0]]
//
signed short int test_vextract_4xi16_(unsigned long idx)
{
  signed short int result;
  __epi_4xi16 lhs;
  result = __builtin_epi_vextract_4xi16(lhs, idx);
  return result;
}


// CHECK-O2-LABEL: @test_vextract_2xi32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.epi.vext.x.v.i32.nxv2i32(<vscale x 2 x i32> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i32 [[TMP0]]
//
signed int test_vextract_2xi32_(unsigned long idx)
{
  signed int result;
  __epi_2xi32 lhs;
  result = __builtin_epi_vextract_2xi32(lhs, idx);
  return result;
}


// CHECK-O2-LABEL: @test_vextract_1xi64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.epi.vext.x.v.i64.nxv1i64(<vscale x 1 x i64> undef, i64 [[IDX:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vextract_1xi64_(unsigned long idx)
{
  signed long int result;
  __epi_1xi64 lhs;
  result = __builtin_epi_vextract_1xi64(lhs, idx);
  return result;
}


signed char* p590;
// CHECK-O2-LABEL: @test_vsetfirst_8xi8_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[VALUE:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vmv.s.x.nxv8i8.i8(i8 [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p590 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_8xi8_(unsigned long int value, unsigned long gvl)
{
  __epi_8xi8 result;
  result = __builtin_epi_vsetfirst_8xi8(value, gvl);
  __builtin_epi_vstore_8xi8(p590, result, gvl);
}


short* p591;
// CHECK-O2-LABEL: @test_vsetfirst_4xi16_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[VALUE:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vmv.s.x.nxv4i16.i16(i16 [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p591 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_4xi16_(unsigned long int value, unsigned long gvl)
{
  __epi_4xi16 result;
  result = __builtin_epi_vsetfirst_4xi16(value, gvl);
  __builtin_epi_vstore_4xi16(p591, result, gvl);
}


int* p592;
// CHECK-O2-LABEL: @test_vsetfirst_2xi32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[VALUE:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vmv.s.x.nxv2i32.i32(i32 [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p592 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_2xi32_(unsigned long int value, unsigned long gvl)
{
  __epi_2xi32 result;
  result = __builtin_epi_vsetfirst_2xi32(value, gvl);
  __builtin_epi_vstore_2xi32(p592, result, gvl);
}


long* p593;
// CHECK-O2-LABEL: @test_vsetfirst_1xi64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vmv.s.x.nxv1i64.i64(i64 [[VALUE:%.*]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p593 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_1xi64_(unsigned long int value, unsigned long gvl)
{
  __epi_1xi64 result;
  result = __builtin_epi_vsetfirst_1xi64(value, gvl);
  __builtin_epi_vstore_1xi64(p593, result, gvl);
}


float* p594;
// CHECK-O2-LABEL: @test_vsetfirst_2xf32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[VALUE:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfmv.s.f.nxv2f32.f32(float [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p594 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_2xf32_(unsigned long int value, unsigned long gvl)
{
  __epi_2xf32 result;
  result = __builtin_epi_vsetfirst_2xf32(value, gvl);
  __builtin_epi_vstore_2xf32(p594, result, gvl);
}


double* p595;
// CHECK-O2-LABEL: @test_vsetfirst_1xf64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[VALUE:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfmv.s.f.nxv1f64.f64(double [[CONV]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p595 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vsetfirst_1xf64_(unsigned long int value, unsigned long gvl)
{
  __epi_1xf64 result;
  result = __builtin_epi_vsetfirst_1xf64(value, gvl);
  __builtin_epi_vstore_1xf64(p595, result, gvl);
}


// CHECK-O2-LABEL: @test_vgetfirst_2xf32_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call float @llvm.epi.vfmv.f.s.f32.nxv2f32(<vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    ret float [[TMP0]]
//
float test_vgetfirst_2xf32_(unsigned long gvl)
{
  float result;
  __epi_2xf32 lhs;
  result = __builtin_epi_vgetfirst_2xf32(lhs, gvl);
  return result;
}


// CHECK-O2-LABEL: @test_vgetfirst_1xf64_(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call double @llvm.epi.vfmv.f.s.f64.nxv1f64(<vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    ret double [[TMP0]]
//
double test_vgetfirst_1xf64_(unsigned long gvl)
{
  double result;
  __epi_1xf64 lhs;
  result = __builtin_epi_vgetfirst_1xf64(lhs, gvl);
  return result;
}


signed char* p596;
// CHECK-O2-LABEL: @test_vslideup_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslideup.nxv8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p596 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p596, result, gvl);
}


short* p597;
// CHECK-O2-LABEL: @test_vslideup_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslideup.nxv4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p597 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p597, result, gvl);
}


int* p598;
// CHECK-O2-LABEL: @test_vslideup_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslideup.nxv2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p598 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p598, result, gvl);
}


long* p599;
// CHECK-O2-LABEL: @test_vslideup_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p599 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p599, result, gvl);
}


float* p600;
// CHECK-O2-LABEL: @test_vslideup_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslideup.nxv2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p600 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p600, result, gvl);
}


double* p601;
// CHECK-O2-LABEL: @test_vslideup_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p601 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslideup_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p601, result, gvl);
}


signed char* p602;
// CHECK-O2-LABEL: @test_vslideup_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslideup.mask.nxv8i8.i64.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p602 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslideup_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p602, result, gvl);
}


short* p603;
// CHECK-O2-LABEL: @test_vslideup_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslideup.mask.nxv4i16.i64.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p603 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslideup_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p603, result, gvl);
}


int* p604;
// CHECK-O2-LABEL: @test_vslideup_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslideup.mask.nxv2i32.i64.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p604 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslideup_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p604, result, gvl);
}


long* p605;
// CHECK-O2-LABEL: @test_vslideup_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p605 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslideup_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p605, result, gvl);
}


float* p606;
// CHECK-O2-LABEL: @test_vslideup_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslideup.mask.nxv2f32.i64.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p606 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslideup_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p606, result, gvl);
}


double* p607;
// CHECK-O2-LABEL: @test_vslideup_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p607 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslideup_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslideup_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p607, result, gvl);
}


signed char* p608;
// CHECK-O2-LABEL: @test_vslidedown_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslidedown.nxv8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p608 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p608, result, gvl);
}


short* p609;
// CHECK-O2-LABEL: @test_vslidedown_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslidedown.nxv4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p609 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p609, result, gvl);
}


int* p610;
// CHECK-O2-LABEL: @test_vslidedown_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslidedown.nxv2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p610 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p610, result, gvl);
}


long* p611;
// CHECK-O2-LABEL: @test_vslidedown_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p611 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p611, result, gvl);
}


float* p612;
// CHECK-O2-LABEL: @test_vslidedown_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslidedown.nxv2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p612 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p612, result, gvl);
}


double* p613;
// CHECK-O2-LABEL: @test_vslidedown_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p613 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslidedown_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p613, result, gvl);
}


signed char* p614;
// CHECK-O2-LABEL: @test_vslidedown_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslidedown.mask.nxv8i8.i64.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p614 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslidedown_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p614, result, gvl);
}


short* p615;
// CHECK-O2-LABEL: @test_vslidedown_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslidedown.mask.nxv4i16.i64.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p615 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslidedown_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p615, result, gvl);
}


int* p616;
// CHECK-O2-LABEL: @test_vslidedown_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslidedown.mask.nxv2i32.i64.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p616 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslidedown_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p616, result, gvl);
}


long* p617;
// CHECK-O2-LABEL: @test_vslidedown_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p617 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslidedown_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p617, result, gvl);
}


float* p618;
// CHECK-O2-LABEL: @test_vslidedown_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslidedown.mask.nxv2f32.i64.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p618 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslidedown_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p618, result, gvl);
}


double* p619;
// CHECK-O2-LABEL: @test_vslidedown_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p619 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslidedown_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslidedown_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p619, result, gvl);
}


signed char* p620;
// CHECK-O2-LABEL: @test_vslide1up_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1up.nxv8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p620 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p620, result, gvl);
}


short* p621;
// CHECK-O2-LABEL: @test_vslide1up_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1up.nxv4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p621 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p621, result, gvl);
}


int* p622;
// CHECK-O2-LABEL: @test_vslide1up_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1up.nxv2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p622 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p622, result, gvl);
}


long* p623;
// CHECK-O2-LABEL: @test_vslide1up_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p623 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p623, result, gvl);
}


float* p624;
// CHECK-O2-LABEL: @test_vslide1up_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1up.nxv2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p624 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p624, result, gvl);
}


double* p625;
// CHECK-O2-LABEL: @test_vslide1up_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p625 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1up_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p625, result, gvl);
}


signed char* p626;
// CHECK-O2-LABEL: @test_vslide1up_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1up.mask.nxv8i8.i64.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p626 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslide1up_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p626, result, gvl);
}


short* p627;
// CHECK-O2-LABEL: @test_vslide1up_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1up.mask.nxv4i16.i64.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p627 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslide1up_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p627, result, gvl);
}


int* p628;
// CHECK-O2-LABEL: @test_vslide1up_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1up.mask.nxv2i32.i64.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p628 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1up_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p628, result, gvl);
}


long* p629;
// CHECK-O2-LABEL: @test_vslide1up_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p629 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1up_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p629, result, gvl);
}


float* p630;
// CHECK-O2-LABEL: @test_vslide1up_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1up.mask.nxv2f32.i64.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p630 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1up_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p630, result, gvl);
}


double* p631;
// CHECK-O2-LABEL: @test_vslide1up_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p631 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1up_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1up_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p631, result, gvl);
}


signed char* p632;
// CHECK-O2-LABEL: @test_vslide1down_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1down.nxv8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p632 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p632, result, gvl);
}


short* p633;
// CHECK-O2-LABEL: @test_vslide1down_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1down.nxv4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p633 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p633, result, gvl);
}


int* p634;
// CHECK-O2-LABEL: @test_vslide1down_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1down.nxv2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p634 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p634, result, gvl);
}


long* p635;
// CHECK-O2-LABEL: @test_vslide1down_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p635 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p635, result, gvl);
}


float* p636;
// CHECK-O2-LABEL: @test_vslide1down_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1down.nxv2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p636 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p636, result, gvl);
}


double* p637;
// CHECK-O2-LABEL: @test_vslide1down_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p637 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  result = __builtin_epi_vslide1down_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p637, result, gvl);
}


signed char* p638;
// CHECK-O2-LABEL: @test_vslide1down_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vslide1down.mask.nxv8i8.i64.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p638 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  unsigned long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vslide1down_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p638, result, gvl);
}


short* p639;
// CHECK-O2-LABEL: @test_vslide1down_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vslide1down.mask.nxv4i16.i64.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p639 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  unsigned long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vslide1down_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p639, result, gvl);
}


int* p640;
// CHECK-O2-LABEL: @test_vslide1down_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vslide1down.mask.nxv2i32.i64.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p640 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1down_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p640, result, gvl);
}


long* p641;
// CHECK-O2-LABEL: @test_vslide1down_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p641 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1down_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p641, result, gvl);
}


float* p642;
// CHECK-O2-LABEL: @test_vslide1down_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vslide1down.mask.nxv2f32.i64.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p642 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  unsigned long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vslide1down_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p642, result, gvl);
}


double* p643;
// CHECK-O2-LABEL: @test_vslide1down_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p643 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vslide1down_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  unsigned long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vslide1down_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p643, result, gvl);
}


signed char* p644;
// CHECK-O2-LABEL: @test_vrgather_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.i64(<vscale x 8 x i8> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p644 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p644, result, gvl);
}


short* p645;
// CHECK-O2-LABEL: @test_vrgather_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.i64(<vscale x 4 x i16> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p645 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p645, result, gvl);
}


int* p646;
// CHECK-O2-LABEL: @test_vrgather_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.i64(<vscale x 2 x i32> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p646 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p646, result, gvl);
}


long* p647;
// CHECK-O2-LABEL: @test_vrgather_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(<vscale x 1 x i64> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p647 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p647, result, gvl);
}


float* p648;
// CHECK-O2-LABEL: @test_vrgather_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.i64(<vscale x 2 x float> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p648 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p648, result, gvl);
}


double* p649;
// CHECK-O2-LABEL: @test_vrgather_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(<vscale x 1 x double> undef, i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p649 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  signed long int rhs;
  result = __builtin_epi_vrgather_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p649, result, gvl);
}


signed char* p650;
// CHECK-O2-LABEL: @test_vrgather_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.i64.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p650 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  signed long int rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vrgather_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p650, result, gvl);
}


short* p651;
// CHECK-O2-LABEL: @test_vrgather_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.i64.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p651 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  signed long int rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vrgather_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p651, result, gvl);
}


int* p652;
// CHECK-O2-LABEL: @test_vrgather_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.i64.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p652 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  signed long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrgather_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p652, result, gvl);
}


long* p653;
// CHECK-O2-LABEL: @test_vrgather_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p653 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  signed long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrgather_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p653, result, gvl);
}


float* p654;
// CHECK-O2-LABEL: @test_vrgather_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.i64.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p654 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  signed long int rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vrgather_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p654, result, gvl);
}


double* p655;
// CHECK-O2-LABEL: @test_vrgather_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p655 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vrgather_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  signed long int rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vrgather_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p655, result, gvl);
}


signed char* p656;
// CHECK-O2-LABEL: @test_vcompress_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vcompress.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p656 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi1 rhs;
  result = __builtin_epi_vcompress_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p656, result, gvl);
}


short* p657;
// CHECK-O2-LABEL: @test_vcompress_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vcompress.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p657 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi1 rhs;
  result = __builtin_epi_vcompress_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p657, result, gvl);
}


int* p658;
// CHECK-O2-LABEL: @test_vcompress_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vcompress.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p658 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vcompress_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p658, result, gvl);
}


long* p659;
// CHECK-O2-LABEL: @test_vcompress_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vcompress.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p659 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vcompress_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p659, result, gvl);
}


float* p660;
// CHECK-O2-LABEL: @test_vcompress_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vcompress.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p660 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xi1 rhs;
  result = __builtin_epi_vcompress_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p660, result, gvl);
}


double* p661;
// CHECK-O2-LABEL: @test_vcompress_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vcompress.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p661 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vcompress_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xi1 rhs;
  result = __builtin_epi_vcompress_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p661, result, gvl);
}


signed char* p662;
// CHECK-O2-LABEL: @test_vdot_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdot.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p662 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdot_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p662, result, gvl);
}


short* p663;
// CHECK-O2-LABEL: @test_vdot_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdot.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p663 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdot_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p663, result, gvl);
}


int* p664;
// CHECK-O2-LABEL: @test_vdot_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdot.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p664 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdot_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p664, result, gvl);
}


long* p665;
// CHECK-O2-LABEL: @test_vdot_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdot.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p665 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdot_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p665, result, gvl);
}


signed char* p666;
// CHECK-O2-LABEL: @test_vdot_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdot.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p666 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdot_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p666, result, gvl);
}


short* p667;
// CHECK-O2-LABEL: @test_vdot_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdot.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p667 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdot_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p667, result, gvl);
}


int* p668;
// CHECK-O2-LABEL: @test_vdot_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdot.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p668 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdot_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p668, result, gvl);
}


long* p669;
// CHECK-O2-LABEL: @test_vdot_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdot.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p669 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdot_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdot_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p669, result, gvl);
}


signed char* p670;
// CHECK-O2-LABEL: @test_vdotu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdotu.nxv8i8.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p670 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  result = __builtin_epi_vdotu_8xi8(lhs, rhs, gvl);
  __builtin_epi_vstore_8xi8(p670, result, gvl);
}


short* p671;
// CHECK-O2-LABEL: @test_vdotu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdotu.nxv4i16.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p671 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  result = __builtin_epi_vdotu_4xi16(lhs, rhs, gvl);
  __builtin_epi_vstore_4xi16(p671, result, gvl);
}


int* p672;
// CHECK-O2-LABEL: @test_vdotu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdotu.nxv2i32.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p672 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  result = __builtin_epi_vdotu_2xi32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xi32(p672, result, gvl);
}


long* p673;
// CHECK-O2-LABEL: @test_vdotu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdotu.nxv1i64.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p673 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  result = __builtin_epi_vdotu_1xi64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xi64(p673, result, gvl);
}


signed char* p674;
// CHECK-O2-LABEL: @test_vdotu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vdotu.mask.nxv8i8.nxv8i8.nxv8i1(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p674 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_8xi8_mask(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi8 merge;
  __epi_8xi8 lhs;
  __epi_8xi8 rhs;
  __epi_8xi1 mask;
  result = __builtin_epi_vdotu_8xi8_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_8xi8(p674, result, gvl);
}


short* p675;
// CHECK-O2-LABEL: @test_vdotu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vdotu.mask.nxv4i16.nxv4i16.nxv4i1(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p675 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_4xi16_mask(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi16 merge;
  __epi_4xi16 lhs;
  __epi_4xi16 rhs;
  __epi_4xi1 mask;
  result = __builtin_epi_vdotu_4xi16_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_4xi16(p675, result, gvl);
}


int* p676;
// CHECK-O2-LABEL: @test_vdotu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vdotu.mask.nxv2i32.nxv2i32.nxv2i1(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p676 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_2xi32_mask(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi32 merge;
  __epi_2xi32 lhs;
  __epi_2xi32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vdotu_2xi32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xi32(p676, result, gvl);
}


long* p677;
// CHECK-O2-LABEL: @test_vdotu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vdotu.mask.nxv1i64.nxv1i64.nxv1i1(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p677 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vdotu_1xi64_mask(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi64 merge;
  __epi_1xi64 lhs;
  __epi_1xi64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vdotu_1xi64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xi64(p677, result, gvl);
}


float* p678;
// CHECK-O2-LABEL: @test_vfdot_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdot.nxv2f32.nxv2f32(<vscale x 2 x float> undef, <vscale x 2 x float> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p678 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  result = __builtin_epi_vfdot_2xf32(lhs, rhs, gvl);
  __builtin_epi_vstore_2xf32(p678, result, gvl);
}


double* p679;
// CHECK-O2-LABEL: @test_vfdot_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdot.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p679 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  result = __builtin_epi_vfdot_1xf64(lhs, rhs, gvl);
  __builtin_epi_vstore_1xf64(p679, result, gvl);
}


float* p680;
// CHECK-O2-LABEL: @test_vfdot_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vfdot.mask.nxv2f32.nxv2f32.nxv2i1(<vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x float> undef, <vscale x 2 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p680 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_2xf32_mask(unsigned long gvl)
{
  __epi_2xf32 result;
  __epi_2xf32 merge;
  __epi_2xf32 lhs;
  __epi_2xf32 rhs;
  __epi_2xi1 mask;
  result = __builtin_epi_vfdot_2xf32_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_2xf32(p680, result, gvl);
}


double* p681;
// CHECK-O2-LABEL: @test_vfdot_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vfdot.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x i1> undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p681 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vfdot_1xf64_mask(unsigned long gvl)
{
  __epi_1xf64 result;
  __epi_1xf64 merge;
  __epi_1xf64 lhs;
  __epi_1xf64 rhs;
  __epi_1xi1 mask;
  result = __builtin_epi_vfdot_1xf64_mask(merge, lhs, rhs, mask, gvl);
  __builtin_epi_vstore_1xf64(p681, result, gvl);
}


signed char* p682;
// CHECK-O2-LABEL: @test_vbroadcast_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.vbroadcast.nxv8i8.i8(i8 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p682 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_8xi8(unsigned long gvl)
{
  __epi_8xi8 result;
  signed char lhs;
  result = __builtin_epi_vbroadcast_8xi8(lhs, gvl);
  __builtin_epi_vstore_8xi8(p682, result, gvl);
}


short* p683;
// CHECK-O2-LABEL: @test_vbroadcast_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.vbroadcast.nxv4i16.i16(i16 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p683 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_4xi16(unsigned long gvl)
{
  __epi_4xi16 result;
  signed short int lhs;
  result = __builtin_epi_vbroadcast_4xi16(lhs, gvl);
  __builtin_epi_vstore_4xi16(p683, result, gvl);
}


int* p684;
// CHECK-O2-LABEL: @test_vbroadcast_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.vbroadcast.nxv2i32.i32(i32 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p684 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_2xi32(unsigned long gvl)
{
  __epi_2xi32 result;
  signed int lhs;
  result = __builtin_epi_vbroadcast_2xi32(lhs, gvl);
  __builtin_epi_vstore_2xi32(p684, result, gvl);
}


long* p685;
// CHECK-O2-LABEL: @test_vbroadcast_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.vbroadcast.nxv1i64.i64(i64 undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p685 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_1xi64(unsigned long gvl)
{
  __epi_1xi64 result;
  signed long int lhs;
  result = __builtin_epi_vbroadcast_1xi64(lhs, gvl);
  __builtin_epi_vstore_1xi64(p685, result, gvl);
}


float* p686;
// CHECK-O2-LABEL: @test_vbroadcast_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.epi.vbroadcast.nxv2f32.f32(float undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x float>*, <vscale x 2 x float>** bitcast (float** @p686 to <vscale x 2 x float>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2f32(<vscale x 2 x float> [[TMP0]], <vscale x 2 x float>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_2xf32(unsigned long gvl)
{
  __epi_2xf32 result;
  float lhs;
  result = __builtin_epi_vbroadcast_2xf32(lhs, gvl);
  __builtin_epi_vstore_2xf32(p686, result, gvl);
}


double* p687;
// CHECK-O2-LABEL: @test_vbroadcast_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.epi.vbroadcast.nxv1f64.f64(double undef, i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x double>*, <vscale x 1 x double>** bitcast (double** @p687 to <vscale x 1 x double>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> [[TMP0]], <vscale x 1 x double>* [[TMP1]], i64 [[GVL]])
// CHECK-O2-NEXT:    ret void
//
void test_vbroadcast_1xf64(unsigned long gvl)
{
  __epi_1xf64 result;
  double lhs;
  result = __builtin_epi_vbroadcast_1xf64(lhs, gvl);
  __builtin_epi_vstore_1xf64(p687, result, gvl);
}


unsigned char* p688;
// CHECK-O2-LABEL: @test_cast_8xi1_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.epi.mask.cast.nxv8i1.nxv8i8(<vscale x 8 x i8> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i1>*, <vscale x 8 x i1>** bitcast (i8** @p688 to <vscale x 8 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1>* [[TMP1]], align 1
// CHECK-O2-NEXT:    ret void
//
void test_cast_8xi1_8xi8(unsigned long gvl)
{
  __epi_8xi1 result;
  __epi_8xi8 lhs;
  result = __builtin_epi_cast_8xi1_8xi8(lhs);
  __builtin_epi_vstore_8xi1(p688, result);
}


unsigned short* p689;
// CHECK-O2-LABEL: @test_cast_4xi1_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.epi.mask.cast.nxv4i1.nxv4i16(<vscale x 4 x i16> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i1>*, <vscale x 4 x i1>** bitcast (i16** @p689 to <vscale x 4 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1>* [[TMP1]], align 2
// CHECK-O2-NEXT:    ret void
//
void test_cast_4xi1_4xi16(unsigned long gvl)
{
  __epi_4xi1 result;
  __epi_4xi16 lhs;
  result = __builtin_epi_cast_4xi1_4xi16(lhs);
  __builtin_epi_vstore_4xi1(p689, result);
}


unsigned int* p690;
// CHECK-O2-LABEL: @test_cast_2xi1_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.epi.mask.cast.nxv2i1.nxv2i32(<vscale x 2 x i32> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i1>*, <vscale x 2 x i1>** bitcast (i32** @p690 to <vscale x 2 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1>* [[TMP1]], align 4
// CHECK-O2-NEXT:    ret void
//
void test_cast_2xi1_2xi32(unsigned long gvl)
{
  __epi_2xi1 result;
  __epi_2xi32 lhs;
  result = __builtin_epi_cast_2xi1_2xi32(lhs);
  __builtin_epi_vstore_2xi1(p690, result);
}


unsigned long* p691;
// CHECK-O2-LABEL: @test_cast_1xi1_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.epi.mask.cast.nxv1i1.nxv1i64(<vscale x 1 x i64> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i1>*, <vscale x 1 x i1>** bitcast (i64** @p691 to <vscale x 1 x i1>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    store <vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1>* [[TMP1]], align 8
// CHECK-O2-NEXT:    ret void
//
void test_cast_1xi1_1xi64(unsigned long gvl)
{
  __epi_1xi1 result;
  __epi_1xi64 lhs;
  result = __builtin_epi_cast_1xi1_1xi64(lhs);
  __builtin_epi_vstore_1xi1(p691, result);
}


signed char* p692;
// CHECK-O2-LABEL: @test_cast_8xi8_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.epi.mask.cast.nxv8i8.nxv8i1(<vscale x 8 x i1> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 8 x i8>*, <vscale x 8 x i8>** bitcast (i8** @p692 to <vscale x 8 x i8>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv8i8(<vscale x 8 x i8> [[TMP0]], <vscale x 8 x i8>* [[TMP1]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_cast_8xi8_8xi1(unsigned long gvl)
{
  __epi_8xi8 result;
  __epi_8xi1 lhs;
  result = __builtin_epi_cast_8xi8_8xi1(lhs);
  __builtin_epi_vstore_8xi8(p692, result, gvl);
}


short* p693;
// CHECK-O2-LABEL: @test_cast_4xi16_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.epi.mask.cast.nxv4i16.nxv4i1(<vscale x 4 x i1> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i16>*, <vscale x 4 x i16>** bitcast (i16** @p693 to <vscale x 4 x i16>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv4i16(<vscale x 4 x i16> [[TMP0]], <vscale x 4 x i16>* [[TMP1]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_cast_4xi16_4xi1(unsigned long gvl)
{
  __epi_4xi16 result;
  __epi_4xi1 lhs;
  result = __builtin_epi_cast_4xi16_4xi1(lhs);
  __builtin_epi_vstore_4xi16(p693, result, gvl);
}


int* p694;
// CHECK-O2-LABEL: @test_cast_2xi32_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.epi.mask.cast.nxv2i32.nxv2i1(<vscale x 2 x i1> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 2 x i32>*, <vscale x 2 x i32>** bitcast (i32** @p694 to <vscale x 2 x i32>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> [[TMP0]], <vscale x 2 x i32>* [[TMP1]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_cast_2xi32_2xi1(unsigned long gvl)
{
  __epi_2xi32 result;
  __epi_2xi1 lhs;
  result = __builtin_epi_cast_2xi32_2xi1(lhs);
  __builtin_epi_vstore_2xi32(p694, result, gvl);
}


long* p695;
// CHECK-O2-LABEL: @test_cast_1xi64_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.epi.mask.cast.nxv1i64.nxv1i1(<vscale x 1 x i1> undef)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = load <vscale x 1 x i64>*, <vscale x 1 x i64>** bitcast (i64** @p695 to <vscale x 1 x i64>**), align 8, !tbaa !2
// CHECK-O2-NEXT:    tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> [[TMP0]], <vscale x 1 x i64>* [[TMP1]], i64 [[GVL:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_cast_1xi64_1xi1(unsigned long gvl)
{
  __epi_1xi64 result;
  __epi_1xi1 lhs;
  result = __builtin_epi_cast_1xi64_1xi1(lhs);
  __builtin_epi_vstore_1xi64(p695, result, gvl);
}

